{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 1), (10,), 10)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing the dataset\n",
    "dataset = pd.read_csv('Position_Salaries.csv')\n",
    "x_true = dataset.iloc[:,1:2].values\n",
    "y_true = dataset.iloc[:,2].values\n",
    "N = np.size(x_true)\n",
    "\n",
    "x_true.shape,y_true.shape,N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the training data\n",
    "plt.scatter(x_true,y_true)\n",
    "plt.title(\"Experience - Salary Dataset\")\n",
    "plt.xlabel(\"x values\")\n",
    "plt.ylabel(\"y values\")\n",
    "plt.savefig(\"train_data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 1), (10,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the training data\n",
    "x_mean = x_true.mean()\n",
    "x_std = x_true.std()\n",
    "\n",
    "y_mean = y_true.mean()\n",
    "y_std = y_true.std()\n",
    "\n",
    "x_normal = (x_true - x_mean) / x_std\n",
    "y_normal = (y_true - y_mean) / y_std\n",
    "\n",
    "x_normal.shape,y_normal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'placeholders/Placeholder:0' shape=(10, 1) dtype=float32>,\n",
       " <tf.Tensor 'placeholders/Placeholder_1:0' shape=(10,) dtype=float32>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Placeholders\n",
    "with tf.name_scope(\"placeholders\"):\n",
    "    x = tf.placeholder(tf.float32,(N,1))\n",
    "    y = tf.placeholder(tf.float32,(N,))\n",
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'weights/Variable:0' shape=(1, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'weights/Variable_1:0' shape=(1, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'weights/Variable_2:0' shape=(1, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'weights/Variable_3:0' shape=(1, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'weights/Variable_4:0' shape=(1,) dtype=float32_ref>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weights --> a,b,c,d,e\n",
    "with tf.name_scope(\"weights\"):\n",
    "    a = tf.Variable(tf.random_normal((1,1)))\n",
    "    b = tf.Variable(tf.random_normal((1,1)))\n",
    "    c = tf.Variable(tf.random_normal((1,1)))\n",
    "    d = tf.Variable(tf.random_normal((1,1)))\n",
    "    e = tf.Variable(tf.random_normal((1,)))\n",
    "a,b,c,d,e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'prediction/add_3:0' shape=(10, 1) dtype=float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y = a*x**4 + b*x**3 + c*x**2 + d*x + e\n",
    "with tf.name_scope(\"prediction\"):\n",
    "    y_pred = tf.matmul(x**4,a) + tf.matmul(x**3,b) + tf.matmul(x**2,c) + tf.matmul(x,d) + e\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss/Sum:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Squared error --> (y - y_pred) ** 2\n",
    "with tf.name_scope(\"loss\"):\n",
    "    l = tf.reduce_sum((y - tf.squeeze(y_pred))**2)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Operation 'optim/Adam' type=NoOp>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adam Optimizer minimizing the squared error\n",
    "with tf.name_scope(\"optim\"):\n",
    "    train_op = tf.train.AdamOptimizer(.001).minimize(l)\n",
    "train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.summary.writer.writer.FileWriter at 0x1af7104c780>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.name_scope(\"summaries\"):\n",
    "    tf.summary.scalar(\"loss\",l)\n",
    "    merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter('/tensor_flow/linear_regression', tf.get_default_graph())\n",
    "train_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 119.233170\n",
      "step 1, loss: 118.929962\n",
      "step 2, loss: 118.627396\n",
      "step 3, loss: 118.325500\n",
      "step 4, loss: 118.024315\n",
      "step 5, loss: 117.723816\n",
      "step 6, loss: 117.424011\n",
      "step 7, loss: 117.124924\n",
      "step 8, loss: 116.826553\n",
      "step 9, loss: 116.528931\n",
      "step 10, loss: 116.232056\n",
      "step 11, loss: 115.935921\n",
      "step 12, loss: 115.640549\n",
      "step 13, loss: 115.345978\n",
      "step 14, loss: 115.052162\n",
      "step 15, loss: 114.759109\n",
      "step 16, loss: 114.466866\n",
      "step 17, loss: 114.175430\n",
      "step 18, loss: 113.884781\n",
      "step 19, loss: 113.594925\n",
      "step 20, loss: 113.305893\n",
      "step 21, loss: 113.017693\n",
      "step 22, loss: 112.730270\n",
      "step 23, loss: 112.443741\n",
      "step 24, loss: 112.158005\n",
      "step 25, loss: 111.873077\n",
      "step 26, loss: 111.588982\n",
      "step 27, loss: 111.305740\n",
      "step 28, loss: 111.023308\n",
      "step 29, loss: 110.741707\n",
      "step 30, loss: 110.460938\n",
      "step 31, loss: 110.181000\n",
      "step 32, loss: 109.901917\n",
      "step 33, loss: 109.623627\n",
      "step 34, loss: 109.346161\n",
      "step 35, loss: 109.069534\n",
      "step 36, loss: 108.793716\n",
      "step 37, loss: 108.518723\n",
      "step 38, loss: 108.244530\n",
      "step 39, loss: 107.971169\n",
      "step 40, loss: 107.698578\n",
      "step 41, loss: 107.426826\n",
      "step 42, loss: 107.155869\n",
      "step 43, loss: 106.885681\n",
      "step 44, loss: 106.616272\n",
      "step 45, loss: 106.347664\n",
      "step 46, loss: 106.079819\n",
      "step 47, loss: 105.812744\n",
      "step 48, loss: 105.546402\n",
      "step 49, loss: 105.280853\n",
      "step 50, loss: 105.016045\n",
      "step 51, loss: 104.751953\n",
      "step 52, loss: 104.488602\n",
      "step 53, loss: 104.225983\n",
      "step 54, loss: 103.964081\n",
      "step 55, loss: 103.702896\n",
      "step 56, loss: 103.442436\n",
      "step 57, loss: 103.182648\n",
      "step 58, loss: 102.923538\n",
      "step 59, loss: 102.665146\n",
      "step 60, loss: 102.407410\n",
      "step 61, loss: 102.150345\n",
      "step 62, loss: 101.893906\n",
      "step 63, loss: 101.638168\n",
      "step 64, loss: 101.383064\n",
      "step 65, loss: 101.128578\n",
      "step 66, loss: 100.874756\n",
      "step 67, loss: 100.621521\n",
      "step 68, loss: 100.368958\n",
      "step 69, loss: 100.116959\n",
      "step 70, loss: 99.865578\n",
      "step 71, loss: 99.614807\n",
      "step 72, loss: 99.364639\n",
      "step 73, loss: 99.115044\n",
      "step 74, loss: 98.866020\n",
      "step 75, loss: 98.617615\n",
      "step 76, loss: 98.369766\n",
      "step 77, loss: 98.122498\n",
      "step 78, loss: 97.875763\n",
      "step 79, loss: 97.629601\n",
      "step 80, loss: 97.384010\n",
      "step 81, loss: 97.138954\n",
      "step 82, loss: 96.894447\n",
      "step 83, loss: 96.650497\n",
      "step 84, loss: 96.407074\n",
      "step 85, loss: 96.164200\n",
      "step 86, loss: 95.921844\n",
      "step 87, loss: 95.680038\n",
      "step 88, loss: 95.438774\n",
      "step 89, loss: 95.198013\n",
      "step 90, loss: 94.957779\n",
      "step 91, loss: 94.718063\n",
      "step 92, loss: 94.478874\n",
      "step 93, loss: 94.240181\n",
      "step 94, loss: 94.002022\n",
      "step 95, loss: 93.764374\n",
      "step 96, loss: 93.527222\n",
      "step 97, loss: 93.290596\n",
      "step 98, loss: 93.054474\n",
      "step 99, loss: 92.818848\n",
      "step 100, loss: 92.583740\n",
      "step 101, loss: 92.349121\n",
      "step 102, loss: 92.115028\n",
      "step 103, loss: 91.881424\n",
      "step 104, loss: 91.648315\n",
      "step 105, loss: 91.415672\n",
      "step 106, loss: 91.183563\n",
      "step 107, loss: 90.951920\n",
      "step 108, loss: 90.720802\n",
      "step 109, loss: 90.490158\n",
      "step 110, loss: 90.260010\n",
      "step 111, loss: 90.030357\n",
      "step 112, loss: 89.801193\n",
      "step 113, loss: 89.572510\n",
      "step 114, loss: 89.344322\n",
      "step 115, loss: 89.116623\n",
      "step 116, loss: 88.889404\n",
      "step 117, loss: 88.662682\n",
      "step 118, loss: 88.436424\n",
      "step 119, loss: 88.210670\n",
      "step 120, loss: 87.985397\n",
      "step 121, loss: 87.760590\n",
      "step 122, loss: 87.536278\n",
      "step 123, loss: 87.312454\n",
      "step 124, loss: 87.089096\n",
      "step 125, loss: 86.866234\n",
      "step 126, loss: 86.643837\n",
      "step 127, loss: 86.421921\n",
      "step 128, loss: 86.200478\n",
      "step 129, loss: 85.979530\n",
      "step 130, loss: 85.759026\n",
      "step 131, loss: 85.539024\n",
      "step 132, loss: 85.319496\n",
      "step 133, loss: 85.100449\n",
      "step 134, loss: 84.881866\n",
      "step 135, loss: 84.663750\n",
      "step 136, loss: 84.446098\n",
      "step 137, loss: 84.228928\n",
      "step 138, loss: 84.012238\n",
      "step 139, loss: 83.795990\n",
      "step 140, loss: 83.580231\n",
      "step 141, loss: 83.364960\n",
      "step 142, loss: 83.150124\n",
      "step 143, loss: 82.935783\n",
      "step 144, loss: 82.721893\n",
      "step 145, loss: 82.508461\n",
      "step 146, loss: 82.295517\n",
      "step 147, loss: 82.083038\n",
      "step 148, loss: 81.871010\n",
      "step 149, loss: 81.659454\n",
      "step 150, loss: 81.448357\n",
      "step 151, loss: 81.237717\n",
      "step 152, loss: 81.027542\n",
      "step 153, loss: 80.817856\n",
      "step 154, loss: 80.608597\n",
      "step 155, loss: 80.399811\n",
      "step 156, loss: 80.191483\n",
      "step 157, loss: 79.983612\n",
      "step 158, loss: 79.776199\n",
      "step 159, loss: 79.569260\n",
      "step 160, loss: 79.362762\n",
      "step 161, loss: 79.156723\n",
      "step 162, loss: 78.951149\n",
      "step 163, loss: 78.746010\n",
      "step 164, loss: 78.541336\n",
      "step 165, loss: 78.337112\n",
      "step 166, loss: 78.133347\n",
      "step 167, loss: 77.930038\n",
      "step 168, loss: 77.727173\n",
      "step 169, loss: 77.524773\n",
      "step 170, loss: 77.322815\n",
      "step 171, loss: 77.121300\n",
      "step 172, loss: 76.920242\n",
      "step 173, loss: 76.719643\n",
      "step 174, loss: 76.519485\n",
      "step 175, loss: 76.319756\n",
      "step 176, loss: 76.120483\n",
      "step 177, loss: 75.921669\n",
      "step 178, loss: 75.723289\n",
      "step 179, loss: 75.525360\n",
      "step 180, loss: 75.327873\n",
      "step 181, loss: 75.130829\n",
      "step 182, loss: 74.934219\n",
      "step 183, loss: 74.738068\n",
      "step 184, loss: 74.542343\n",
      "step 185, loss: 74.347061\n",
      "step 186, loss: 74.152229\n",
      "step 187, loss: 73.957825\n",
      "step 188, loss: 73.763870\n",
      "step 189, loss: 73.570351\n",
      "step 190, loss: 73.377266\n",
      "step 191, loss: 73.184616\n",
      "step 192, loss: 72.992416\n",
      "step 193, loss: 72.800621\n",
      "step 194, loss: 72.609291\n",
      "step 195, loss: 72.418388\n",
      "step 196, loss: 72.227905\n",
      "step 197, loss: 72.037865\n",
      "step 198, loss: 71.848267\n",
      "step 199, loss: 71.659088\n",
      "step 200, loss: 71.470337\n",
      "step 201, loss: 71.282028\n",
      "step 202, loss: 71.094139\n",
      "step 203, loss: 70.906677\n",
      "step 204, loss: 70.719650\n",
      "step 205, loss: 70.533051\n",
      "step 206, loss: 70.346870\n",
      "step 207, loss: 70.161118\n",
      "step 208, loss: 69.975784\n",
      "step 209, loss: 69.790901\n",
      "step 210, loss: 69.606422\n",
      "step 211, loss: 69.422371\n",
      "step 212, loss: 69.238731\n",
      "step 213, loss: 69.055511\n",
      "step 214, loss: 68.872742\n",
      "step 215, loss: 68.690369\n",
      "step 216, loss: 68.508423\n",
      "step 217, loss: 68.326904\n",
      "step 218, loss: 68.145775\n",
      "step 219, loss: 67.965096\n",
      "step 220, loss: 67.784821\n",
      "step 221, loss: 67.604950\n",
      "step 222, loss: 67.425522\n",
      "step 223, loss: 67.246490\n",
      "step 224, loss: 67.067871\n",
      "step 225, loss: 66.889671\n",
      "step 226, loss: 66.711876\n",
      "step 227, loss: 66.534508\n",
      "step 228, loss: 66.357536\n",
      "step 229, loss: 66.180984\n",
      "step 230, loss: 66.004845\n",
      "step 231, loss: 65.829117\n",
      "step 232, loss: 65.653778\n",
      "step 233, loss: 65.478874\n",
      "step 234, loss: 65.304367\n",
      "step 235, loss: 65.130264\n",
      "step 236, loss: 64.956566\n",
      "step 237, loss: 64.783272\n",
      "step 238, loss: 64.610382\n",
      "step 239, loss: 64.437912\n",
      "step 240, loss: 64.265823\n",
      "step 241, loss: 64.094147\n",
      "step 242, loss: 63.922874\n",
      "step 243, loss: 63.751999\n",
      "step 244, loss: 63.581535\n",
      "step 245, loss: 63.411461\n",
      "step 246, loss: 63.241787\n",
      "step 247, loss: 63.072506\n",
      "step 248, loss: 62.903622\n",
      "step 249, loss: 62.735138\n",
      "step 250, loss: 62.567062\n",
      "step 251, loss: 62.399368\n",
      "step 252, loss: 62.232071\n",
      "step 253, loss: 62.065174\n",
      "step 254, loss: 61.898674\n",
      "step 255, loss: 61.732555\n",
      "step 256, loss: 61.566837\n",
      "step 257, loss: 61.401505\n",
      "step 258, loss: 61.236565\n",
      "step 259, loss: 61.072018\n",
      "step 260, loss: 60.907856\n",
      "step 261, loss: 60.744095\n",
      "step 262, loss: 60.580704\n",
      "step 263, loss: 60.417709\n",
      "step 264, loss: 60.255096\n",
      "step 265, loss: 60.092880\n",
      "step 266, loss: 59.931034\n",
      "step 267, loss: 59.769585\n",
      "step 268, loss: 59.608521\n",
      "step 269, loss: 59.447830\n",
      "step 270, loss: 59.287533\n",
      "step 271, loss: 59.127609\n",
      "step 272, loss: 58.968071\n",
      "step 273, loss: 58.808914\n",
      "step 274, loss: 58.650135\n",
      "step 275, loss: 58.491741\n",
      "step 276, loss: 58.333717\n",
      "step 277, loss: 58.176064\n",
      "step 278, loss: 58.018803\n",
      "step 279, loss: 57.861908\n",
      "step 280, loss: 57.705391\n",
      "step 281, loss: 57.549252\n",
      "step 282, loss: 57.393494\n",
      "step 283, loss: 57.238102\n",
      "step 284, loss: 57.083073\n",
      "step 285, loss: 56.928421\n",
      "step 286, loss: 56.774155\n",
      "step 287, loss: 56.620247\n",
      "step 288, loss: 56.466705\n",
      "step 289, loss: 56.313545\n",
      "step 290, loss: 56.160755\n",
      "step 291, loss: 56.008324\n",
      "step 292, loss: 55.856262\n",
      "step 293, loss: 55.704567\n",
      "step 294, loss: 55.553249\n",
      "step 295, loss: 55.402283\n",
      "step 296, loss: 55.251686\n",
      "step 297, loss: 55.101452\n",
      "step 298, loss: 54.951591\n",
      "step 299, loss: 54.802078\n",
      "step 300, loss: 54.652935\n",
      "step 301, loss: 54.504154\n",
      "step 302, loss: 54.355736\n",
      "step 303, loss: 54.207664\n",
      "step 304, loss: 54.059963\n",
      "step 305, loss: 53.912621\n",
      "step 306, loss: 53.765636\n",
      "step 307, loss: 53.619007\n",
      "step 308, loss: 53.472733\n",
      "step 309, loss: 53.326817\n",
      "step 310, loss: 53.181252\n",
      "step 311, loss: 53.036041\n",
      "step 312, loss: 52.891201\n",
      "step 313, loss: 52.746700\n",
      "step 314, loss: 52.602547\n",
      "step 315, loss: 52.458752\n",
      "step 316, loss: 52.315311\n",
      "step 317, loss: 52.172218\n",
      "step 318, loss: 52.029469\n",
      "step 319, loss: 51.887077\n",
      "step 320, loss: 51.745041\n",
      "step 321, loss: 51.603340\n",
      "step 322, loss: 51.461998\n",
      "step 323, loss: 51.320984\n",
      "step 324, loss: 51.180328\n",
      "step 325, loss: 51.040020\n",
      "step 326, loss: 50.900047\n",
      "step 327, loss: 50.760422\n",
      "step 328, loss: 50.621147\n",
      "step 329, loss: 50.482208\n",
      "step 330, loss: 50.343609\n",
      "step 331, loss: 50.205360\n",
      "step 332, loss: 50.067448\n",
      "step 333, loss: 49.929878\n",
      "step 334, loss: 49.792633\n",
      "step 335, loss: 49.655735\n",
      "step 336, loss: 49.519180\n",
      "step 337, loss: 49.382965\n",
      "step 338, loss: 49.247078\n",
      "step 339, loss: 49.111526\n",
      "step 340, loss: 48.976307\n",
      "step 341, loss: 48.841442\n",
      "step 342, loss: 48.706890\n",
      "step 343, loss: 48.572685\n",
      "step 344, loss: 48.438805\n",
      "step 345, loss: 48.305260\n",
      "step 346, loss: 48.172047\n",
      "step 347, loss: 48.039169\n",
      "step 348, loss: 47.906612\n",
      "step 349, loss: 47.774391\n",
      "step 350, loss: 47.642490\n",
      "step 351, loss: 47.510929\n",
      "step 352, loss: 47.379677\n",
      "step 353, loss: 47.248772\n",
      "step 354, loss: 47.118179\n",
      "step 355, loss: 46.987923\n",
      "step 356, loss: 46.857986\n",
      "step 357, loss: 46.728374\n",
      "step 358, loss: 46.599091\n",
      "step 359, loss: 46.470116\n",
      "step 360, loss: 46.341476\n",
      "step 361, loss: 46.213158\n",
      "step 362, loss: 46.085152\n",
      "step 363, loss: 45.957481\n",
      "step 364, loss: 45.830120\n",
      "step 365, loss: 45.703079\n",
      "step 366, loss: 45.576351\n",
      "step 367, loss: 45.449951\n",
      "step 368, loss: 45.323868\n",
      "step 369, loss: 45.198093\n",
      "step 370, loss: 45.072636\n",
      "step 371, loss: 44.947495\n",
      "step 372, loss: 44.822678\n",
      "step 373, loss: 44.698166\n",
      "step 374, loss: 44.573967\n",
      "step 375, loss: 44.450081\n",
      "step 376, loss: 44.326511\n",
      "step 377, loss: 44.203247\n",
      "step 378, loss: 44.080303\n",
      "step 379, loss: 43.957653\n",
      "step 380, loss: 43.835331\n",
      "step 381, loss: 43.713303\n",
      "step 382, loss: 43.591599\n",
      "step 383, loss: 43.470196\n",
      "step 384, loss: 43.349094\n",
      "step 385, loss: 43.228302\n",
      "step 386, loss: 43.107819\n",
      "step 387, loss: 42.987633\n",
      "step 388, loss: 42.867760\n",
      "step 389, loss: 42.748184\n",
      "step 390, loss: 42.628918\n",
      "step 391, loss: 42.509949\n",
      "step 392, loss: 42.391289\n",
      "step 393, loss: 42.272926\n",
      "step 394, loss: 42.154865\n",
      "step 395, loss: 42.037098\n",
      "step 396, loss: 41.919636\n",
      "step 397, loss: 41.802475\n",
      "step 398, loss: 41.685612\n",
      "step 399, loss: 41.569042\n",
      "step 400, loss: 41.452778\n",
      "step 401, loss: 41.336792\n",
      "step 402, loss: 41.221130\n",
      "step 403, loss: 41.105740\n",
      "step 404, loss: 40.990654\n",
      "step 405, loss: 40.875858\n",
      "step 406, loss: 40.761360\n",
      "step 407, loss: 40.647141\n",
      "step 408, loss: 40.533226\n",
      "step 409, loss: 40.419609\n",
      "step 410, loss: 40.306274\n",
      "step 411, loss: 40.193237\n",
      "step 412, loss: 40.080486\n",
      "step 413, loss: 39.968021\n",
      "step 414, loss: 39.855846\n",
      "step 415, loss: 39.743965\n",
      "step 416, loss: 39.632359\n",
      "step 417, loss: 39.521049\n",
      "step 418, loss: 39.410027\n",
      "step 419, loss: 39.299286\n",
      "step 420, loss: 39.188828\n",
      "step 421, loss: 39.078659\n",
      "step 422, loss: 38.968769\n",
      "step 423, loss: 38.859165\n",
      "step 424, loss: 38.749847\n",
      "step 425, loss: 38.640800\n",
      "step 426, loss: 38.532043\n",
      "step 427, loss: 38.423565\n",
      "step 428, loss: 38.315361\n",
      "step 429, loss: 38.207447\n",
      "step 430, loss: 38.099808\n",
      "step 431, loss: 37.992439\n",
      "step 432, loss: 37.885353\n",
      "step 433, loss: 37.778549\n",
      "step 434, loss: 37.672020\n",
      "step 435, loss: 37.565765\n",
      "step 436, loss: 37.459793\n",
      "step 437, loss: 37.354080\n",
      "step 438, loss: 37.248650\n",
      "step 439, loss: 37.143494\n",
      "step 440, loss: 37.038612\n",
      "step 441, loss: 36.933998\n",
      "step 442, loss: 36.829666\n",
      "step 443, loss: 36.725594\n",
      "step 444, loss: 36.621796\n",
      "step 445, loss: 36.518272\n",
      "step 446, loss: 36.415009\n",
      "step 447, loss: 36.312023\n",
      "step 448, loss: 36.209297\n",
      "step 449, loss: 36.106850\n",
      "step 450, loss: 36.004662\n",
      "step 451, loss: 35.902740\n",
      "step 452, loss: 35.801086\n",
      "step 453, loss: 35.699699\n",
      "step 454, loss: 35.598579\n",
      "step 455, loss: 35.497719\n",
      "step 456, loss: 35.397125\n",
      "step 457, loss: 35.296791\n",
      "step 458, loss: 35.196720\n",
      "step 459, loss: 35.096912\n",
      "step 460, loss: 34.997368\n",
      "step 461, loss: 34.898087\n",
      "step 462, loss: 34.799061\n",
      "step 463, loss: 34.700298\n",
      "step 464, loss: 34.601791\n",
      "step 465, loss: 34.503548\n",
      "step 466, loss: 34.405556\n",
      "step 467, loss: 34.307831\n",
      "step 468, loss: 34.210358\n",
      "step 469, loss: 34.113144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 470, loss: 34.016178\n",
      "step 471, loss: 33.919472\n",
      "step 472, loss: 33.823017\n",
      "step 473, loss: 33.726814\n",
      "step 474, loss: 33.630875\n",
      "step 475, loss: 33.535183\n",
      "step 476, loss: 33.439747\n",
      "step 477, loss: 33.344559\n",
      "step 478, loss: 33.249619\n",
      "step 479, loss: 33.154934\n",
      "step 480, loss: 33.060501\n",
      "step 481, loss: 32.966316\n",
      "step 482, loss: 32.872375\n",
      "step 483, loss: 32.778687\n",
      "step 484, loss: 32.685249\n",
      "step 485, loss: 32.592060\n",
      "step 486, loss: 32.499111\n",
      "step 487, loss: 32.406414\n",
      "step 488, loss: 32.313957\n",
      "step 489, loss: 32.221748\n",
      "step 490, loss: 32.129787\n",
      "step 491, loss: 32.038071\n",
      "step 492, loss: 31.946594\n",
      "step 493, loss: 31.855362\n",
      "step 494, loss: 31.764370\n",
      "step 495, loss: 31.673626\n",
      "step 496, loss: 31.583111\n",
      "step 497, loss: 31.492846\n",
      "step 498, loss: 31.402824\n",
      "step 499, loss: 31.313036\n",
      "step 500, loss: 31.223494\n",
      "step 501, loss: 31.134186\n",
      "step 502, loss: 31.045109\n",
      "step 503, loss: 30.956280\n",
      "step 504, loss: 30.867689\n",
      "step 505, loss: 30.779327\n",
      "step 506, loss: 30.691208\n",
      "step 507, loss: 30.603321\n",
      "step 508, loss: 30.515669\n",
      "step 509, loss: 30.428253\n",
      "step 510, loss: 30.341064\n",
      "step 511, loss: 30.254124\n",
      "step 512, loss: 30.167400\n",
      "step 513, loss: 30.080915\n",
      "step 514, loss: 29.994665\n",
      "step 515, loss: 29.908640\n",
      "step 516, loss: 29.822851\n",
      "step 517, loss: 29.737293\n",
      "step 518, loss: 29.651964\n",
      "step 519, loss: 29.566860\n",
      "step 520, loss: 29.481989\n",
      "step 521, loss: 29.397341\n",
      "step 522, loss: 29.312927\n",
      "step 523, loss: 29.228733\n",
      "step 524, loss: 29.144772\n",
      "step 525, loss: 29.061037\n",
      "step 526, loss: 28.977524\n",
      "step 527, loss: 28.894239\n",
      "step 528, loss: 28.811169\n",
      "step 529, loss: 28.728336\n",
      "step 530, loss: 28.645720\n",
      "step 531, loss: 28.563326\n",
      "step 532, loss: 28.481159\n",
      "step 533, loss: 28.399212\n",
      "step 534, loss: 28.317486\n",
      "step 535, loss: 28.235985\n",
      "step 536, loss: 28.154697\n",
      "step 537, loss: 28.073631\n",
      "step 538, loss: 27.992788\n",
      "step 539, loss: 27.912157\n",
      "step 540, loss: 27.831747\n",
      "step 541, loss: 27.751560\n",
      "step 542, loss: 27.671589\n",
      "step 543, loss: 27.591829\n",
      "step 544, loss: 27.512291\n",
      "step 545, loss: 27.432970\n",
      "step 546, loss: 27.353865\n",
      "step 547, loss: 27.274971\n",
      "step 548, loss: 27.196289\n",
      "step 549, loss: 27.117821\n",
      "step 550, loss: 27.039574\n",
      "step 551, loss: 26.961536\n",
      "step 552, loss: 26.883711\n",
      "step 553, loss: 26.806093\n",
      "step 554, loss: 26.728689\n",
      "step 555, loss: 26.651501\n",
      "step 556, loss: 26.574520\n",
      "step 557, loss: 26.497749\n",
      "step 558, loss: 26.421185\n",
      "step 559, loss: 26.344833\n",
      "step 560, loss: 26.268688\n",
      "step 561, loss: 26.192751\n",
      "step 562, loss: 26.117023\n",
      "step 563, loss: 26.041504\n",
      "step 564, loss: 25.966190\n",
      "step 565, loss: 25.891079\n",
      "step 566, loss: 25.816177\n",
      "step 567, loss: 25.741474\n",
      "step 568, loss: 25.666977\n",
      "step 569, loss: 25.592691\n",
      "step 570, loss: 25.518600\n",
      "step 571, loss: 25.444721\n",
      "step 572, loss: 25.371044\n",
      "step 573, loss: 25.297562\n",
      "step 574, loss: 25.224285\n",
      "step 575, loss: 25.151211\n",
      "step 576, loss: 25.078337\n",
      "step 577, loss: 25.005661\n",
      "step 578, loss: 24.933189\n",
      "step 579, loss: 24.860912\n",
      "step 580, loss: 24.788836\n",
      "step 581, loss: 24.716959\n",
      "step 582, loss: 24.645279\n",
      "step 583, loss: 24.573795\n",
      "step 584, loss: 24.502508\n",
      "step 585, loss: 24.431421\n",
      "step 586, loss: 24.360531\n",
      "step 587, loss: 24.289833\n",
      "step 588, loss: 24.219330\n",
      "step 589, loss: 24.149023\n",
      "step 590, loss: 24.078907\n",
      "step 591, loss: 24.008987\n",
      "step 592, loss: 23.939262\n",
      "step 593, loss: 23.869728\n",
      "step 594, loss: 23.800390\n",
      "step 595, loss: 23.731239\n",
      "step 596, loss: 23.662281\n",
      "step 597, loss: 23.593515\n",
      "step 598, loss: 23.524937\n",
      "step 599, loss: 23.456554\n",
      "step 600, loss: 23.388355\n",
      "step 601, loss: 23.320349\n",
      "step 602, loss: 23.252529\n",
      "step 603, loss: 23.184900\n",
      "step 604, loss: 23.117460\n",
      "step 605, loss: 23.050201\n",
      "step 606, loss: 22.983131\n",
      "step 607, loss: 22.916248\n",
      "step 608, loss: 22.849556\n",
      "step 609, loss: 22.783045\n",
      "step 610, loss: 22.716717\n",
      "step 611, loss: 22.650578\n",
      "step 612, loss: 22.584621\n",
      "step 613, loss: 22.518847\n",
      "step 614, loss: 22.453260\n",
      "step 615, loss: 22.387854\n",
      "step 616, loss: 22.322630\n",
      "step 617, loss: 22.257587\n",
      "step 618, loss: 22.192726\n",
      "step 619, loss: 22.128048\n",
      "step 620, loss: 22.063547\n",
      "step 621, loss: 21.999229\n",
      "step 622, loss: 21.935089\n",
      "step 623, loss: 21.871130\n",
      "step 624, loss: 21.807352\n",
      "step 625, loss: 21.743752\n",
      "step 626, loss: 21.680326\n",
      "step 627, loss: 21.617083\n",
      "step 628, loss: 21.554012\n",
      "step 629, loss: 21.491123\n",
      "step 630, loss: 21.428406\n",
      "step 631, loss: 21.365866\n",
      "step 632, loss: 21.303501\n",
      "step 633, loss: 21.241310\n",
      "step 634, loss: 21.179296\n",
      "step 635, loss: 21.117453\n",
      "step 636, loss: 21.055788\n",
      "step 637, loss: 20.994293\n",
      "step 638, loss: 20.932976\n",
      "step 639, loss: 20.871822\n",
      "step 640, loss: 20.810848\n",
      "step 641, loss: 20.750046\n",
      "step 642, loss: 20.689413\n",
      "step 643, loss: 20.628952\n",
      "step 644, loss: 20.568657\n",
      "step 645, loss: 20.508535\n",
      "step 646, loss: 20.448582\n",
      "step 647, loss: 20.388798\n",
      "step 648, loss: 20.329185\n",
      "step 649, loss: 20.269739\n",
      "step 650, loss: 20.210464\n",
      "step 651, loss: 20.151352\n",
      "step 652, loss: 20.092403\n",
      "step 653, loss: 20.033627\n",
      "step 654, loss: 19.975016\n",
      "step 655, loss: 19.916573\n",
      "step 656, loss: 19.858292\n",
      "step 657, loss: 19.800179\n",
      "step 658, loss: 19.742226\n",
      "step 659, loss: 19.684441\n",
      "step 660, loss: 19.626820\n",
      "step 661, loss: 19.569359\n",
      "step 662, loss: 19.512068\n",
      "step 663, loss: 19.454931\n",
      "step 664, loss: 19.397961\n",
      "step 665, loss: 19.341152\n",
      "step 666, loss: 19.284504\n",
      "step 667, loss: 19.228016\n",
      "step 668, loss: 19.171692\n",
      "step 669, loss: 19.115524\n",
      "step 670, loss: 19.059517\n",
      "step 671, loss: 19.003670\n",
      "step 672, loss: 18.947981\n",
      "step 673, loss: 18.892452\n",
      "step 674, loss: 18.837078\n",
      "step 675, loss: 18.781866\n",
      "step 676, loss: 18.726810\n",
      "step 677, loss: 18.671909\n",
      "step 678, loss: 18.617165\n",
      "step 679, loss: 18.562580\n",
      "step 680, loss: 18.508148\n",
      "step 681, loss: 18.453873\n",
      "step 682, loss: 18.399750\n",
      "step 683, loss: 18.345783\n",
      "step 684, loss: 18.291973\n",
      "step 685, loss: 18.238316\n",
      "step 686, loss: 18.184811\n",
      "step 687, loss: 18.131454\n",
      "step 688, loss: 18.078260\n",
      "step 689, loss: 18.025215\n",
      "step 690, loss: 17.972317\n",
      "step 691, loss: 17.919579\n",
      "step 692, loss: 17.866982\n",
      "step 693, loss: 17.814543\n",
      "step 694, loss: 17.762253\n",
      "step 695, loss: 17.710114\n",
      "step 696, loss: 17.658123\n",
      "step 697, loss: 17.606281\n",
      "step 698, loss: 17.554586\n",
      "step 699, loss: 17.503044\n",
      "step 700, loss: 17.451649\n",
      "step 701, loss: 17.400398\n",
      "step 702, loss: 17.349297\n",
      "step 703, loss: 17.298344\n",
      "step 704, loss: 17.247540\n",
      "step 705, loss: 17.196878\n",
      "step 706, loss: 17.146366\n",
      "step 707, loss: 17.095995\n",
      "step 708, loss: 17.045773\n",
      "step 709, loss: 16.995693\n",
      "step 710, loss: 16.945755\n",
      "step 711, loss: 16.895966\n",
      "step 712, loss: 16.846317\n",
      "step 713, loss: 16.796814\n",
      "step 714, loss: 16.747454\n",
      "step 715, loss: 16.698233\n",
      "step 716, loss: 16.649158\n",
      "step 717, loss: 16.600224\n",
      "step 718, loss: 16.551430\n",
      "step 719, loss: 16.502777\n",
      "step 720, loss: 16.454268\n",
      "step 721, loss: 16.405895\n",
      "step 722, loss: 16.357668\n",
      "step 723, loss: 16.309576\n",
      "step 724, loss: 16.261623\n",
      "step 725, loss: 16.213810\n",
      "step 726, loss: 16.166138\n",
      "step 727, loss: 16.118605\n",
      "step 728, loss: 16.071207\n",
      "step 729, loss: 16.023947\n",
      "step 730, loss: 15.976826\n",
      "step 731, loss: 15.929837\n",
      "step 732, loss: 15.882990\n",
      "step 733, loss: 15.836276\n",
      "step 734, loss: 15.789700\n",
      "step 735, loss: 15.743258\n",
      "step 736, loss: 15.696950\n",
      "step 737, loss: 15.650778\n",
      "step 738, loss: 15.604741\n",
      "step 739, loss: 15.558834\n",
      "step 740, loss: 15.513067\n",
      "step 741, loss: 15.467427\n",
      "step 742, loss: 15.421926\n",
      "step 743, loss: 15.376554\n",
      "step 744, loss: 15.331317\n",
      "step 745, loss: 15.286211\n",
      "step 746, loss: 15.241235\n",
      "step 747, loss: 15.196392\n",
      "step 748, loss: 15.151680\n",
      "step 749, loss: 15.107098\n",
      "step 750, loss: 15.062644\n",
      "step 751, loss: 15.018322\n",
      "step 752, loss: 14.974133\n",
      "step 753, loss: 14.930067\n",
      "step 754, loss: 14.886133\n",
      "step 755, loss: 14.842329\n",
      "step 756, loss: 14.798656\n",
      "step 757, loss: 14.755106\n",
      "step 758, loss: 14.711686\n",
      "step 759, loss: 14.668392\n",
      "step 760, loss: 14.625226\n",
      "step 761, loss: 14.582184\n",
      "step 762, loss: 14.539268\n",
      "step 763, loss: 14.496480\n",
      "step 764, loss: 14.453819\n",
      "step 765, loss: 14.411281\n",
      "step 766, loss: 14.368870\n",
      "step 767, loss: 14.326582\n",
      "step 768, loss: 14.284418\n",
      "step 769, loss: 14.242380\n",
      "step 770, loss: 14.200462\n",
      "step 771, loss: 14.158671\n",
      "step 772, loss: 14.117002\n",
      "step 773, loss: 14.075457\n",
      "step 774, loss: 14.034033\n",
      "step 775, loss: 13.992729\n",
      "step 776, loss: 13.951550\n",
      "step 777, loss: 13.910492\n",
      "step 778, loss: 13.869553\n",
      "step 779, loss: 13.828735\n",
      "step 780, loss: 13.788037\n",
      "step 781, loss: 13.747463\n",
      "step 782, loss: 13.707006\n",
      "step 783, loss: 13.666669\n",
      "step 784, loss: 13.626450\n",
      "step 785, loss: 13.586349\n",
      "step 786, loss: 13.546371\n",
      "step 787, loss: 13.506509\n",
      "step 788, loss: 13.466764\n",
      "step 789, loss: 13.427137\n",
      "step 790, loss: 13.387629\n",
      "step 791, loss: 13.348236\n",
      "step 792, loss: 13.308963\n",
      "step 793, loss: 13.269802\n",
      "step 794, loss: 13.230761\n",
      "step 795, loss: 13.191833\n",
      "step 796, loss: 13.153021\n",
      "step 797, loss: 13.114325\n",
      "step 798, loss: 13.075745\n",
      "step 799, loss: 13.037277\n",
      "step 800, loss: 12.998924\n",
      "step 801, loss: 12.960686\n",
      "step 802, loss: 12.922562\n",
      "step 803, loss: 12.884548\n",
      "step 804, loss: 12.846650\n",
      "step 805, loss: 12.808864\n",
      "step 806, loss: 12.771189\n",
      "step 807, loss: 12.733627\n",
      "step 808, loss: 12.696178\n",
      "step 809, loss: 12.658838\n",
      "step 810, loss: 12.621613\n",
      "step 811, loss: 12.584496\n",
      "step 812, loss: 12.547493\n",
      "step 813, loss: 12.510597\n",
      "step 814, loss: 12.473811\n",
      "step 815, loss: 12.437138\n",
      "step 816, loss: 12.400573\n",
      "step 817, loss: 12.364114\n",
      "step 818, loss: 12.327768\n",
      "step 819, loss: 12.291530\n",
      "step 820, loss: 12.255400\n",
      "step 821, loss: 12.219374\n",
      "step 822, loss: 12.183461\n",
      "step 823, loss: 12.147654\n",
      "step 824, loss: 12.111954\n",
      "step 825, loss: 12.076359\n",
      "step 826, loss: 12.040874\n",
      "step 827, loss: 12.005491\n",
      "step 828, loss: 11.970219\n",
      "step 829, loss: 11.935051\n",
      "step 830, loss: 11.899986\n",
      "step 831, loss: 11.865028\n",
      "step 832, loss: 11.830176\n",
      "step 833, loss: 11.795425\n",
      "step 834, loss: 11.760781\n",
      "step 835, loss: 11.726242\n",
      "step 836, loss: 11.691804\n",
      "step 837, loss: 11.657471\n",
      "step 838, loss: 11.623240\n",
      "step 839, loss: 11.589115\n",
      "step 840, loss: 11.555088\n",
      "step 841, loss: 11.521166\n",
      "step 842, loss: 11.487347\n",
      "step 843, loss: 11.453630\n",
      "step 844, loss: 11.420012\n",
      "step 845, loss: 11.386497\n",
      "step 846, loss: 11.353083\n",
      "step 847, loss: 11.319767\n",
      "step 848, loss: 11.286553\n",
      "step 849, loss: 11.253441\n",
      "step 850, loss: 11.220428\n",
      "step 851, loss: 11.187514\n",
      "step 852, loss: 11.154699\n",
      "step 853, loss: 11.121983\n",
      "step 854, loss: 11.089367\n",
      "step 855, loss: 11.056849\n",
      "step 856, loss: 11.024427\n",
      "step 857, loss: 10.992105\n",
      "step 858, loss: 10.959881\n",
      "step 859, loss: 10.927753\n",
      "step 860, loss: 10.895723\n",
      "step 861, loss: 10.863791\n",
      "step 862, loss: 10.831955\n",
      "step 863, loss: 10.800214\n",
      "step 864, loss: 10.768570\n",
      "step 865, loss: 10.737023\n",
      "step 866, loss: 10.705572\n",
      "step 867, loss: 10.674213\n",
      "step 868, loss: 10.642952\n",
      "step 869, loss: 10.611787\n",
      "step 870, loss: 10.580713\n",
      "step 871, loss: 10.549734\n",
      "step 872, loss: 10.518850\n",
      "step 873, loss: 10.488060\n",
      "step 874, loss: 10.457365\n",
      "step 875, loss: 10.426760\n",
      "step 876, loss: 10.396251\n",
      "step 877, loss: 10.365833\n",
      "step 878, loss: 10.335509\n",
      "step 879, loss: 10.305275\n",
      "step 880, loss: 10.275135\n",
      "step 881, loss: 10.245087\n",
      "step 882, loss: 10.215128\n",
      "step 883, loss: 10.185263\n",
      "step 884, loss: 10.155487\n",
      "step 885, loss: 10.125801\n",
      "step 886, loss: 10.096209\n",
      "step 887, loss: 10.066704\n",
      "step 888, loss: 10.037290\n",
      "step 889, loss: 10.007965\n",
      "step 890, loss: 9.978731\n",
      "step 891, loss: 9.949585\n",
      "step 892, loss: 9.920528\n",
      "step 893, loss: 9.891560\n",
      "step 894, loss: 9.862680\n",
      "step 895, loss: 9.833891\n",
      "step 896, loss: 9.805187\n",
      "step 897, loss: 9.776571\n",
      "step 898, loss: 9.748045\n",
      "step 899, loss: 9.719603\n",
      "step 900, loss: 9.691252\n",
      "step 901, loss: 9.662983\n",
      "step 902, loss: 9.634804\n",
      "step 903, loss: 9.606710\n",
      "step 904, loss: 9.578703\n",
      "step 905, loss: 9.550781\n",
      "step 906, loss: 9.522944\n",
      "step 907, loss: 9.495192\n",
      "step 908, loss: 9.467527\n",
      "step 909, loss: 9.439947\n",
      "step 910, loss: 9.412451\n",
      "step 911, loss: 9.385038\n",
      "step 912, loss: 9.357710\n",
      "step 913, loss: 9.330467\n",
      "step 914, loss: 9.303306\n",
      "step 915, loss: 9.276228\n",
      "step 916, loss: 9.249235\n",
      "step 917, loss: 9.222324\n",
      "step 918, loss: 9.195498\n",
      "step 919, loss: 9.168753\n",
      "step 920, loss: 9.142089\n",
      "step 921, loss: 9.115509\n",
      "step 922, loss: 9.089010\n",
      "step 923, loss: 9.062592\n",
      "step 924, loss: 9.036257\n",
      "step 925, loss: 9.010001\n",
      "step 926, loss: 8.983826\n",
      "step 927, loss: 8.957733\n",
      "step 928, loss: 8.931721\n",
      "step 929, loss: 8.905788\n",
      "step 930, loss: 8.879938\n",
      "step 931, loss: 8.854164\n",
      "step 932, loss: 8.828470\n",
      "step 933, loss: 8.802856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 934, loss: 8.777322\n",
      "step 935, loss: 8.751865\n",
      "step 936, loss: 8.726489\n",
      "step 937, loss: 8.701189\n",
      "step 938, loss: 8.675969\n",
      "step 939, loss: 8.650826\n",
      "step 940, loss: 8.625760\n",
      "step 941, loss: 8.600774\n",
      "step 942, loss: 8.575863\n",
      "step 943, loss: 8.551030\n",
      "step 944, loss: 8.526274\n",
      "step 945, loss: 8.501593\n",
      "step 946, loss: 8.476992\n",
      "step 947, loss: 8.452464\n",
      "step 948, loss: 8.428015\n",
      "step 949, loss: 8.403641\n",
      "step 950, loss: 8.379341\n",
      "step 951, loss: 8.355120\n",
      "step 952, loss: 8.330971\n",
      "step 953, loss: 8.306896\n",
      "step 954, loss: 8.282898\n",
      "step 955, loss: 8.258974\n",
      "step 956, loss: 8.235124\n",
      "step 957, loss: 8.211349\n",
      "step 958, loss: 8.187646\n",
      "step 959, loss: 8.164017\n",
      "step 960, loss: 8.140463\n",
      "step 961, loss: 8.116982\n",
      "step 962, loss: 8.093574\n",
      "step 963, loss: 8.070237\n",
      "step 964, loss: 8.046973\n",
      "step 965, loss: 8.023784\n",
      "step 966, loss: 8.000666\n",
      "step 967, loss: 7.977620\n",
      "step 968, loss: 7.954645\n",
      "step 969, loss: 7.931743\n",
      "step 970, loss: 7.908913\n",
      "step 971, loss: 7.886152\n",
      "step 972, loss: 7.863463\n",
      "step 973, loss: 7.840846\n",
      "step 974, loss: 7.818298\n",
      "step 975, loss: 7.795820\n",
      "step 976, loss: 7.773414\n",
      "step 977, loss: 7.751077\n",
      "step 978, loss: 7.728809\n",
      "step 979, loss: 7.706613\n",
      "step 980, loss: 7.684484\n",
      "step 981, loss: 7.662424\n",
      "step 982, loss: 7.640434\n",
      "step 983, loss: 7.618514\n",
      "step 984, loss: 7.596660\n",
      "step 985, loss: 7.574875\n",
      "step 986, loss: 7.553160\n",
      "step 987, loss: 7.531511\n",
      "step 988, loss: 7.509931\n",
      "step 989, loss: 7.488420\n",
      "step 990, loss: 7.466973\n",
      "step 991, loss: 7.445595\n",
      "step 992, loss: 7.424284\n",
      "step 993, loss: 7.403039\n",
      "step 994, loss: 7.381861\n",
      "step 995, loss: 7.360751\n",
      "step 996, loss: 7.339706\n",
      "step 997, loss: 7.318727\n",
      "step 998, loss: 7.297814\n",
      "step 999, loss: 7.276966\n",
      "step 1000, loss: 7.256185\n",
      "step 1001, loss: 7.235467\n",
      "step 1002, loss: 7.214817\n",
      "step 1003, loss: 7.194230\n",
      "step 1004, loss: 7.173708\n",
      "step 1005, loss: 7.153250\n",
      "step 1006, loss: 7.132856\n",
      "step 1007, loss: 7.112528\n",
      "step 1008, loss: 7.092263\n",
      "step 1009, loss: 7.072062\n",
      "step 1010, loss: 7.051924\n",
      "step 1011, loss: 7.031849\n",
      "step 1012, loss: 7.011839\n",
      "step 1013, loss: 6.991891\n",
      "step 1014, loss: 6.972006\n",
      "step 1015, loss: 6.952183\n",
      "step 1016, loss: 6.932424\n",
      "step 1017, loss: 6.912726\n",
      "step 1018, loss: 6.893090\n",
      "step 1019, loss: 6.873517\n",
      "step 1020, loss: 6.854004\n",
      "step 1021, loss: 6.834554\n",
      "step 1022, loss: 6.815165\n",
      "step 1023, loss: 6.795837\n",
      "step 1024, loss: 6.776571\n",
      "step 1025, loss: 6.757366\n",
      "step 1026, loss: 6.738220\n",
      "step 1027, loss: 6.719135\n",
      "step 1028, loss: 6.700112\n",
      "step 1029, loss: 6.681148\n",
      "step 1030, loss: 6.662245\n",
      "step 1031, loss: 6.643400\n",
      "step 1032, loss: 6.624616\n",
      "step 1033, loss: 6.605890\n",
      "step 1034, loss: 6.587225\n",
      "step 1035, loss: 6.568617\n",
      "step 1036, loss: 6.550070\n",
      "step 1037, loss: 6.531580\n",
      "step 1038, loss: 6.513150\n",
      "step 1039, loss: 6.494777\n",
      "step 1040, loss: 6.476463\n",
      "step 1041, loss: 6.458206\n",
      "step 1042, loss: 6.440008\n",
      "step 1043, loss: 6.421868\n",
      "step 1044, loss: 6.403785\n",
      "step 1045, loss: 6.385759\n",
      "step 1046, loss: 6.367790\n",
      "step 1047, loss: 6.349879\n",
      "step 1048, loss: 6.332026\n",
      "step 1049, loss: 6.314227\n",
      "step 1050, loss: 6.296486\n",
      "step 1051, loss: 6.278802\n",
      "step 1052, loss: 6.261173\n",
      "step 1053, loss: 6.243600\n",
      "step 1054, loss: 6.226084\n",
      "step 1055, loss: 6.208622\n",
      "step 1056, loss: 6.191218\n",
      "step 1057, loss: 6.173868\n",
      "step 1058, loss: 6.156571\n",
      "step 1059, loss: 6.139333\n",
      "step 1060, loss: 6.122148\n",
      "step 1061, loss: 6.105018\n",
      "step 1062, loss: 6.087942\n",
      "step 1063, loss: 6.070920\n",
      "step 1064, loss: 6.053954\n",
      "step 1065, loss: 6.037040\n",
      "step 1066, loss: 6.020181\n",
      "step 1067, loss: 6.003375\n",
      "step 1068, loss: 5.986623\n",
      "step 1069, loss: 5.969924\n",
      "step 1070, loss: 5.953279\n",
      "step 1071, loss: 5.936687\n",
      "step 1072, loss: 5.920148\n",
      "step 1073, loss: 5.903662\n",
      "step 1074, loss: 5.887228\n",
      "step 1075, loss: 5.870846\n",
      "step 1076, loss: 5.854518\n",
      "step 1077, loss: 5.838240\n",
      "step 1078, loss: 5.822015\n",
      "step 1079, loss: 5.805842\n",
      "step 1080, loss: 5.789721\n",
      "step 1081, loss: 5.773650\n",
      "step 1082, loss: 5.757631\n",
      "step 1083, loss: 5.741664\n",
      "step 1084, loss: 5.725747\n",
      "step 1085, loss: 5.709882\n",
      "step 1086, loss: 5.694068\n",
      "step 1087, loss: 5.678303\n",
      "step 1088, loss: 5.662589\n",
      "step 1089, loss: 5.646926\n",
      "step 1090, loss: 5.631313\n",
      "step 1091, loss: 5.615751\n",
      "step 1092, loss: 5.600236\n",
      "step 1093, loss: 5.584773\n",
      "step 1094, loss: 5.569357\n",
      "step 1095, loss: 5.553993\n",
      "step 1096, loss: 5.538678\n",
      "step 1097, loss: 5.523412\n",
      "step 1098, loss: 5.508194\n",
      "step 1099, loss: 5.493026\n",
      "step 1100, loss: 5.477906\n",
      "step 1101, loss: 5.462835\n",
      "step 1102, loss: 5.447812\n",
      "step 1103, loss: 5.432837\n",
      "step 1104, loss: 5.417910\n",
      "step 1105, loss: 5.403031\n",
      "step 1106, loss: 5.388200\n",
      "step 1107, loss: 5.373416\n",
      "step 1108, loss: 5.358680\n",
      "step 1109, loss: 5.343991\n",
      "step 1110, loss: 5.329350\n",
      "step 1111, loss: 5.314755\n",
      "step 1112, loss: 5.300207\n",
      "step 1113, loss: 5.285707\n",
      "step 1114, loss: 5.271252\n",
      "step 1115, loss: 5.256845\n",
      "step 1116, loss: 5.242483\n",
      "step 1117, loss: 5.228168\n",
      "step 1118, loss: 5.213899\n",
      "step 1119, loss: 5.199677\n",
      "step 1120, loss: 5.185499\n",
      "step 1121, loss: 5.171366\n",
      "step 1122, loss: 5.157280\n",
      "step 1123, loss: 5.143240\n",
      "step 1124, loss: 5.129244\n",
      "step 1125, loss: 5.115294\n",
      "step 1126, loss: 5.101388\n",
      "step 1127, loss: 5.087526\n",
      "step 1128, loss: 5.073711\n",
      "step 1129, loss: 5.059938\n",
      "step 1130, loss: 5.046211\n",
      "step 1131, loss: 5.032528\n",
      "step 1132, loss: 5.018888\n",
      "step 1133, loss: 5.005294\n",
      "step 1134, loss: 4.991743\n",
      "step 1135, loss: 4.978236\n",
      "step 1136, loss: 4.964772\n",
      "step 1137, loss: 4.951352\n",
      "step 1138, loss: 4.937974\n",
      "step 1139, loss: 4.924641\n",
      "step 1140, loss: 4.911349\n",
      "step 1141, loss: 4.898102\n",
      "step 1142, loss: 4.884897\n",
      "step 1143, loss: 4.871734\n",
      "step 1144, loss: 4.858614\n",
      "step 1145, loss: 4.845536\n",
      "step 1146, loss: 4.832501\n",
      "step 1147, loss: 4.819507\n",
      "step 1148, loss: 4.806557\n",
      "step 1149, loss: 4.793647\n",
      "step 1150, loss: 4.780780\n",
      "step 1151, loss: 4.767954\n",
      "step 1152, loss: 4.755168\n",
      "step 1153, loss: 4.742426\n",
      "step 1154, loss: 4.729724\n",
      "step 1155, loss: 4.717063\n",
      "step 1156, loss: 4.704443\n",
      "step 1157, loss: 4.691864\n",
      "step 1158, loss: 4.679326\n",
      "step 1159, loss: 4.666828\n",
      "step 1160, loss: 4.654372\n",
      "step 1161, loss: 4.641954\n",
      "step 1162, loss: 4.629578\n",
      "step 1163, loss: 4.617241\n",
      "step 1164, loss: 4.604944\n",
      "step 1165, loss: 4.592688\n",
      "step 1166, loss: 4.580471\n",
      "step 1167, loss: 4.568294\n",
      "step 1168, loss: 4.556155\n",
      "step 1169, loss: 4.544057\n",
      "step 1170, loss: 4.531999\n",
      "step 1171, loss: 4.519978\n",
      "step 1172, loss: 4.507998\n",
      "step 1173, loss: 4.496055\n",
      "step 1174, loss: 4.484150\n",
      "step 1175, loss: 4.472286\n",
      "step 1176, loss: 4.460460\n",
      "step 1177, loss: 4.448671\n",
      "step 1178, loss: 4.436922\n",
      "step 1179, loss: 4.425210\n",
      "step 1180, loss: 4.413536\n",
      "step 1181, loss: 4.401901\n",
      "step 1182, loss: 4.390302\n",
      "step 1183, loss: 4.378742\n",
      "step 1184, loss: 4.367219\n",
      "step 1185, loss: 4.355733\n",
      "step 1186, loss: 4.344286\n",
      "step 1187, loss: 4.332874\n",
      "step 1188, loss: 4.321500\n",
      "step 1189, loss: 4.310163\n",
      "step 1190, loss: 4.298864\n",
      "step 1191, loss: 4.287601\n",
      "step 1192, loss: 4.276374\n",
      "step 1193, loss: 4.265183\n",
      "step 1194, loss: 4.254030\n",
      "step 1195, loss: 4.242913\n",
      "step 1196, loss: 4.231832\n",
      "step 1197, loss: 4.220787\n",
      "step 1198, loss: 4.209776\n",
      "step 1199, loss: 4.198804\n",
      "step 1200, loss: 4.187866\n",
      "step 1201, loss: 4.176964\n",
      "step 1202, loss: 4.166097\n",
      "step 1203, loss: 4.155267\n",
      "step 1204, loss: 4.144470\n",
      "step 1205, loss: 4.133709\n",
      "step 1206, loss: 4.122984\n",
      "step 1207, loss: 4.112293\n",
      "step 1208, loss: 4.101636\n",
      "step 1209, loss: 4.091016\n",
      "step 1210, loss: 4.080429\n",
      "step 1211, loss: 4.069877\n",
      "step 1212, loss: 4.059359\n",
      "step 1213, loss: 4.048875\n",
      "step 1214, loss: 4.038425\n",
      "step 1215, loss: 4.028011\n",
      "step 1216, loss: 4.017629\n",
      "step 1217, loss: 4.007281\n",
      "step 1218, loss: 3.996967\n",
      "step 1219, loss: 3.986688\n",
      "step 1220, loss: 3.976441\n",
      "step 1221, loss: 3.966227\n",
      "step 1222, loss: 3.956048\n",
      "step 1223, loss: 3.945901\n",
      "step 1224, loss: 3.935788\n",
      "step 1225, loss: 3.925707\n",
      "step 1226, loss: 3.915659\n",
      "step 1227, loss: 3.905645\n",
      "step 1228, loss: 3.895662\n",
      "step 1229, loss: 3.885713\n",
      "step 1230, loss: 3.875795\n",
      "step 1231, loss: 3.865910\n",
      "step 1232, loss: 3.856058\n",
      "step 1233, loss: 3.846239\n",
      "step 1234, loss: 3.836450\n",
      "step 1235, loss: 3.826694\n",
      "step 1236, loss: 3.816969\n",
      "step 1237, loss: 3.807277\n",
      "step 1238, loss: 3.797615\n",
      "step 1239, loss: 3.787986\n",
      "step 1240, loss: 3.778388\n",
      "step 1241, loss: 3.768821\n",
      "step 1242, loss: 3.759285\n",
      "step 1243, loss: 3.749782\n",
      "step 1244, loss: 3.740309\n",
      "step 1245, loss: 3.730866\n",
      "step 1246, loss: 3.721455\n",
      "step 1247, loss: 3.712074\n",
      "step 1248, loss: 3.702725\n",
      "step 1249, loss: 3.693405\n",
      "step 1250, loss: 3.684116\n",
      "step 1251, loss: 3.674858\n",
      "step 1252, loss: 3.665630\n",
      "step 1253, loss: 3.656432\n",
      "step 1254, loss: 3.647264\n",
      "step 1255, loss: 3.638126\n",
      "step 1256, loss: 3.629018\n",
      "step 1257, loss: 3.619940\n",
      "step 1258, loss: 3.610891\n",
      "step 1259, loss: 3.601871\n",
      "step 1260, loss: 3.592883\n",
      "step 1261, loss: 3.583922\n",
      "step 1262, loss: 3.574993\n",
      "step 1263, loss: 3.566090\n",
      "step 1264, loss: 3.557218\n",
      "step 1265, loss: 3.548376\n",
      "step 1266, loss: 3.539561\n",
      "step 1267, loss: 3.530775\n",
      "step 1268, loss: 3.522018\n",
      "step 1269, loss: 3.513290\n",
      "step 1270, loss: 3.504591\n",
      "step 1271, loss: 3.495920\n",
      "step 1272, loss: 3.487278\n",
      "step 1273, loss: 3.478664\n",
      "step 1274, loss: 3.470078\n",
      "step 1275, loss: 3.461520\n",
      "step 1276, loss: 3.452990\n",
      "step 1277, loss: 3.444487\n",
      "step 1278, loss: 3.436013\n",
      "step 1279, loss: 3.427567\n",
      "step 1280, loss: 3.419149\n",
      "step 1281, loss: 3.410757\n",
      "step 1282, loss: 3.402393\n",
      "step 1283, loss: 3.394058\n",
      "step 1284, loss: 3.385749\n",
      "step 1285, loss: 3.377468\n",
      "step 1286, loss: 3.369213\n",
      "step 1287, loss: 3.360985\n",
      "step 1288, loss: 3.352785\n",
      "step 1289, loss: 3.344611\n",
      "step 1290, loss: 3.336465\n",
      "step 1291, loss: 3.328345\n",
      "step 1292, loss: 3.320251\n",
      "step 1293, loss: 3.312185\n",
      "step 1294, loss: 3.304144\n",
      "step 1295, loss: 3.296130\n",
      "step 1296, loss: 3.288142\n",
      "step 1297, loss: 3.280180\n",
      "step 1298, loss: 3.272246\n",
      "step 1299, loss: 3.264336\n",
      "step 1300, loss: 3.256452\n",
      "step 1301, loss: 3.248595\n",
      "step 1302, loss: 3.240763\n",
      "step 1303, loss: 3.232956\n",
      "step 1304, loss: 3.225175\n",
      "step 1305, loss: 3.217421\n",
      "step 1306, loss: 3.209691\n",
      "step 1307, loss: 3.201987\n",
      "step 1308, loss: 3.194308\n",
      "step 1309, loss: 3.186654\n",
      "step 1310, loss: 3.179026\n",
      "step 1311, loss: 3.171421\n",
      "step 1312, loss: 3.163844\n",
      "step 1313, loss: 3.156290\n",
      "step 1314, loss: 3.148760\n",
      "step 1315, loss: 3.141255\n",
      "step 1316, loss: 3.133776\n",
      "step 1317, loss: 3.126322\n",
      "step 1318, loss: 3.118891\n",
      "step 1319, loss: 3.111484\n",
      "step 1320, loss: 3.104102\n",
      "step 1321, loss: 3.096745\n",
      "step 1322, loss: 3.089411\n",
      "step 1323, loss: 3.082102\n",
      "step 1324, loss: 3.074816\n",
      "step 1325, loss: 3.067554\n",
      "step 1326, loss: 3.060317\n",
      "step 1327, loss: 3.053102\n",
      "step 1328, loss: 3.045913\n",
      "step 1329, loss: 3.038746\n",
      "step 1330, loss: 3.031602\n",
      "step 1331, loss: 3.024482\n",
      "step 1332, loss: 3.017387\n",
      "step 1333, loss: 3.010314\n",
      "step 1334, loss: 3.003263\n",
      "step 1335, loss: 2.996237\n",
      "step 1336, loss: 2.989234\n",
      "step 1337, loss: 2.982253\n",
      "step 1338, loss: 2.975296\n",
      "step 1339, loss: 2.968359\n",
      "step 1340, loss: 2.961449\n",
      "step 1341, loss: 2.954559\n",
      "step 1342, loss: 2.947692\n",
      "step 1343, loss: 2.940848\n",
      "step 1344, loss: 2.934026\n",
      "step 1345, loss: 2.927227\n",
      "step 1346, loss: 2.920450\n",
      "step 1347, loss: 2.913695\n",
      "step 1348, loss: 2.906962\n",
      "step 1349, loss: 2.900252\n",
      "step 1350, loss: 2.893563\n",
      "step 1351, loss: 2.886897\n",
      "step 1352, loss: 2.880252\n",
      "step 1353, loss: 2.873630\n",
      "step 1354, loss: 2.867030\n",
      "step 1355, loss: 2.860450\n",
      "step 1356, loss: 2.853893\n",
      "step 1357, loss: 2.847357\n",
      "step 1358, loss: 2.840842\n",
      "step 1359, loss: 2.834349\n",
      "step 1360, loss: 2.827877\n",
      "step 1361, loss: 2.821426\n",
      "step 1362, loss: 2.814996\n",
      "step 1363, loss: 2.808589\n",
      "step 1364, loss: 2.802201\n",
      "step 1365, loss: 2.795835\n",
      "step 1366, loss: 2.789490\n",
      "step 1367, loss: 2.783165\n",
      "step 1368, loss: 2.776861\n",
      "step 1369, loss: 2.770578\n",
      "step 1370, loss: 2.764316\n",
      "step 1371, loss: 2.758073\n",
      "step 1372, loss: 2.751852\n",
      "step 1373, loss: 2.745652\n",
      "step 1374, loss: 2.739470\n",
      "step 1375, loss: 2.733310\n",
      "step 1376, loss: 2.727171\n",
      "step 1377, loss: 2.721051\n",
      "step 1378, loss: 2.714952\n",
      "step 1379, loss: 2.708872\n",
      "step 1380, loss: 2.702812\n",
      "step 1381, loss: 2.696772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1382, loss: 2.690752\n",
      "step 1383, loss: 2.684753\n",
      "step 1384, loss: 2.678771\n",
      "step 1385, loss: 2.672810\n",
      "step 1386, loss: 2.666870\n",
      "step 1387, loss: 2.660948\n",
      "step 1388, loss: 2.655046\n",
      "step 1389, loss: 2.649162\n",
      "step 1390, loss: 2.643299\n",
      "step 1391, loss: 2.637455\n",
      "step 1392, loss: 2.631629\n",
      "step 1393, loss: 2.625824\n",
      "step 1394, loss: 2.620037\n",
      "step 1395, loss: 2.614269\n",
      "step 1396, loss: 2.608520\n",
      "step 1397, loss: 2.602790\n",
      "step 1398, loss: 2.597079\n",
      "step 1399, loss: 2.591387\n",
      "step 1400, loss: 2.585712\n",
      "step 1401, loss: 2.580057\n",
      "step 1402, loss: 2.574421\n",
      "step 1403, loss: 2.568803\n",
      "step 1404, loss: 2.563204\n",
      "step 1405, loss: 2.557622\n",
      "step 1406, loss: 2.552059\n",
      "step 1407, loss: 2.546515\n",
      "step 1408, loss: 2.540987\n",
      "step 1409, loss: 2.535480\n",
      "step 1410, loss: 2.529990\n",
      "step 1411, loss: 2.524517\n",
      "step 1412, loss: 2.519063\n",
      "step 1413, loss: 2.513627\n",
      "step 1414, loss: 2.508209\n",
      "step 1415, loss: 2.502808\n",
      "step 1416, loss: 2.497425\n",
      "step 1417, loss: 2.492059\n",
      "step 1418, loss: 2.486712\n",
      "step 1419, loss: 2.481381\n",
      "step 1420, loss: 2.476069\n",
      "step 1421, loss: 2.470775\n",
      "step 1422, loss: 2.465497\n",
      "step 1423, loss: 2.460236\n",
      "step 1424, loss: 2.454992\n",
      "step 1425, loss: 2.449767\n",
      "step 1426, loss: 2.444559\n",
      "step 1427, loss: 2.439367\n",
      "step 1428, loss: 2.434192\n",
      "step 1429, loss: 2.429034\n",
      "step 1430, loss: 2.423893\n",
      "step 1431, loss: 2.418770\n",
      "step 1432, loss: 2.413662\n",
      "step 1433, loss: 2.408572\n",
      "step 1434, loss: 2.403499\n",
      "step 1435, loss: 2.398441\n",
      "step 1436, loss: 2.393401\n",
      "step 1437, loss: 2.388378\n",
      "step 1438, loss: 2.383370\n",
      "step 1439, loss: 2.378380\n",
      "step 1440, loss: 2.373405\n",
      "step 1441, loss: 2.368447\n",
      "step 1442, loss: 2.363504\n",
      "step 1443, loss: 2.358579\n",
      "step 1444, loss: 2.353670\n",
      "step 1445, loss: 2.348776\n",
      "step 1446, loss: 2.343899\n",
      "step 1447, loss: 2.339037\n",
      "step 1448, loss: 2.334192\n",
      "step 1449, loss: 2.329363\n",
      "step 1450, loss: 2.324548\n",
      "step 1451, loss: 2.319751\n",
      "step 1452, loss: 2.314969\n",
      "step 1453, loss: 2.310202\n",
      "step 1454, loss: 2.305452\n",
      "step 1455, loss: 2.300716\n",
      "step 1456, loss: 2.295997\n",
      "step 1457, loss: 2.291292\n",
      "step 1458, loss: 2.286603\n",
      "step 1459, loss: 2.281930\n",
      "step 1460, loss: 2.277271\n",
      "step 1461, loss: 2.272628\n",
      "step 1462, loss: 2.268001\n",
      "step 1463, loss: 2.263389\n",
      "step 1464, loss: 2.258791\n",
      "step 1465, loss: 2.254209\n",
      "step 1466, loss: 2.249642\n",
      "step 1467, loss: 2.245090\n",
      "step 1468, loss: 2.240551\n",
      "step 1469, loss: 2.236030\n",
      "step 1470, loss: 2.231522\n",
      "step 1471, loss: 2.227029\n",
      "step 1472, loss: 2.222551\n",
      "step 1473, loss: 2.218087\n",
      "step 1474, loss: 2.213638\n",
      "step 1475, loss: 2.209204\n",
      "step 1476, loss: 2.204784\n",
      "step 1477, loss: 2.200378\n",
      "step 1478, loss: 2.195988\n",
      "step 1479, loss: 2.191611\n",
      "step 1480, loss: 2.187249\n",
      "step 1481, loss: 2.182901\n",
      "step 1482, loss: 2.178568\n",
      "step 1483, loss: 2.174248\n",
      "step 1484, loss: 2.169943\n",
      "step 1485, loss: 2.165652\n",
      "step 1486, loss: 2.161375\n",
      "step 1487, loss: 2.157112\n",
      "step 1488, loss: 2.152862\n",
      "step 1489, loss: 2.148628\n",
      "step 1490, loss: 2.144405\n",
      "step 1491, loss: 2.140198\n",
      "step 1492, loss: 2.136004\n",
      "step 1493, loss: 2.131824\n",
      "step 1494, loss: 2.127658\n",
      "step 1495, loss: 2.123505\n",
      "step 1496, loss: 2.119366\n",
      "step 1497, loss: 2.115239\n",
      "step 1498, loss: 2.111128\n",
      "step 1499, loss: 2.107029\n",
      "step 1500, loss: 2.102944\n",
      "step 1501, loss: 2.098871\n",
      "step 1502, loss: 2.094813\n",
      "step 1503, loss: 2.090767\n",
      "step 1504, loss: 2.086735\n",
      "step 1505, loss: 2.082717\n",
      "step 1506, loss: 2.078711\n",
      "step 1507, loss: 2.074718\n",
      "step 1508, loss: 2.070739\n",
      "step 1509, loss: 2.066772\n",
      "step 1510, loss: 2.062819\n",
      "step 1511, loss: 2.058878\n",
      "step 1512, loss: 2.054950\n",
      "step 1513, loss: 2.051035\n",
      "step 1514, loss: 2.047134\n",
      "step 1515, loss: 2.043244\n",
      "step 1516, loss: 2.039367\n",
      "step 1517, loss: 2.035503\n",
      "step 1518, loss: 2.031652\n",
      "step 1519, loss: 2.027813\n",
      "step 1520, loss: 2.023987\n",
      "step 1521, loss: 2.020174\n",
      "step 1522, loss: 2.016372\n",
      "step 1523, loss: 2.012583\n",
      "step 1524, loss: 2.008807\n",
      "step 1525, loss: 2.005043\n",
      "step 1526, loss: 2.001292\n",
      "step 1527, loss: 1.997551\n",
      "step 1528, loss: 1.993824\n",
      "step 1529, loss: 1.990109\n",
      "step 1530, loss: 1.986406\n",
      "step 1531, loss: 1.982715\n",
      "step 1532, loss: 1.979037\n",
      "step 1533, loss: 1.975370\n",
      "step 1534, loss: 1.971714\n",
      "step 1535, loss: 1.968072\n",
      "step 1536, loss: 1.964441\n",
      "step 1537, loss: 1.960822\n",
      "step 1538, loss: 1.957215\n",
      "step 1539, loss: 1.953619\n",
      "step 1540, loss: 1.950035\n",
      "step 1541, loss: 1.946462\n",
      "step 1542, loss: 1.942902\n",
      "step 1543, loss: 1.939353\n",
      "step 1544, loss: 1.935816\n",
      "step 1545, loss: 1.932290\n",
      "step 1546, loss: 1.928776\n",
      "step 1547, loss: 1.925273\n",
      "step 1548, loss: 1.921781\n",
      "step 1549, loss: 1.918302\n",
      "step 1550, loss: 1.914833\n",
      "step 1551, loss: 1.911375\n",
      "step 1552, loss: 1.907930\n",
      "step 1553, loss: 1.904495\n",
      "step 1554, loss: 1.901070\n",
      "step 1555, loss: 1.897658\n",
      "step 1556, loss: 1.894258\n",
      "step 1557, loss: 1.890867\n",
      "step 1558, loss: 1.887488\n",
      "step 1559, loss: 1.884120\n",
      "step 1560, loss: 1.880762\n",
      "step 1561, loss: 1.877416\n",
      "step 1562, loss: 1.874081\n",
      "step 1563, loss: 1.870756\n",
      "step 1564, loss: 1.867443\n",
      "step 1565, loss: 1.864140\n",
      "step 1566, loss: 1.860847\n",
      "step 1567, loss: 1.857566\n",
      "step 1568, loss: 1.854296\n",
      "step 1569, loss: 1.851036\n",
      "step 1570, loss: 1.847786\n",
      "step 1571, loss: 1.844548\n",
      "step 1572, loss: 1.841319\n",
      "step 1573, loss: 1.838102\n",
      "step 1574, loss: 1.834894\n",
      "step 1575, loss: 1.831698\n",
      "step 1576, loss: 1.828511\n",
      "step 1577, loss: 1.825335\n",
      "step 1578, loss: 1.822169\n",
      "step 1579, loss: 1.819014\n",
      "step 1580, loss: 1.815869\n",
      "step 1581, loss: 1.812734\n",
      "step 1582, loss: 1.809609\n",
      "step 1583, loss: 1.806494\n",
      "step 1584, loss: 1.803390\n",
      "step 1585, loss: 1.800296\n",
      "step 1586, loss: 1.797212\n",
      "step 1587, loss: 1.794137\n",
      "step 1588, loss: 1.791073\n",
      "step 1589, loss: 1.788019\n",
      "step 1590, loss: 1.784974\n",
      "step 1591, loss: 1.781941\n",
      "step 1592, loss: 1.778916\n",
      "step 1593, loss: 1.775901\n",
      "step 1594, loss: 1.772896\n",
      "step 1595, loss: 1.769901\n",
      "step 1596, loss: 1.766916\n",
      "step 1597, loss: 1.763940\n",
      "step 1598, loss: 1.760974\n",
      "step 1599, loss: 1.758017\n",
      "step 1600, loss: 1.755070\n",
      "step 1601, loss: 1.752133\n",
      "step 1602, loss: 1.749206\n",
      "step 1603, loss: 1.746287\n",
      "step 1604, loss: 1.743378\n",
      "step 1605, loss: 1.740480\n",
      "step 1606, loss: 1.737589\n",
      "step 1607, loss: 1.734709\n",
      "step 1608, loss: 1.731838\n",
      "step 1609, loss: 1.728977\n",
      "step 1610, loss: 1.726124\n",
      "step 1611, loss: 1.723281\n",
      "step 1612, loss: 1.720446\n",
      "step 1613, loss: 1.717622\n",
      "step 1614, loss: 1.714807\n",
      "step 1615, loss: 1.712000\n",
      "step 1616, loss: 1.709202\n",
      "step 1617, loss: 1.706414\n",
      "step 1618, loss: 1.703634\n",
      "step 1619, loss: 1.700865\n",
      "step 1620, loss: 1.698103\n",
      "step 1621, loss: 1.695350\n",
      "step 1622, loss: 1.692608\n",
      "step 1623, loss: 1.689874\n",
      "step 1624, loss: 1.687147\n",
      "step 1625, loss: 1.684431\n",
      "step 1626, loss: 1.681723\n",
      "step 1627, loss: 1.679024\n",
      "step 1628, loss: 1.676333\n",
      "step 1629, loss: 1.673651\n",
      "step 1630, loss: 1.670978\n",
      "step 1631, loss: 1.668314\n",
      "step 1632, loss: 1.665658\n",
      "step 1633, loss: 1.663011\n",
      "step 1634, loss: 1.660373\n",
      "step 1635, loss: 1.657743\n",
      "step 1636, loss: 1.655122\n",
      "step 1637, loss: 1.652508\n",
      "step 1638, loss: 1.649904\n",
      "step 1639, loss: 1.647308\n",
      "step 1640, loss: 1.644721\n",
      "step 1641, loss: 1.642141\n",
      "step 1642, loss: 1.639570\n",
      "step 1643, loss: 1.637007\n",
      "step 1644, loss: 1.634453\n",
      "step 1645, loss: 1.631907\n",
      "step 1646, loss: 1.629369\n",
      "step 1647, loss: 1.626840\n",
      "step 1648, loss: 1.624318\n",
      "step 1649, loss: 1.621805\n",
      "step 1650, loss: 1.619299\n",
      "step 1651, loss: 1.616802\n",
      "step 1652, loss: 1.614313\n",
      "step 1653, loss: 1.611832\n",
      "step 1654, loss: 1.609360\n",
      "step 1655, loss: 1.606895\n",
      "step 1656, loss: 1.604438\n",
      "step 1657, loss: 1.601989\n",
      "step 1658, loss: 1.599548\n",
      "step 1659, loss: 1.597115\n",
      "step 1660, loss: 1.594690\n",
      "step 1661, loss: 1.592272\n",
      "step 1662, loss: 1.589863\n",
      "step 1663, loss: 1.587461\n",
      "step 1664, loss: 1.585067\n",
      "step 1665, loss: 1.582681\n",
      "step 1666, loss: 1.580302\n",
      "step 1667, loss: 1.577931\n",
      "step 1668, loss: 1.575568\n",
      "step 1669, loss: 1.573212\n",
      "step 1670, loss: 1.570864\n",
      "step 1671, loss: 1.568523\n",
      "step 1672, loss: 1.566190\n",
      "step 1673, loss: 1.563865\n",
      "step 1674, loss: 1.561547\n",
      "step 1675, loss: 1.559237\n",
      "step 1676, loss: 1.556933\n",
      "step 1677, loss: 1.554638\n",
      "step 1678, loss: 1.552350\n",
      "step 1679, loss: 1.550069\n",
      "step 1680, loss: 1.547795\n",
      "step 1681, loss: 1.545530\n",
      "step 1682, loss: 1.543272\n",
      "step 1683, loss: 1.541020\n",
      "step 1684, loss: 1.538776\n",
      "step 1685, loss: 1.536539\n",
      "step 1686, loss: 1.534309\n",
      "step 1687, loss: 1.532087\n",
      "step 1688, loss: 1.529871\n",
      "step 1689, loss: 1.527663\n",
      "step 1690, loss: 1.525461\n",
      "step 1691, loss: 1.523267\n",
      "step 1692, loss: 1.521080\n",
      "step 1693, loss: 1.518900\n",
      "step 1694, loss: 1.516728\n",
      "step 1695, loss: 1.514562\n",
      "step 1696, loss: 1.512403\n",
      "step 1697, loss: 1.510251\n",
      "step 1698, loss: 1.508106\n",
      "step 1699, loss: 1.505968\n",
      "step 1700, loss: 1.503837\n",
      "step 1701, loss: 1.501712\n",
      "step 1702, loss: 1.499594\n",
      "step 1703, loss: 1.497484\n",
      "step 1704, loss: 1.495379\n",
      "step 1705, loss: 1.493282\n",
      "step 1706, loss: 1.491192\n",
      "step 1707, loss: 1.489108\n",
      "step 1708, loss: 1.487032\n",
      "step 1709, loss: 1.484962\n",
      "step 1710, loss: 1.482897\n",
      "step 1711, loss: 1.480840\n",
      "step 1712, loss: 1.478790\n",
      "step 1713, loss: 1.476746\n",
      "step 1714, loss: 1.474709\n",
      "step 1715, loss: 1.472678\n",
      "step 1716, loss: 1.470654\n",
      "step 1717, loss: 1.468636\n",
      "step 1718, loss: 1.466625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1719, loss: 1.464620\n",
      "step 1720, loss: 1.462622\n",
      "step 1721, loss: 1.460630\n",
      "step 1722, loss: 1.458644\n",
      "step 1723, loss: 1.456666\n",
      "step 1724, loss: 1.454693\n",
      "step 1725, loss: 1.452726\n",
      "step 1726, loss: 1.450766\n",
      "step 1727, loss: 1.448812\n",
      "step 1728, loss: 1.446864\n",
      "step 1729, loss: 1.444924\n",
      "step 1730, loss: 1.442989\n",
      "step 1731, loss: 1.441059\n",
      "step 1732, loss: 1.439137\n",
      "step 1733, loss: 1.437221\n",
      "step 1734, loss: 1.435310\n",
      "step 1735, loss: 1.433405\n",
      "step 1736, loss: 1.431508\n",
      "step 1737, loss: 1.429615\n",
      "step 1738, loss: 1.427730\n",
      "step 1739, loss: 1.425849\n",
      "step 1740, loss: 1.423976\n",
      "step 1741, loss: 1.422108\n",
      "step 1742, loss: 1.420246\n",
      "step 1743, loss: 1.418390\n",
      "step 1744, loss: 1.416540\n",
      "step 1745, loss: 1.414696\n",
      "step 1746, loss: 1.412858\n",
      "step 1747, loss: 1.411025\n",
      "step 1748, loss: 1.409199\n",
      "step 1749, loss: 1.407378\n",
      "step 1750, loss: 1.405564\n",
      "step 1751, loss: 1.403755\n",
      "step 1752, loss: 1.401951\n",
      "step 1753, loss: 1.400154\n",
      "step 1754, loss: 1.398362\n",
      "step 1755, loss: 1.396577\n",
      "step 1756, loss: 1.394796\n",
      "step 1757, loss: 1.393022\n",
      "step 1758, loss: 1.391253\n",
      "step 1759, loss: 1.389490\n",
      "step 1760, loss: 1.387733\n",
      "step 1761, loss: 1.385981\n",
      "step 1762, loss: 1.384234\n",
      "step 1763, loss: 1.382494\n",
      "step 1764, loss: 1.380758\n",
      "step 1765, loss: 1.379029\n",
      "step 1766, loss: 1.377305\n",
      "step 1767, loss: 1.375587\n",
      "step 1768, loss: 1.373873\n",
      "step 1769, loss: 1.372166\n",
      "step 1770, loss: 1.370464\n",
      "step 1771, loss: 1.368767\n",
      "step 1772, loss: 1.367076\n",
      "step 1773, loss: 1.365390\n",
      "step 1774, loss: 1.363709\n",
      "step 1775, loss: 1.362035\n",
      "step 1776, loss: 1.360364\n",
      "step 1777, loss: 1.358700\n",
      "step 1778, loss: 1.357041\n",
      "step 1779, loss: 1.355387\n",
      "step 1780, loss: 1.353738\n",
      "step 1781, loss: 1.352095\n",
      "step 1782, loss: 1.350456\n",
      "step 1783, loss: 1.348824\n",
      "step 1784, loss: 1.347197\n",
      "step 1785, loss: 1.345573\n",
      "step 1786, loss: 1.343957\n",
      "step 1787, loss: 1.342345\n",
      "step 1788, loss: 1.340737\n",
      "step 1789, loss: 1.339136\n",
      "step 1790, loss: 1.337539\n",
      "step 1791, loss: 1.335947\n",
      "step 1792, loss: 1.334360\n",
      "step 1793, loss: 1.332779\n",
      "step 1794, loss: 1.331202\n",
      "step 1795, loss: 1.329631\n",
      "step 1796, loss: 1.328064\n",
      "step 1797, loss: 1.326502\n",
      "step 1798, loss: 1.324946\n",
      "step 1799, loss: 1.323394\n",
      "step 1800, loss: 1.321848\n",
      "step 1801, loss: 1.320306\n",
      "step 1802, loss: 1.318769\n",
      "step 1803, loss: 1.317237\n",
      "step 1804, loss: 1.315710\n",
      "step 1805, loss: 1.314187\n",
      "step 1806, loss: 1.312670\n",
      "step 1807, loss: 1.311158\n",
      "step 1808, loss: 1.309649\n",
      "step 1809, loss: 1.308146\n",
      "step 1810, loss: 1.306649\n",
      "step 1811, loss: 1.305155\n",
      "step 1812, loss: 1.303666\n",
      "step 1813, loss: 1.302182\n",
      "step 1814, loss: 1.300703\n",
      "step 1815, loss: 1.299228\n",
      "step 1816, loss: 1.297758\n",
      "step 1817, loss: 1.296292\n",
      "step 1818, loss: 1.294832\n",
      "step 1819, loss: 1.293376\n",
      "step 1820, loss: 1.291925\n",
      "step 1821, loss: 1.290478\n",
      "step 1822, loss: 1.289035\n",
      "step 1823, loss: 1.287598\n",
      "step 1824, loss: 1.286165\n",
      "step 1825, loss: 1.284736\n",
      "step 1826, loss: 1.283312\n",
      "step 1827, loss: 1.281893\n",
      "step 1828, loss: 1.280478\n",
      "step 1829, loss: 1.279068\n",
      "step 1830, loss: 1.277661\n",
      "step 1831, loss: 1.276259\n",
      "step 1832, loss: 1.274862\n",
      "step 1833, loss: 1.273470\n",
      "step 1834, loss: 1.272081\n",
      "step 1835, loss: 1.270697\n",
      "step 1836, loss: 1.269317\n",
      "step 1837, loss: 1.267943\n",
      "step 1838, loss: 1.266571\n",
      "step 1839, loss: 1.265204\n",
      "step 1840, loss: 1.263842\n",
      "step 1841, loss: 1.262485\n",
      "step 1842, loss: 1.261131\n",
      "step 1843, loss: 1.259781\n",
      "step 1844, loss: 1.258436\n",
      "step 1845, loss: 1.257095\n",
      "step 1846, loss: 1.255759\n",
      "step 1847, loss: 1.254426\n",
      "step 1848, loss: 1.253098\n",
      "step 1849, loss: 1.251774\n",
      "step 1850, loss: 1.250454\n",
      "step 1851, loss: 1.249139\n",
      "step 1852, loss: 1.247827\n",
      "step 1853, loss: 1.246520\n",
      "step 1854, loss: 1.245216\n",
      "step 1855, loss: 1.243917\n",
      "step 1856, loss: 1.242622\n",
      "step 1857, loss: 1.241331\n",
      "step 1858, loss: 1.240044\n",
      "step 1859, loss: 1.238761\n",
      "step 1860, loss: 1.237483\n",
      "step 1861, loss: 1.236207\n",
      "step 1862, loss: 1.234937\n",
      "step 1863, loss: 1.233670\n",
      "step 1864, loss: 1.232407\n",
      "step 1865, loss: 1.231149\n",
      "step 1866, loss: 1.229894\n",
      "step 1867, loss: 1.228642\n",
      "step 1868, loss: 1.227395\n",
      "step 1869, loss: 1.226153\n",
      "step 1870, loss: 1.224913\n",
      "step 1871, loss: 1.223678\n",
      "step 1872, loss: 1.222446\n",
      "step 1873, loss: 1.221219\n",
      "step 1874, loss: 1.219995\n",
      "step 1875, loss: 1.218775\n",
      "step 1876, loss: 1.217559\n",
      "step 1877, loss: 1.216347\n",
      "step 1878, loss: 1.215139\n",
      "step 1879, loss: 1.213934\n",
      "step 1880, loss: 1.212733\n",
      "step 1881, loss: 1.211536\n",
      "step 1882, loss: 1.210343\n",
      "step 1883, loss: 1.209153\n",
      "step 1884, loss: 1.207967\n",
      "step 1885, loss: 1.206784\n",
      "step 1886, loss: 1.205606\n",
      "step 1887, loss: 1.204431\n",
      "step 1888, loss: 1.203260\n",
      "step 1889, loss: 1.202093\n",
      "step 1890, loss: 1.200929\n",
      "step 1891, loss: 1.199769\n",
      "step 1892, loss: 1.198612\n",
      "step 1893, loss: 1.197459\n",
      "step 1894, loss: 1.196310\n",
      "step 1895, loss: 1.195164\n",
      "step 1896, loss: 1.194022\n",
      "step 1897, loss: 1.192883\n",
      "step 1898, loss: 1.191748\n",
      "step 1899, loss: 1.190617\n",
      "step 1900, loss: 1.189490\n",
      "step 1901, loss: 1.188365\n",
      "step 1902, loss: 1.187244\n",
      "step 1903, loss: 1.186126\n",
      "step 1904, loss: 1.185013\n",
      "step 1905, loss: 1.183902\n",
      "step 1906, loss: 1.182795\n",
      "step 1907, loss: 1.181691\n",
      "step 1908, loss: 1.180591\n",
      "step 1909, loss: 1.179494\n",
      "step 1910, loss: 1.178401\n",
      "step 1911, loss: 1.177311\n",
      "step 1912, loss: 1.176224\n",
      "step 1913, loss: 1.175141\n",
      "step 1914, loss: 1.174061\n",
      "step 1915, loss: 1.172985\n",
      "step 1916, loss: 1.171911\n",
      "step 1917, loss: 1.170842\n",
      "step 1918, loss: 1.169775\n",
      "step 1919, loss: 1.168712\n",
      "step 1920, loss: 1.167653\n",
      "step 1921, loss: 1.166596\n",
      "step 1922, loss: 1.165543\n",
      "step 1923, loss: 1.164493\n",
      "step 1924, loss: 1.163446\n",
      "step 1925, loss: 1.162403\n",
      "step 1926, loss: 1.161362\n",
      "step 1927, loss: 1.160326\n",
      "step 1928, loss: 1.159292\n",
      "step 1929, loss: 1.158261\n",
      "step 1930, loss: 1.157233\n",
      "step 1931, loss: 1.156209\n",
      "step 1932, loss: 1.155188\n",
      "step 1933, loss: 1.154170\n",
      "step 1934, loss: 1.153155\n",
      "step 1935, loss: 1.152144\n",
      "step 1936, loss: 1.151136\n",
      "step 1937, loss: 1.150130\n",
      "step 1938, loss: 1.149127\n",
      "step 1939, loss: 1.148128\n",
      "step 1940, loss: 1.147132\n",
      "step 1941, loss: 1.146139\n",
      "step 1942, loss: 1.145149\n",
      "step 1943, loss: 1.144162\n",
      "step 1944, loss: 1.143178\n",
      "step 1945, loss: 1.142197\n",
      "step 1946, loss: 1.141220\n",
      "step 1947, loss: 1.140245\n",
      "step 1948, loss: 1.139273\n",
      "step 1949, loss: 1.138304\n",
      "step 1950, loss: 1.137338\n",
      "step 1951, loss: 1.136375\n",
      "step 1952, loss: 1.135415\n",
      "step 1953, loss: 1.134458\n",
      "step 1954, loss: 1.133504\n",
      "step 1955, loss: 1.132553\n",
      "step 1956, loss: 1.131605\n",
      "step 1957, loss: 1.130660\n",
      "step 1958, loss: 1.129717\n",
      "step 1959, loss: 1.128777\n",
      "step 1960, loss: 1.127841\n",
      "step 1961, loss: 1.126907\n",
      "step 1962, loss: 1.125976\n",
      "step 1963, loss: 1.125048\n",
      "step 1964, loss: 1.124123\n",
      "step 1965, loss: 1.123200\n",
      "step 1966, loss: 1.122281\n",
      "step 1967, loss: 1.121364\n",
      "step 1968, loss: 1.120450\n",
      "step 1969, loss: 1.119539\n",
      "step 1970, loss: 1.118631\n",
      "step 1971, loss: 1.117725\n",
      "step 1972, loss: 1.116822\n",
      "step 1973, loss: 1.115922\n",
      "step 1974, loss: 1.115025\n",
      "step 1975, loss: 1.114130\n",
      "step 1976, loss: 1.113239\n",
      "step 1977, loss: 1.112349\n",
      "step 1978, loss: 1.111463\n",
      "step 1979, loss: 1.110579\n",
      "step 1980, loss: 1.109699\n",
      "step 1981, loss: 1.108820\n",
      "step 1982, loss: 1.107944\n",
      "step 1983, loss: 1.107071\n",
      "step 1984, loss: 1.106201\n",
      "step 1985, loss: 1.105333\n",
      "step 1986, loss: 1.104467\n",
      "step 1987, loss: 1.103605\n",
      "step 1988, loss: 1.102745\n",
      "step 1989, loss: 1.101888\n",
      "step 1990, loss: 1.101034\n",
      "step 1991, loss: 1.100181\n",
      "step 1992, loss: 1.099332\n",
      "step 1993, loss: 1.098485\n",
      "step 1994, loss: 1.097640\n",
      "step 1995, loss: 1.096799\n",
      "step 1996, loss: 1.095959\n",
      "step 1997, loss: 1.095123\n",
      "step 1998, loss: 1.094288\n",
      "step 1999, loss: 1.093457\n",
      "step 2000, loss: 1.092627\n",
      "step 2001, loss: 1.091800\n",
      "step 2002, loss: 1.090976\n",
      "step 2003, loss: 1.090154\n",
      "step 2004, loss: 1.089336\n",
      "step 2005, loss: 1.088518\n",
      "step 2006, loss: 1.087704\n",
      "step 2007, loss: 1.086892\n",
      "step 2008, loss: 1.086083\n",
      "step 2009, loss: 1.085275\n",
      "step 2010, loss: 1.084471\n",
      "step 2011, loss: 1.083669\n",
      "step 2012, loss: 1.082869\n",
      "step 2013, loss: 1.082072\n",
      "step 2014, loss: 1.081276\n",
      "step 2015, loss: 1.080484\n",
      "step 2016, loss: 1.079693\n",
      "step 2017, loss: 1.078906\n",
      "step 2018, loss: 1.078120\n",
      "step 2019, loss: 1.077337\n",
      "step 2020, loss: 1.076556\n",
      "step 2021, loss: 1.075777\n",
      "step 2022, loss: 1.075001\n",
      "step 2023, loss: 1.074228\n",
      "step 2024, loss: 1.073456\n",
      "step 2025, loss: 1.072686\n",
      "step 2026, loss: 1.071920\n",
      "step 2027, loss: 1.071155\n",
      "step 2028, loss: 1.070392\n",
      "step 2029, loss: 1.069632\n",
      "step 2030, loss: 1.068874\n",
      "step 2031, loss: 1.068119\n",
      "step 2032, loss: 1.067366\n",
      "step 2033, loss: 1.066614\n",
      "step 2034, loss: 1.065865\n",
      "step 2035, loss: 1.065119\n",
      "step 2036, loss: 1.064374\n",
      "step 2037, loss: 1.063632\n",
      "step 2038, loss: 1.062892\n",
      "step 2039, loss: 1.062154\n",
      "step 2040, loss: 1.061418\n",
      "step 2041, loss: 1.060685\n",
      "step 2042, loss: 1.059953\n",
      "step 2043, loss: 1.059224\n",
      "step 2044, loss: 1.058497\n",
      "step 2045, loss: 1.057771\n",
      "step 2046, loss: 1.057049\n",
      "step 2047, loss: 1.056329\n",
      "step 2048, loss: 1.055610\n",
      "step 2049, loss: 1.054893\n",
      "step 2050, loss: 1.054179\n",
      "step 2051, loss: 1.053467\n",
      "step 2052, loss: 1.052757\n",
      "step 2053, loss: 1.052049\n",
      "step 2054, loss: 1.051343\n",
      "step 2055, loss: 1.050639\n",
      "step 2056, loss: 1.049937\n",
      "step 2057, loss: 1.049237\n",
      "step 2058, loss: 1.048540\n",
      "step 2059, loss: 1.047844\n",
      "step 2060, loss: 1.047150\n",
      "step 2061, loss: 1.046458\n",
      "step 2062, loss: 1.045769\n",
      "step 2063, loss: 1.045081\n",
      "step 2064, loss: 1.044396\n",
      "step 2065, loss: 1.043712\n",
      "step 2066, loss: 1.043030\n",
      "step 2067, loss: 1.042351\n",
      "step 2068, loss: 1.041673\n",
      "step 2069, loss: 1.040997\n",
      "step 2070, loss: 1.040323\n",
      "step 2071, loss: 1.039652\n",
      "step 2072, loss: 1.038981\n",
      "step 2073, loss: 1.038314\n",
      "step 2074, loss: 1.037648\n",
      "step 2075, loss: 1.036984\n",
      "step 2076, loss: 1.036322\n",
      "step 2077, loss: 1.035662\n",
      "step 2078, loss: 1.035003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2079, loss: 1.034347\n",
      "step 2080, loss: 1.033692\n",
      "step 2081, loss: 1.033039\n",
      "step 2082, loss: 1.032389\n",
      "step 2083, loss: 1.031740\n",
      "step 2084, loss: 1.031093\n",
      "step 2085, loss: 1.030448\n",
      "step 2086, loss: 1.029804\n",
      "step 2087, loss: 1.029163\n",
      "step 2088, loss: 1.028524\n",
      "step 2089, loss: 1.027886\n",
      "step 2090, loss: 1.027250\n",
      "step 2091, loss: 1.026615\n",
      "step 2092, loss: 1.025983\n",
      "step 2093, loss: 1.025353\n",
      "step 2094, loss: 1.024724\n",
      "step 2095, loss: 1.024097\n",
      "step 2096, loss: 1.023472\n",
      "step 2097, loss: 1.022848\n",
      "step 2098, loss: 1.022227\n",
      "step 2099, loss: 1.021607\n",
      "step 2100, loss: 1.020989\n",
      "step 2101, loss: 1.020372\n",
      "step 2102, loss: 1.019758\n",
      "step 2103, loss: 1.019145\n",
      "step 2104, loss: 1.018534\n",
      "step 2105, loss: 1.017925\n",
      "step 2106, loss: 1.017317\n",
      "step 2107, loss: 1.016711\n",
      "step 2108, loss: 1.016107\n",
      "step 2109, loss: 1.015505\n",
      "step 2110, loss: 1.014904\n",
      "step 2111, loss: 1.014305\n",
      "step 2112, loss: 1.013707\n",
      "step 2113, loss: 1.013111\n",
      "step 2114, loss: 1.012517\n",
      "step 2115, loss: 1.011925\n",
      "step 2116, loss: 1.011334\n",
      "step 2117, loss: 1.010745\n",
      "step 2118, loss: 1.010158\n",
      "step 2119, loss: 1.009572\n",
      "step 2120, loss: 1.008987\n",
      "step 2121, loss: 1.008405\n",
      "step 2122, loss: 1.007824\n",
      "step 2123, loss: 1.007245\n",
      "step 2124, loss: 1.006668\n",
      "step 2125, loss: 1.006091\n",
      "step 2126, loss: 1.005517\n",
      "step 2127, loss: 1.004944\n",
      "step 2128, loss: 1.004373\n",
      "step 2129, loss: 1.003803\n",
      "step 2130, loss: 1.003235\n",
      "step 2131, loss: 1.002668\n",
      "step 2132, loss: 1.002103\n",
      "step 2133, loss: 1.001540\n",
      "step 2134, loss: 1.000978\n",
      "step 2135, loss: 1.000417\n",
      "step 2136, loss: 0.999859\n",
      "step 2137, loss: 0.999301\n",
      "step 2138, loss: 0.998746\n",
      "step 2139, loss: 0.998191\n",
      "step 2140, loss: 0.997639\n",
      "step 2141, loss: 0.997088\n",
      "step 2142, loss: 0.996539\n",
      "step 2143, loss: 0.995990\n",
      "step 2144, loss: 0.995444\n",
      "step 2145, loss: 0.994898\n",
      "step 2146, loss: 0.994355\n",
      "step 2147, loss: 0.993812\n",
      "step 2148, loss: 0.993272\n",
      "step 2149, loss: 0.992733\n",
      "step 2150, loss: 0.992195\n",
      "step 2151, loss: 0.991658\n",
      "step 2152, loss: 0.991123\n",
      "step 2153, loss: 0.990590\n",
      "step 2154, loss: 0.990058\n",
      "step 2155, loss: 0.989528\n",
      "step 2156, loss: 0.988999\n",
      "step 2157, loss: 0.988471\n",
      "step 2158, loss: 0.987945\n",
      "step 2159, loss: 0.987420\n",
      "step 2160, loss: 0.986897\n",
      "step 2161, loss: 0.986375\n",
      "step 2162, loss: 0.985854\n",
      "step 2163, loss: 0.985335\n",
      "step 2164, loss: 0.984817\n",
      "step 2165, loss: 0.984300\n",
      "step 2166, loss: 0.983785\n",
      "step 2167, loss: 0.983271\n",
      "step 2168, loss: 0.982759\n",
      "step 2169, loss: 0.982249\n",
      "step 2170, loss: 0.981739\n",
      "step 2171, loss: 0.981230\n",
      "step 2172, loss: 0.980724\n",
      "step 2173, loss: 0.980218\n",
      "step 2174, loss: 0.979714\n",
      "step 2175, loss: 0.979211\n",
      "step 2176, loss: 0.978709\n",
      "step 2177, loss: 0.978209\n",
      "step 2178, loss: 0.977710\n",
      "step 2179, loss: 0.977213\n",
      "step 2180, loss: 0.976716\n",
      "step 2181, loss: 0.976221\n",
      "step 2182, loss: 0.975728\n",
      "step 2183, loss: 0.975235\n",
      "step 2184, loss: 0.974744\n",
      "step 2185, loss: 0.974254\n",
      "step 2186, loss: 0.973766\n",
      "step 2187, loss: 0.973278\n",
      "step 2188, loss: 0.972793\n",
      "step 2189, loss: 0.972308\n",
      "step 2190, loss: 0.971824\n",
      "step 2191, loss: 0.971342\n",
      "step 2192, loss: 0.970861\n",
      "step 2193, loss: 0.970381\n",
      "step 2194, loss: 0.969903\n",
      "step 2195, loss: 0.969425\n",
      "step 2196, loss: 0.968949\n",
      "step 2197, loss: 0.968475\n",
      "step 2198, loss: 0.968001\n",
      "step 2199, loss: 0.967529\n",
      "step 2200, loss: 0.967058\n",
      "step 2201, loss: 0.966588\n",
      "step 2202, loss: 0.966119\n",
      "step 2203, loss: 0.965651\n",
      "step 2204, loss: 0.965185\n",
      "step 2205, loss: 0.964720\n",
      "step 2206, loss: 0.964256\n",
      "step 2207, loss: 0.963793\n",
      "step 2208, loss: 0.963332\n",
      "step 2209, loss: 0.962871\n",
      "step 2210, loss: 0.962412\n",
      "step 2211, loss: 0.961954\n",
      "step 2212, loss: 0.961497\n",
      "step 2213, loss: 0.961041\n",
      "step 2214, loss: 0.960586\n",
      "step 2215, loss: 0.960133\n",
      "step 2216, loss: 0.959680\n",
      "step 2217, loss: 0.959229\n",
      "step 2218, loss: 0.958779\n",
      "step 2219, loss: 0.958330\n",
      "step 2220, loss: 0.957882\n",
      "step 2221, loss: 0.957435\n",
      "step 2222, loss: 0.956990\n",
      "step 2223, loss: 0.956545\n",
      "step 2224, loss: 0.956102\n",
      "step 2225, loss: 0.955660\n",
      "step 2226, loss: 0.955218\n",
      "step 2227, loss: 0.954778\n",
      "step 2228, loss: 0.954340\n",
      "step 2229, loss: 0.953901\n",
      "step 2230, loss: 0.953464\n",
      "step 2231, loss: 0.953029\n",
      "step 2232, loss: 0.952594\n",
      "step 2233, loss: 0.952160\n",
      "step 2234, loss: 0.951727\n",
      "step 2235, loss: 0.951296\n",
      "step 2236, loss: 0.950865\n",
      "step 2237, loss: 0.950436\n",
      "step 2238, loss: 0.950007\n",
      "step 2239, loss: 0.949580\n",
      "step 2240, loss: 0.949154\n",
      "step 2241, loss: 0.948728\n",
      "step 2242, loss: 0.948304\n",
      "step 2243, loss: 0.947880\n",
      "step 2244, loss: 0.947458\n",
      "step 2245, loss: 0.947037\n",
      "step 2246, loss: 0.946617\n",
      "step 2247, loss: 0.946198\n",
      "step 2248, loss: 0.945780\n",
      "step 2249, loss: 0.945362\n",
      "step 2250, loss: 0.944946\n",
      "step 2251, loss: 0.944531\n",
      "step 2252, loss: 0.944116\n",
      "step 2253, loss: 0.943703\n",
      "step 2254, loss: 0.943291\n",
      "step 2255, loss: 0.942880\n",
      "step 2256, loss: 0.942469\n",
      "step 2257, loss: 0.942060\n",
      "step 2258, loss: 0.941652\n",
      "step 2259, loss: 0.941244\n",
      "step 2260, loss: 0.940838\n",
      "step 2261, loss: 0.940432\n",
      "step 2262, loss: 0.940028\n",
      "step 2263, loss: 0.939624\n",
      "step 2264, loss: 0.939222\n",
      "step 2265, loss: 0.938820\n",
      "step 2266, loss: 0.938419\n",
      "step 2267, loss: 0.938019\n",
      "step 2268, loss: 0.937620\n",
      "step 2269, loss: 0.937222\n",
      "step 2270, loss: 0.936825\n",
      "step 2271, loss: 0.936429\n",
      "step 2272, loss: 0.936033\n",
      "step 2273, loss: 0.935639\n",
      "step 2274, loss: 0.935246\n",
      "step 2275, loss: 0.934853\n",
      "step 2276, loss: 0.934461\n",
      "step 2277, loss: 0.934070\n",
      "step 2278, loss: 0.933681\n",
      "step 2279, loss: 0.933292\n",
      "step 2280, loss: 0.932904\n",
      "step 2281, loss: 0.932516\n",
      "step 2282, loss: 0.932130\n",
      "step 2283, loss: 0.931745\n",
      "step 2284, loss: 0.931360\n",
      "step 2285, loss: 0.930976\n",
      "step 2286, loss: 0.930593\n",
      "step 2287, loss: 0.930211\n",
      "step 2288, loss: 0.929830\n",
      "step 2289, loss: 0.929449\n",
      "step 2290, loss: 0.929070\n",
      "step 2291, loss: 0.928692\n",
      "step 2292, loss: 0.928313\n",
      "step 2293, loss: 0.927937\n",
      "step 2294, loss: 0.927560\n",
      "step 2295, loss: 0.927185\n",
      "step 2296, loss: 0.926810\n",
      "step 2297, loss: 0.926437\n",
      "step 2298, loss: 0.926064\n",
      "step 2299, loss: 0.925692\n",
      "step 2300, loss: 0.925321\n",
      "step 2301, loss: 0.924950\n",
      "step 2302, loss: 0.924580\n",
      "step 2303, loss: 0.924212\n",
      "step 2304, loss: 0.923844\n",
      "step 2305, loss: 0.923476\n",
      "step 2306, loss: 0.923110\n",
      "step 2307, loss: 0.922744\n",
      "step 2308, loss: 0.922380\n",
      "step 2309, loss: 0.922015\n",
      "step 2310, loss: 0.921652\n",
      "step 2311, loss: 0.921289\n",
      "step 2312, loss: 0.920928\n",
      "step 2313, loss: 0.920567\n",
      "step 2314, loss: 0.920207\n",
      "step 2315, loss: 0.919847\n",
      "step 2316, loss: 0.919488\n",
      "step 2317, loss: 0.919131\n",
      "step 2318, loss: 0.918773\n",
      "step 2319, loss: 0.918417\n",
      "step 2320, loss: 0.918061\n",
      "step 2321, loss: 0.917706\n",
      "step 2322, loss: 0.917352\n",
      "step 2323, loss: 0.916999\n",
      "step 2324, loss: 0.916646\n",
      "step 2325, loss: 0.916294\n",
      "step 2326, loss: 0.915942\n",
      "step 2327, loss: 0.915592\n",
      "step 2328, loss: 0.915242\n",
      "step 2329, loss: 0.914893\n",
      "step 2330, loss: 0.914544\n",
      "step 2331, loss: 0.914197\n",
      "step 2332, loss: 0.913850\n",
      "step 2333, loss: 0.913503\n",
      "step 2334, loss: 0.913158\n",
      "step 2335, loss: 0.912813\n",
      "step 2336, loss: 0.912469\n",
      "step 2337, loss: 0.912125\n",
      "step 2338, loss: 0.911782\n",
      "step 2339, loss: 0.911441\n",
      "step 2340, loss: 0.911099\n",
      "step 2341, loss: 0.910758\n",
      "step 2342, loss: 0.910418\n",
      "step 2343, loss: 0.910079\n",
      "step 2344, loss: 0.909740\n",
      "step 2345, loss: 0.909402\n",
      "step 2346, loss: 0.909064\n",
      "step 2347, loss: 0.908728\n",
      "step 2348, loss: 0.908392\n",
      "step 2349, loss: 0.908056\n",
      "step 2350, loss: 0.907722\n",
      "step 2351, loss: 0.907387\n",
      "step 2352, loss: 0.907054\n",
      "step 2353, loss: 0.906721\n",
      "step 2354, loss: 0.906389\n",
      "step 2355, loss: 0.906058\n",
      "step 2356, loss: 0.905727\n",
      "step 2357, loss: 0.905396\n",
      "step 2358, loss: 0.905067\n",
      "step 2359, loss: 0.904738\n",
      "step 2360, loss: 0.904410\n",
      "step 2361, loss: 0.904082\n",
      "step 2362, loss: 0.903755\n",
      "step 2363, loss: 0.903428\n",
      "step 2364, loss: 0.903103\n",
      "step 2365, loss: 0.902778\n",
      "step 2366, loss: 0.902453\n",
      "step 2367, loss: 0.902129\n",
      "step 2368, loss: 0.901805\n",
      "step 2369, loss: 0.901483\n",
      "step 2370, loss: 0.901160\n",
      "step 2371, loss: 0.900839\n",
      "step 2372, loss: 0.900517\n",
      "step 2373, loss: 0.900197\n",
      "step 2374, loss: 0.899877\n",
      "step 2375, loss: 0.899558\n",
      "step 2376, loss: 0.899239\n",
      "step 2377, loss: 0.898921\n",
      "step 2378, loss: 0.898604\n",
      "step 2379, loss: 0.898287\n",
      "step 2380, loss: 0.897971\n",
      "step 2381, loss: 0.897654\n",
      "step 2382, loss: 0.897339\n",
      "step 2383, loss: 0.897025\n",
      "step 2384, loss: 0.896711\n",
      "step 2385, loss: 0.896397\n",
      "step 2386, loss: 0.896084\n",
      "step 2387, loss: 0.895772\n",
      "step 2388, loss: 0.895460\n",
      "step 2389, loss: 0.895149\n",
      "step 2390, loss: 0.894838\n",
      "step 2391, loss: 0.894528\n",
      "step 2392, loss: 0.894218\n",
      "step 2393, loss: 0.893909\n",
      "step 2394, loss: 0.893600\n",
      "step 2395, loss: 0.893292\n",
      "step 2396, loss: 0.892985\n",
      "step 2397, loss: 0.892677\n",
      "step 2398, loss: 0.892371\n",
      "step 2399, loss: 0.892065\n",
      "step 2400, loss: 0.891760\n",
      "step 2401, loss: 0.891455\n",
      "step 2402, loss: 0.891150\n",
      "step 2403, loss: 0.890847\n",
      "step 2404, loss: 0.890544\n",
      "step 2405, loss: 0.890241\n",
      "step 2406, loss: 0.889938\n",
      "step 2407, loss: 0.889636\n",
      "step 2408, loss: 0.889335\n",
      "step 2409, loss: 0.889035\n",
      "step 2410, loss: 0.888734\n",
      "step 2411, loss: 0.888434\n",
      "step 2412, loss: 0.888135\n",
      "step 2413, loss: 0.887836\n",
      "step 2414, loss: 0.887538\n",
      "step 2415, loss: 0.887240\n",
      "step 2416, loss: 0.886943\n",
      "step 2417, loss: 0.886645\n",
      "step 2418, loss: 0.886349\n",
      "step 2419, loss: 0.886053\n",
      "step 2420, loss: 0.885758\n",
      "step 2421, loss: 0.885463\n",
      "step 2422, loss: 0.885169\n",
      "step 2423, loss: 0.884875\n",
      "step 2424, loss: 0.884581\n",
      "step 2425, loss: 0.884288\n",
      "step 2426, loss: 0.883995\n",
      "step 2427, loss: 0.883704\n",
      "step 2428, loss: 0.883412\n",
      "step 2429, loss: 0.883121\n",
      "step 2430, loss: 0.882830\n",
      "step 2431, loss: 0.882540\n",
      "step 2432, loss: 0.882250\n",
      "step 2433, loss: 0.881961\n",
      "step 2434, loss: 0.881671\n",
      "step 2435, loss: 0.881383\n",
      "step 2436, loss: 0.881095\n",
      "step 2437, loss: 0.880807\n",
      "step 2438, loss: 0.880520\n",
      "step 2439, loss: 0.880233\n",
      "step 2440, loss: 0.879947\n",
      "step 2441, loss: 0.879661\n",
      "step 2442, loss: 0.879376\n",
      "step 2443, loss: 0.879091\n",
      "step 2444, loss: 0.878806\n",
      "step 2445, loss: 0.878522\n",
      "step 2446, loss: 0.878238\n",
      "step 2447, loss: 0.877954\n",
      "step 2448, loss: 0.877672\n",
      "step 2449, loss: 0.877389\n",
      "step 2450, loss: 0.877107\n",
      "step 2451, loss: 0.876826\n",
      "step 2452, loss: 0.876544\n",
      "step 2453, loss: 0.876263\n",
      "step 2454, loss: 0.875983\n",
      "step 2455, loss: 0.875703\n",
      "step 2456, loss: 0.875423\n",
      "step 2457, loss: 0.875144\n",
      "step 2458, loss: 0.874865\n",
      "step 2459, loss: 0.874587\n",
      "step 2460, loss: 0.874309\n",
      "step 2461, loss: 0.874031\n",
      "step 2462, loss: 0.873754\n",
      "step 2463, loss: 0.873477\n",
      "step 2464, loss: 0.873201\n",
      "step 2465, loss: 0.872924\n",
      "step 2466, loss: 0.872649\n",
      "step 2467, loss: 0.872373\n",
      "step 2468, loss: 0.872098\n",
      "step 2469, loss: 0.871823\n",
      "step 2470, loss: 0.871549\n",
      "step 2471, loss: 0.871276\n",
      "step 2472, loss: 0.871002\n",
      "step 2473, loss: 0.870729\n",
      "step 2474, loss: 0.870456\n",
      "step 2475, loss: 0.870184\n",
      "step 2476, loss: 0.869912\n",
      "step 2477, loss: 0.869640\n",
      "step 2478, loss: 0.869369\n",
      "step 2479, loss: 0.869098\n",
      "step 2480, loss: 0.868828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2481, loss: 0.868557\n",
      "step 2482, loss: 0.868287\n",
      "step 2483, loss: 0.868018\n",
      "step 2484, loss: 0.867748\n",
      "step 2485, loss: 0.867480\n",
      "step 2486, loss: 0.867211\n",
      "step 2487, loss: 0.866944\n",
      "step 2488, loss: 0.866675\n",
      "step 2489, loss: 0.866408\n",
      "step 2490, loss: 0.866141\n",
      "step 2491, loss: 0.865874\n",
      "step 2492, loss: 0.865608\n",
      "step 2493, loss: 0.865342\n",
      "step 2494, loss: 0.865076\n",
      "step 2495, loss: 0.864811\n",
      "step 2496, loss: 0.864545\n",
      "step 2497, loss: 0.864281\n",
      "step 2498, loss: 0.864016\n",
      "step 2499, loss: 0.863752\n",
      "step 2500, loss: 0.863488\n",
      "step 2501, loss: 0.863225\n",
      "step 2502, loss: 0.862962\n",
      "step 2503, loss: 0.862699\n",
      "step 2504, loss: 0.862436\n",
      "step 2505, loss: 0.862174\n",
      "step 2506, loss: 0.861912\n",
      "step 2507, loss: 0.861651\n",
      "step 2508, loss: 0.861389\n",
      "step 2509, loss: 0.861128\n",
      "step 2510, loss: 0.860868\n",
      "step 2511, loss: 0.860607\n",
      "step 2512, loss: 0.860347\n",
      "step 2513, loss: 0.860088\n",
      "step 2514, loss: 0.859828\n",
      "step 2515, loss: 0.859569\n",
      "step 2516, loss: 0.859310\n",
      "step 2517, loss: 0.859051\n",
      "step 2518, loss: 0.858793\n",
      "step 2519, loss: 0.858535\n",
      "step 2520, loss: 0.858277\n",
      "step 2521, loss: 0.858020\n",
      "step 2522, loss: 0.857763\n",
      "step 2523, loss: 0.857506\n",
      "step 2524, loss: 0.857250\n",
      "step 2525, loss: 0.856993\n",
      "step 2526, loss: 0.856737\n",
      "step 2527, loss: 0.856482\n",
      "step 2528, loss: 0.856227\n",
      "step 2529, loss: 0.855971\n",
      "step 2530, loss: 0.855716\n",
      "step 2531, loss: 0.855462\n",
      "step 2532, loss: 0.855208\n",
      "step 2533, loss: 0.854953\n",
      "step 2534, loss: 0.854699\n",
      "step 2535, loss: 0.854446\n",
      "step 2536, loss: 0.854192\n",
      "step 2537, loss: 0.853940\n",
      "step 2538, loss: 0.853687\n",
      "step 2539, loss: 0.853434\n",
      "step 2540, loss: 0.853182\n",
      "step 2541, loss: 0.852930\n",
      "step 2542, loss: 0.852678\n",
      "step 2543, loss: 0.852427\n",
      "step 2544, loss: 0.852176\n",
      "step 2545, loss: 0.851925\n",
      "step 2546, loss: 0.851674\n",
      "step 2547, loss: 0.851423\n",
      "step 2548, loss: 0.851173\n",
      "step 2549, loss: 0.850924\n",
      "step 2550, loss: 0.850674\n",
      "step 2551, loss: 0.850424\n",
      "step 2552, loss: 0.850175\n",
      "step 2553, loss: 0.849926\n",
      "step 2554, loss: 0.849677\n",
      "step 2555, loss: 0.849429\n",
      "step 2556, loss: 0.849181\n",
      "step 2557, loss: 0.848933\n",
      "step 2558, loss: 0.848685\n",
      "step 2559, loss: 0.848437\n",
      "step 2560, loss: 0.848190\n",
      "step 2561, loss: 0.847943\n",
      "step 2562, loss: 0.847696\n",
      "step 2563, loss: 0.847450\n",
      "step 2564, loss: 0.847203\n",
      "step 2565, loss: 0.846957\n",
      "step 2566, loss: 0.846711\n",
      "step 2567, loss: 0.846465\n",
      "step 2568, loss: 0.846220\n",
      "step 2569, loss: 0.845975\n",
      "step 2570, loss: 0.845730\n",
      "step 2571, loss: 0.845485\n",
      "step 2572, loss: 0.845240\n",
      "step 2573, loss: 0.844996\n",
      "step 2574, loss: 0.844752\n",
      "step 2575, loss: 0.844507\n",
      "step 2576, loss: 0.844264\n",
      "step 2577, loss: 0.844021\n",
      "step 2578, loss: 0.843777\n",
      "step 2579, loss: 0.843534\n",
      "step 2580, loss: 0.843291\n",
      "step 2581, loss: 0.843048\n",
      "step 2582, loss: 0.842806\n",
      "step 2583, loss: 0.842563\n",
      "step 2584, loss: 0.842321\n",
      "step 2585, loss: 0.842079\n",
      "step 2586, loss: 0.841838\n",
      "step 2587, loss: 0.841596\n",
      "step 2588, loss: 0.841355\n",
      "step 2589, loss: 0.841114\n",
      "step 2590, loss: 0.840873\n",
      "step 2591, loss: 0.840632\n",
      "step 2592, loss: 0.840392\n",
      "step 2593, loss: 0.840151\n",
      "step 2594, loss: 0.839912\n",
      "step 2595, loss: 0.839671\n",
      "step 2596, loss: 0.839432\n",
      "step 2597, loss: 0.839192\n",
      "step 2598, loss: 0.838952\n",
      "step 2599, loss: 0.838714\n",
      "step 2600, loss: 0.838475\n",
      "step 2601, loss: 0.838235\n",
      "step 2602, loss: 0.837997\n",
      "step 2603, loss: 0.837759\n",
      "step 2604, loss: 0.837520\n",
      "step 2605, loss: 0.837282\n",
      "step 2606, loss: 0.837044\n",
      "step 2607, loss: 0.836807\n",
      "step 2608, loss: 0.836569\n",
      "step 2609, loss: 0.836331\n",
      "step 2610, loss: 0.836094\n",
      "step 2611, loss: 0.835857\n",
      "step 2612, loss: 0.835620\n",
      "step 2613, loss: 0.835383\n",
      "step 2614, loss: 0.835147\n",
      "step 2615, loss: 0.834911\n",
      "step 2616, loss: 0.834674\n",
      "step 2617, loss: 0.834438\n",
      "step 2618, loss: 0.834203\n",
      "step 2619, loss: 0.833967\n",
      "step 2620, loss: 0.833731\n",
      "step 2621, loss: 0.833496\n",
      "step 2622, loss: 0.833260\n",
      "step 2623, loss: 0.833025\n",
      "step 2624, loss: 0.832790\n",
      "step 2625, loss: 0.832556\n",
      "step 2626, loss: 0.832321\n",
      "step 2627, loss: 0.832087\n",
      "step 2628, loss: 0.831852\n",
      "step 2629, loss: 0.831618\n",
      "step 2630, loss: 0.831384\n",
      "step 2631, loss: 0.831150\n",
      "step 2632, loss: 0.830916\n",
      "step 2633, loss: 0.830683\n",
      "step 2634, loss: 0.830450\n",
      "step 2635, loss: 0.830216\n",
      "step 2636, loss: 0.829984\n",
      "step 2637, loss: 0.829750\n",
      "step 2638, loss: 0.829518\n",
      "step 2639, loss: 0.829285\n",
      "step 2640, loss: 0.829052\n",
      "step 2641, loss: 0.828820\n",
      "step 2642, loss: 0.828587\n",
      "step 2643, loss: 0.828355\n",
      "step 2644, loss: 0.828124\n",
      "step 2645, loss: 0.827892\n",
      "step 2646, loss: 0.827660\n",
      "step 2647, loss: 0.827428\n",
      "step 2648, loss: 0.827197\n",
      "step 2649, loss: 0.826966\n",
      "step 2650, loss: 0.826735\n",
      "step 2651, loss: 0.826504\n",
      "step 2652, loss: 0.826273\n",
      "step 2653, loss: 0.826042\n",
      "step 2654, loss: 0.825811\n",
      "step 2655, loss: 0.825581\n",
      "step 2656, loss: 0.825351\n",
      "step 2657, loss: 0.825120\n",
      "step 2658, loss: 0.824890\n",
      "step 2659, loss: 0.824661\n",
      "step 2660, loss: 0.824430\n",
      "step 2661, loss: 0.824201\n",
      "step 2662, loss: 0.823971\n",
      "step 2663, loss: 0.823741\n",
      "step 2664, loss: 0.823512\n",
      "step 2665, loss: 0.823283\n",
      "step 2666, loss: 0.823054\n",
      "step 2667, loss: 0.822825\n",
      "step 2668, loss: 0.822596\n",
      "step 2669, loss: 0.822367\n",
      "step 2670, loss: 0.822138\n",
      "step 2671, loss: 0.821910\n",
      "step 2672, loss: 0.821681\n",
      "step 2673, loss: 0.821453\n",
      "step 2674, loss: 0.821225\n",
      "step 2675, loss: 0.820997\n",
      "step 2676, loss: 0.820769\n",
      "step 2677, loss: 0.820541\n",
      "step 2678, loss: 0.820313\n",
      "step 2679, loss: 0.820086\n",
      "step 2680, loss: 0.819858\n",
      "step 2681, loss: 0.819631\n",
      "step 2682, loss: 0.819403\n",
      "step 2683, loss: 0.819176\n",
      "step 2684, loss: 0.818949\n",
      "step 2685, loss: 0.818722\n",
      "step 2686, loss: 0.818495\n",
      "step 2687, loss: 0.818268\n",
      "step 2688, loss: 0.818041\n",
      "step 2689, loss: 0.817815\n",
      "step 2690, loss: 0.817588\n",
      "step 2691, loss: 0.817362\n",
      "step 2692, loss: 0.817135\n",
      "step 2693, loss: 0.816909\n",
      "step 2694, loss: 0.816683\n",
      "step 2695, loss: 0.816457\n",
      "step 2696, loss: 0.816231\n",
      "step 2697, loss: 0.816005\n",
      "step 2698, loss: 0.815779\n",
      "step 2699, loss: 0.815554\n",
      "step 2700, loss: 0.815328\n",
      "step 2701, loss: 0.815103\n",
      "step 2702, loss: 0.814878\n",
      "step 2703, loss: 0.814652\n",
      "step 2704, loss: 0.814427\n",
      "step 2705, loss: 0.814202\n",
      "step 2706, loss: 0.813977\n",
      "step 2707, loss: 0.813752\n",
      "step 2708, loss: 0.813527\n",
      "step 2709, loss: 0.813302\n",
      "step 2710, loss: 0.813078\n",
      "step 2711, loss: 0.812853\n",
      "step 2712, loss: 0.812629\n",
      "step 2713, loss: 0.812404\n",
      "step 2714, loss: 0.812180\n",
      "step 2715, loss: 0.811956\n",
      "step 2716, loss: 0.811732\n",
      "step 2717, loss: 0.811508\n",
      "step 2718, loss: 0.811283\n",
      "step 2719, loss: 0.811060\n",
      "step 2720, loss: 0.810836\n",
      "step 2721, loss: 0.810612\n",
      "step 2722, loss: 0.810388\n",
      "step 2723, loss: 0.810165\n",
      "step 2724, loss: 0.809941\n",
      "step 2725, loss: 0.809718\n",
      "step 2726, loss: 0.809494\n",
      "step 2727, loss: 0.809271\n",
      "step 2728, loss: 0.809048\n",
      "step 2729, loss: 0.808825\n",
      "step 2730, loss: 0.808602\n",
      "step 2731, loss: 0.808379\n",
      "step 2732, loss: 0.808156\n",
      "step 2733, loss: 0.807933\n",
      "step 2734, loss: 0.807710\n",
      "step 2735, loss: 0.807487\n",
      "step 2736, loss: 0.807264\n",
      "step 2737, loss: 0.807042\n",
      "step 2738, loss: 0.806819\n",
      "step 2739, loss: 0.806597\n",
      "step 2740, loss: 0.806374\n",
      "step 2741, loss: 0.806152\n",
      "step 2742, loss: 0.805930\n",
      "step 2743, loss: 0.805708\n",
      "step 2744, loss: 0.805486\n",
      "step 2745, loss: 0.805264\n",
      "step 2746, loss: 0.805041\n",
      "step 2747, loss: 0.804820\n",
      "step 2748, loss: 0.804598\n",
      "step 2749, loss: 0.804376\n",
      "step 2750, loss: 0.804154\n",
      "step 2751, loss: 0.803933\n",
      "step 2752, loss: 0.803711\n",
      "step 2753, loss: 0.803489\n",
      "step 2754, loss: 0.803268\n",
      "step 2755, loss: 0.803046\n",
      "step 2756, loss: 0.802825\n",
      "step 2757, loss: 0.802603\n",
      "step 2758, loss: 0.802382\n",
      "step 2759, loss: 0.802161\n",
      "step 2760, loss: 0.801940\n",
      "step 2761, loss: 0.801718\n",
      "step 2762, loss: 0.801497\n",
      "step 2763, loss: 0.801276\n",
      "step 2764, loss: 0.801055\n",
      "step 2765, loss: 0.800834\n",
      "step 2766, loss: 0.800614\n",
      "step 2767, loss: 0.800393\n",
      "step 2768, loss: 0.800172\n",
      "step 2769, loss: 0.799951\n",
      "step 2770, loss: 0.799731\n",
      "step 2771, loss: 0.799510\n",
      "step 2772, loss: 0.799289\n",
      "step 2773, loss: 0.799069\n",
      "step 2774, loss: 0.798848\n",
      "step 2775, loss: 0.798628\n",
      "step 2776, loss: 0.798408\n",
      "step 2777, loss: 0.798187\n",
      "step 2778, loss: 0.797967\n",
      "step 2779, loss: 0.797747\n",
      "step 2780, loss: 0.797527\n",
      "step 2781, loss: 0.797306\n",
      "step 2782, loss: 0.797086\n",
      "step 2783, loss: 0.796866\n",
      "step 2784, loss: 0.796646\n",
      "step 2785, loss: 0.796426\n",
      "step 2786, loss: 0.796206\n",
      "step 2787, loss: 0.795986\n",
      "step 2788, loss: 0.795767\n",
      "step 2789, loss: 0.795546\n",
      "step 2790, loss: 0.795327\n",
      "step 2791, loss: 0.795107\n",
      "step 2792, loss: 0.794888\n",
      "step 2793, loss: 0.794668\n",
      "step 2794, loss: 0.794448\n",
      "step 2795, loss: 0.794229\n",
      "step 2796, loss: 0.794009\n",
      "step 2797, loss: 0.793790\n",
      "step 2798, loss: 0.793570\n",
      "step 2799, loss: 0.793351\n",
      "step 2800, loss: 0.793131\n",
      "step 2801, loss: 0.792912\n",
      "step 2802, loss: 0.792692\n",
      "step 2803, loss: 0.792473\n",
      "step 2804, loss: 0.792254\n",
      "step 2805, loss: 0.792034\n",
      "step 2806, loss: 0.791815\n",
      "step 2807, loss: 0.791596\n",
      "step 2808, loss: 0.791377\n",
      "step 2809, loss: 0.791158\n",
      "step 2810, loss: 0.790939\n",
      "step 2811, loss: 0.790720\n",
      "step 2812, loss: 0.790501\n",
      "step 2813, loss: 0.790282\n",
      "step 2814, loss: 0.790063\n",
      "step 2815, loss: 0.789844\n",
      "step 2816, loss: 0.789625\n",
      "step 2817, loss: 0.789406\n",
      "step 2818, loss: 0.789188\n",
      "step 2819, loss: 0.788969\n",
      "step 2820, loss: 0.788750\n",
      "step 2821, loss: 0.788531\n",
      "step 2822, loss: 0.788312\n",
      "step 2823, loss: 0.788094\n",
      "step 2824, loss: 0.787875\n",
      "step 2825, loss: 0.787656\n",
      "step 2826, loss: 0.787437\n",
      "step 2827, loss: 0.787219\n",
      "step 2828, loss: 0.787000\n",
      "step 2829, loss: 0.786782\n",
      "step 2830, loss: 0.786563\n",
      "step 2831, loss: 0.786345\n",
      "step 2832, loss: 0.786126\n",
      "step 2833, loss: 0.785908\n",
      "step 2834, loss: 0.785689\n",
      "step 2835, loss: 0.785471\n",
      "step 2836, loss: 0.785252\n",
      "step 2837, loss: 0.785034\n",
      "step 2838, loss: 0.784816\n",
      "step 2839, loss: 0.784597\n",
      "step 2840, loss: 0.784379\n",
      "step 2841, loss: 0.784161\n",
      "step 2842, loss: 0.783942\n",
      "step 2843, loss: 0.783724\n",
      "step 2844, loss: 0.783505\n",
      "step 2845, loss: 0.783287\n",
      "step 2846, loss: 0.783069\n",
      "step 2847, loss: 0.782851\n",
      "step 2848, loss: 0.782632\n",
      "step 2849, loss: 0.782415\n",
      "step 2850, loss: 0.782196\n",
      "step 2851, loss: 0.781978\n",
      "step 2852, loss: 0.781760\n",
      "step 2853, loss: 0.781542\n",
      "step 2854, loss: 0.781323\n",
      "step 2855, loss: 0.781106\n",
      "step 2856, loss: 0.780888\n",
      "step 2857, loss: 0.780669\n",
      "step 2858, loss: 0.780451\n",
      "step 2859, loss: 0.780233\n",
      "step 2860, loss: 0.780015\n",
      "step 2861, loss: 0.779797\n",
      "step 2862, loss: 0.779579\n",
      "step 2863, loss: 0.779361\n",
      "step 2864, loss: 0.779143\n",
      "step 2865, loss: 0.778925\n",
      "step 2866, loss: 0.778707\n",
      "step 2867, loss: 0.778489\n",
      "step 2868, loss: 0.778271\n",
      "step 2869, loss: 0.778053\n",
      "step 2870, loss: 0.777836\n",
      "step 2871, loss: 0.777617\n",
      "step 2872, loss: 0.777400\n",
      "step 2873, loss: 0.777182\n",
      "step 2874, loss: 0.776963\n",
      "step 2875, loss: 0.776746\n",
      "step 2876, loss: 0.776528\n",
      "step 2877, loss: 0.776310\n",
      "step 2878, loss: 0.776092\n",
      "step 2879, loss: 0.775874\n",
      "step 2880, loss: 0.775656\n",
      "step 2881, loss: 0.775438\n",
      "step 2882, loss: 0.775221\n",
      "step 2883, loss: 0.775003\n",
      "step 2884, loss: 0.774785\n",
      "step 2885, loss: 0.774567\n",
      "step 2886, loss: 0.774349\n",
      "step 2887, loss: 0.774131\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2888, loss: 0.773914\n",
      "step 2889, loss: 0.773696\n",
      "step 2890, loss: 0.773478\n",
      "step 2891, loss: 0.773260\n",
      "step 2892, loss: 0.773043\n",
      "step 2893, loss: 0.772825\n",
      "step 2894, loss: 0.772607\n",
      "step 2895, loss: 0.772389\n",
      "step 2896, loss: 0.772171\n",
      "step 2897, loss: 0.771953\n",
      "step 2898, loss: 0.771736\n",
      "step 2899, loss: 0.771518\n",
      "step 2900, loss: 0.771300\n",
      "step 2901, loss: 0.771082\n",
      "step 2902, loss: 0.770865\n",
      "step 2903, loss: 0.770647\n",
      "step 2904, loss: 0.770429\n",
      "step 2905, loss: 0.770211\n",
      "step 2906, loss: 0.769993\n",
      "step 2907, loss: 0.769776\n",
      "step 2908, loss: 0.769557\n",
      "step 2909, loss: 0.769340\n",
      "step 2910, loss: 0.769122\n",
      "step 2911, loss: 0.768904\n",
      "step 2912, loss: 0.768686\n",
      "step 2913, loss: 0.768468\n",
      "step 2914, loss: 0.768251\n",
      "step 2915, loss: 0.768033\n",
      "step 2916, loss: 0.767815\n",
      "step 2917, loss: 0.767597\n",
      "step 2918, loss: 0.767379\n",
      "step 2919, loss: 0.767161\n",
      "step 2920, loss: 0.766944\n",
      "step 2921, loss: 0.766726\n",
      "step 2922, loss: 0.766508\n",
      "step 2923, loss: 0.766290\n",
      "step 2924, loss: 0.766072\n",
      "step 2925, loss: 0.765854\n",
      "step 2926, loss: 0.765636\n",
      "step 2927, loss: 0.765419\n",
      "step 2928, loss: 0.765200\n",
      "step 2929, loss: 0.764983\n",
      "step 2930, loss: 0.764765\n",
      "step 2931, loss: 0.764547\n",
      "step 2932, loss: 0.764329\n",
      "step 2933, loss: 0.764111\n",
      "step 2934, loss: 0.763893\n",
      "step 2935, loss: 0.763675\n",
      "step 2936, loss: 0.763457\n",
      "step 2937, loss: 0.763239\n",
      "step 2938, loss: 0.763022\n",
      "step 2939, loss: 0.762803\n",
      "step 2940, loss: 0.762585\n",
      "step 2941, loss: 0.762367\n",
      "step 2942, loss: 0.762150\n",
      "step 2943, loss: 0.761932\n",
      "step 2944, loss: 0.761713\n",
      "step 2945, loss: 0.761495\n",
      "step 2946, loss: 0.761277\n",
      "step 2947, loss: 0.761059\n",
      "step 2948, loss: 0.760842\n",
      "step 2949, loss: 0.760623\n",
      "step 2950, loss: 0.760405\n",
      "step 2951, loss: 0.760187\n",
      "step 2952, loss: 0.759969\n",
      "step 2953, loss: 0.759751\n",
      "step 2954, loss: 0.759533\n",
      "step 2955, loss: 0.759315\n",
      "step 2956, loss: 0.759096\n",
      "step 2957, loss: 0.758878\n",
      "step 2958, loss: 0.758660\n",
      "step 2959, loss: 0.758442\n",
      "step 2960, loss: 0.758224\n",
      "step 2961, loss: 0.758005\n",
      "step 2962, loss: 0.757787\n",
      "step 2963, loss: 0.757569\n",
      "step 2964, loss: 0.757351\n",
      "step 2965, loss: 0.757133\n",
      "step 2966, loss: 0.756914\n",
      "step 2967, loss: 0.756696\n",
      "step 2968, loss: 0.756477\n",
      "step 2969, loss: 0.756259\n",
      "step 2970, loss: 0.756041\n",
      "step 2971, loss: 0.755823\n",
      "step 2972, loss: 0.755605\n",
      "step 2973, loss: 0.755386\n",
      "step 2974, loss: 0.755168\n",
      "step 2975, loss: 0.754949\n",
      "step 2976, loss: 0.754731\n",
      "step 2977, loss: 0.754513\n",
      "step 2978, loss: 0.754294\n",
      "step 2979, loss: 0.754075\n",
      "step 2980, loss: 0.753857\n",
      "step 2981, loss: 0.753638\n",
      "step 2982, loss: 0.753420\n",
      "step 2983, loss: 0.753201\n",
      "step 2984, loss: 0.752983\n",
      "step 2985, loss: 0.752764\n",
      "step 2986, loss: 0.752546\n",
      "step 2987, loss: 0.752327\n",
      "step 2988, loss: 0.752108\n",
      "step 2989, loss: 0.751890\n",
      "step 2990, loss: 0.751671\n",
      "step 2991, loss: 0.751453\n",
      "step 2992, loss: 0.751234\n",
      "step 2993, loss: 0.751015\n",
      "step 2994, loss: 0.750796\n",
      "step 2995, loss: 0.750578\n",
      "step 2996, loss: 0.750359\n",
      "step 2997, loss: 0.750140\n",
      "step 2998, loss: 0.749921\n",
      "step 2999, loss: 0.749703\n",
      "step 3000, loss: 0.749484\n",
      "step 3001, loss: 0.749265\n",
      "step 3002, loss: 0.749046\n",
      "step 3003, loss: 0.748827\n",
      "step 3004, loss: 0.748608\n",
      "step 3005, loss: 0.748389\n",
      "step 3006, loss: 0.748170\n",
      "step 3007, loss: 0.747951\n",
      "step 3008, loss: 0.747732\n",
      "step 3009, loss: 0.747513\n",
      "step 3010, loss: 0.747294\n",
      "step 3011, loss: 0.747075\n",
      "step 3012, loss: 0.746856\n",
      "step 3013, loss: 0.746637\n",
      "step 3014, loss: 0.746418\n",
      "step 3015, loss: 0.746199\n",
      "step 3016, loss: 0.745980\n",
      "step 3017, loss: 0.745760\n",
      "step 3018, loss: 0.745541\n",
      "step 3019, loss: 0.745322\n",
      "step 3020, loss: 0.745103\n",
      "step 3021, loss: 0.744884\n",
      "step 3022, loss: 0.744664\n",
      "step 3023, loss: 0.744445\n",
      "step 3024, loss: 0.744226\n",
      "step 3025, loss: 0.744006\n",
      "step 3026, loss: 0.743787\n",
      "step 3027, loss: 0.743567\n",
      "step 3028, loss: 0.743348\n",
      "step 3029, loss: 0.743129\n",
      "step 3030, loss: 0.742909\n",
      "step 3031, loss: 0.742690\n",
      "step 3032, loss: 0.742470\n",
      "step 3033, loss: 0.742251\n",
      "step 3034, loss: 0.742031\n",
      "step 3035, loss: 0.741812\n",
      "step 3036, loss: 0.741592\n",
      "step 3037, loss: 0.741372\n",
      "step 3038, loss: 0.741153\n",
      "step 3039, loss: 0.740933\n",
      "step 3040, loss: 0.740713\n",
      "step 3041, loss: 0.740494\n",
      "step 3042, loss: 0.740274\n",
      "step 3043, loss: 0.740054\n",
      "step 3044, loss: 0.739835\n",
      "step 3045, loss: 0.739615\n",
      "step 3046, loss: 0.739395\n",
      "step 3047, loss: 0.739175\n",
      "step 3048, loss: 0.738955\n",
      "step 3049, loss: 0.738735\n",
      "step 3050, loss: 0.738515\n",
      "step 3051, loss: 0.738296\n",
      "step 3052, loss: 0.738075\n",
      "step 3053, loss: 0.737856\n",
      "step 3054, loss: 0.737635\n",
      "step 3055, loss: 0.737415\n",
      "step 3056, loss: 0.737195\n",
      "step 3057, loss: 0.736975\n",
      "step 3058, loss: 0.736755\n",
      "step 3059, loss: 0.736535\n",
      "step 3060, loss: 0.736315\n",
      "step 3061, loss: 0.736094\n",
      "step 3062, loss: 0.735874\n",
      "step 3063, loss: 0.735654\n",
      "step 3064, loss: 0.735434\n",
      "step 3065, loss: 0.735213\n",
      "step 3066, loss: 0.734993\n",
      "step 3067, loss: 0.734773\n",
      "step 3068, loss: 0.734552\n",
      "step 3069, loss: 0.734332\n",
      "step 3070, loss: 0.734111\n",
      "step 3071, loss: 0.733891\n",
      "step 3072, loss: 0.733671\n",
      "step 3073, loss: 0.733450\n",
      "step 3074, loss: 0.733229\n",
      "step 3075, loss: 0.733009\n",
      "step 3076, loss: 0.732788\n",
      "step 3077, loss: 0.732568\n",
      "step 3078, loss: 0.732347\n",
      "step 3079, loss: 0.732126\n",
      "step 3080, loss: 0.731905\n",
      "step 3081, loss: 0.731685\n",
      "step 3082, loss: 0.731464\n",
      "step 3083, loss: 0.731243\n",
      "step 3084, loss: 0.731022\n",
      "step 3085, loss: 0.730801\n",
      "step 3086, loss: 0.730581\n",
      "step 3087, loss: 0.730360\n",
      "step 3088, loss: 0.730139\n",
      "step 3089, loss: 0.729918\n",
      "step 3090, loss: 0.729697\n",
      "step 3091, loss: 0.729476\n",
      "step 3092, loss: 0.729255\n",
      "step 3093, loss: 0.729034\n",
      "step 3094, loss: 0.728813\n",
      "step 3095, loss: 0.728591\n",
      "step 3096, loss: 0.728370\n",
      "step 3097, loss: 0.728149\n",
      "step 3098, loss: 0.727928\n",
      "step 3099, loss: 0.727707\n",
      "step 3100, loss: 0.727486\n",
      "step 3101, loss: 0.727264\n",
      "step 3102, loss: 0.727043\n",
      "step 3103, loss: 0.726822\n",
      "step 3104, loss: 0.726600\n",
      "step 3105, loss: 0.726379\n",
      "step 3106, loss: 0.726157\n",
      "step 3107, loss: 0.725936\n",
      "step 3108, loss: 0.725715\n",
      "step 3109, loss: 0.725493\n",
      "step 3110, loss: 0.725271\n",
      "step 3111, loss: 0.725050\n",
      "step 3112, loss: 0.724828\n",
      "step 3113, loss: 0.724607\n",
      "step 3114, loss: 0.724385\n",
      "step 3115, loss: 0.724163\n",
      "step 3116, loss: 0.723942\n",
      "step 3117, loss: 0.723720\n",
      "step 3118, loss: 0.723498\n",
      "step 3119, loss: 0.723276\n",
      "step 3120, loss: 0.723054\n",
      "step 3121, loss: 0.722832\n",
      "step 3122, loss: 0.722610\n",
      "step 3123, loss: 0.722389\n",
      "step 3124, loss: 0.722167\n",
      "step 3125, loss: 0.721945\n",
      "step 3126, loss: 0.721722\n",
      "step 3127, loss: 0.721500\n",
      "step 3128, loss: 0.721278\n",
      "step 3129, loss: 0.721057\n",
      "step 3130, loss: 0.720834\n",
      "step 3131, loss: 0.720612\n",
      "step 3132, loss: 0.720390\n",
      "step 3133, loss: 0.720167\n",
      "step 3134, loss: 0.719945\n",
      "step 3135, loss: 0.719723\n",
      "step 3136, loss: 0.719501\n",
      "step 3137, loss: 0.719278\n",
      "step 3138, loss: 0.719056\n",
      "step 3139, loss: 0.718834\n",
      "step 3140, loss: 0.718611\n",
      "step 3141, loss: 0.718389\n",
      "step 3142, loss: 0.718166\n",
      "step 3143, loss: 0.717944\n",
      "step 3144, loss: 0.717721\n",
      "step 3145, loss: 0.717499\n",
      "step 3146, loss: 0.717276\n",
      "step 3147, loss: 0.717054\n",
      "step 3148, loss: 0.716831\n",
      "step 3149, loss: 0.716608\n",
      "step 3150, loss: 0.716385\n",
      "step 3151, loss: 0.716162\n",
      "step 3152, loss: 0.715940\n",
      "step 3153, loss: 0.715717\n",
      "step 3154, loss: 0.715494\n",
      "step 3155, loss: 0.715271\n",
      "step 3156, loss: 0.715048\n",
      "step 3157, loss: 0.714825\n",
      "step 3158, loss: 0.714602\n",
      "step 3159, loss: 0.714379\n",
      "step 3160, loss: 0.714156\n",
      "step 3161, loss: 0.713933\n",
      "step 3162, loss: 0.713710\n",
      "step 3163, loss: 0.713487\n",
      "step 3164, loss: 0.713264\n",
      "step 3165, loss: 0.713040\n",
      "step 3166, loss: 0.712817\n",
      "step 3167, loss: 0.712594\n",
      "step 3168, loss: 0.712370\n",
      "step 3169, loss: 0.712147\n",
      "step 3170, loss: 0.711924\n",
      "step 3171, loss: 0.711701\n",
      "step 3172, loss: 0.711477\n",
      "step 3173, loss: 0.711253\n",
      "step 3174, loss: 0.711030\n",
      "step 3175, loss: 0.710806\n",
      "step 3176, loss: 0.710583\n",
      "step 3177, loss: 0.710359\n",
      "step 3178, loss: 0.710136\n",
      "step 3179, loss: 0.709912\n",
      "step 3180, loss: 0.709688\n",
      "step 3181, loss: 0.709465\n",
      "step 3182, loss: 0.709241\n",
      "step 3183, loss: 0.709017\n",
      "step 3184, loss: 0.708794\n",
      "step 3185, loss: 0.708570\n",
      "step 3186, loss: 0.708346\n",
      "step 3187, loss: 0.708122\n",
      "step 3188, loss: 0.707898\n",
      "step 3189, loss: 0.707674\n",
      "step 3190, loss: 0.707450\n",
      "step 3191, loss: 0.707226\n",
      "step 3192, loss: 0.707002\n",
      "step 3193, loss: 0.706778\n",
      "step 3194, loss: 0.706554\n",
      "step 3195, loss: 0.706330\n",
      "step 3196, loss: 0.706105\n",
      "step 3197, loss: 0.705881\n",
      "step 3198, loss: 0.705657\n",
      "step 3199, loss: 0.705433\n",
      "step 3200, loss: 0.705208\n",
      "step 3201, loss: 0.704984\n",
      "step 3202, loss: 0.704759\n",
      "step 3203, loss: 0.704535\n",
      "step 3204, loss: 0.704311\n",
      "step 3205, loss: 0.704086\n",
      "step 3206, loss: 0.703862\n",
      "step 3207, loss: 0.703637\n",
      "step 3208, loss: 0.703413\n",
      "step 3209, loss: 0.703188\n",
      "step 3210, loss: 0.702963\n",
      "step 3211, loss: 0.702739\n",
      "step 3212, loss: 0.702514\n",
      "step 3213, loss: 0.702289\n",
      "step 3214, loss: 0.702065\n",
      "step 3215, loss: 0.701840\n",
      "step 3216, loss: 0.701615\n",
      "step 3217, loss: 0.701390\n",
      "step 3218, loss: 0.701165\n",
      "step 3219, loss: 0.700940\n",
      "step 3220, loss: 0.700715\n",
      "step 3221, loss: 0.700490\n",
      "step 3222, loss: 0.700265\n",
      "step 3223, loss: 0.700040\n",
      "step 3224, loss: 0.699815\n",
      "step 3225, loss: 0.699590\n",
      "step 3226, loss: 0.699365\n",
      "step 3227, loss: 0.699140\n",
      "step 3228, loss: 0.698915\n",
      "step 3229, loss: 0.698689\n",
      "step 3230, loss: 0.698464\n",
      "step 3231, loss: 0.698239\n",
      "step 3232, loss: 0.698013\n",
      "step 3233, loss: 0.697788\n",
      "step 3234, loss: 0.697563\n",
      "step 3235, loss: 0.697337\n",
      "step 3236, loss: 0.697112\n",
      "step 3237, loss: 0.696886\n",
      "step 3238, loss: 0.696661\n",
      "step 3239, loss: 0.696435\n",
      "step 3240, loss: 0.696210\n",
      "step 3241, loss: 0.695984\n",
      "step 3242, loss: 0.695758\n",
      "step 3243, loss: 0.695533\n",
      "step 3244, loss: 0.695307\n",
      "step 3245, loss: 0.695081\n",
      "step 3246, loss: 0.694855\n",
      "step 3247, loss: 0.694630\n",
      "step 3248, loss: 0.694404\n",
      "step 3249, loss: 0.694178\n",
      "step 3250, loss: 0.693952\n",
      "step 3251, loss: 0.693726\n",
      "step 3252, loss: 0.693500\n",
      "step 3253, loss: 0.693274\n",
      "step 3254, loss: 0.693048\n",
      "step 3255, loss: 0.692822\n",
      "step 3256, loss: 0.692596\n",
      "step 3257, loss: 0.692370\n",
      "step 3258, loss: 0.692143\n",
      "step 3259, loss: 0.691917\n",
      "step 3260, loss: 0.691691\n",
      "step 3261, loss: 0.691465\n",
      "step 3262, loss: 0.691238\n",
      "step 3263, loss: 0.691012\n",
      "step 3264, loss: 0.690786\n",
      "step 3265, loss: 0.690559\n",
      "step 3266, loss: 0.690333\n",
      "step 3267, loss: 0.690107\n",
      "step 3268, loss: 0.689880\n",
      "step 3269, loss: 0.689653\n",
      "step 3270, loss: 0.689427\n",
      "step 3271, loss: 0.689200\n",
      "step 3272, loss: 0.688974\n",
      "step 3273, loss: 0.688747\n",
      "step 3274, loss: 0.688520\n",
      "step 3275, loss: 0.688294\n",
      "step 3276, loss: 0.688067\n",
      "step 3277, loss: 0.687840\n",
      "step 3278, loss: 0.687613\n",
      "step 3279, loss: 0.687386\n",
      "step 3280, loss: 0.687160\n",
      "step 3281, loss: 0.686933\n",
      "step 3282, loss: 0.686706\n",
      "step 3283, loss: 0.686479\n",
      "step 3284, loss: 0.686252\n",
      "step 3285, loss: 0.686025\n",
      "step 3286, loss: 0.685798\n",
      "step 3287, loss: 0.685571\n",
      "step 3288, loss: 0.685344\n",
      "step 3289, loss: 0.685116\n",
      "step 3290, loss: 0.684889\n",
      "step 3291, loss: 0.684662\n",
      "step 3292, loss: 0.684435\n",
      "step 3293, loss: 0.684207\n",
      "step 3294, loss: 0.683980\n",
      "step 3295, loss: 0.683753\n",
      "step 3296, loss: 0.683525\n",
      "step 3297, loss: 0.683298\n",
      "step 3298, loss: 0.683071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3299, loss: 0.682843\n",
      "step 3300, loss: 0.682615\n",
      "step 3301, loss: 0.682388\n",
      "step 3302, loss: 0.682161\n",
      "step 3303, loss: 0.681933\n",
      "step 3304, loss: 0.681705\n",
      "step 3305, loss: 0.681478\n",
      "step 3306, loss: 0.681250\n",
      "step 3307, loss: 0.681022\n",
      "step 3308, loss: 0.680794\n",
      "step 3309, loss: 0.680567\n",
      "step 3310, loss: 0.680339\n",
      "step 3311, loss: 0.680111\n",
      "step 3312, loss: 0.679883\n",
      "step 3313, loss: 0.679655\n",
      "step 3314, loss: 0.679427\n",
      "step 3315, loss: 0.679199\n",
      "step 3316, loss: 0.678971\n",
      "step 3317, loss: 0.678743\n",
      "step 3318, loss: 0.678515\n",
      "step 3319, loss: 0.678287\n",
      "step 3320, loss: 0.678059\n",
      "step 3321, loss: 0.677831\n",
      "step 3322, loss: 0.677603\n",
      "step 3323, loss: 0.677375\n",
      "step 3324, loss: 0.677146\n",
      "step 3325, loss: 0.676918\n",
      "step 3326, loss: 0.676689\n",
      "step 3327, loss: 0.676461\n",
      "step 3328, loss: 0.676233\n",
      "step 3329, loss: 0.676004\n",
      "step 3330, loss: 0.675776\n",
      "step 3331, loss: 0.675547\n",
      "step 3332, loss: 0.675319\n",
      "step 3333, loss: 0.675090\n",
      "step 3334, loss: 0.674862\n",
      "step 3335, loss: 0.674633\n",
      "step 3336, loss: 0.674405\n",
      "step 3337, loss: 0.674176\n",
      "step 3338, loss: 0.673947\n",
      "step 3339, loss: 0.673719\n",
      "step 3340, loss: 0.673490\n",
      "step 3341, loss: 0.673261\n",
      "step 3342, loss: 0.673032\n",
      "step 3343, loss: 0.672803\n",
      "step 3344, loss: 0.672574\n",
      "step 3345, loss: 0.672346\n",
      "step 3346, loss: 0.672116\n",
      "step 3347, loss: 0.671887\n",
      "step 3348, loss: 0.671659\n",
      "step 3349, loss: 0.671430\n",
      "step 3350, loss: 0.671201\n",
      "step 3351, loss: 0.670971\n",
      "step 3352, loss: 0.670742\n",
      "step 3353, loss: 0.670513\n",
      "step 3354, loss: 0.670284\n",
      "step 3355, loss: 0.670055\n",
      "step 3356, loss: 0.669825\n",
      "step 3357, loss: 0.669596\n",
      "step 3358, loss: 0.669367\n",
      "step 3359, loss: 0.669137\n",
      "step 3360, loss: 0.668908\n",
      "step 3361, loss: 0.668679\n",
      "step 3362, loss: 0.668449\n",
      "step 3363, loss: 0.668220\n",
      "step 3364, loss: 0.667990\n",
      "step 3365, loss: 0.667761\n",
      "step 3366, loss: 0.667531\n",
      "step 3367, loss: 0.667302\n",
      "step 3368, loss: 0.667072\n",
      "step 3369, loss: 0.666843\n",
      "step 3370, loss: 0.666613\n",
      "step 3371, loss: 0.666383\n",
      "step 3372, loss: 0.666153\n",
      "step 3373, loss: 0.665924\n",
      "step 3374, loss: 0.665694\n",
      "step 3375, loss: 0.665464\n",
      "step 3376, loss: 0.665234\n",
      "step 3377, loss: 0.665004\n",
      "step 3378, loss: 0.664775\n",
      "step 3379, loss: 0.664545\n",
      "step 3380, loss: 0.664315\n",
      "step 3381, loss: 0.664085\n",
      "step 3382, loss: 0.663855\n",
      "step 3383, loss: 0.663624\n",
      "step 3384, loss: 0.663394\n",
      "step 3385, loss: 0.663164\n",
      "step 3386, loss: 0.662934\n",
      "step 3387, loss: 0.662704\n",
      "step 3388, loss: 0.662474\n",
      "step 3389, loss: 0.662244\n",
      "step 3390, loss: 0.662013\n",
      "step 3391, loss: 0.661783\n",
      "step 3392, loss: 0.661553\n",
      "step 3393, loss: 0.661322\n",
      "step 3394, loss: 0.661092\n",
      "step 3395, loss: 0.660861\n",
      "step 3396, loss: 0.660631\n",
      "step 3397, loss: 0.660401\n",
      "step 3398, loss: 0.660170\n",
      "step 3399, loss: 0.659939\n",
      "step 3400, loss: 0.659709\n",
      "step 3401, loss: 0.659478\n",
      "step 3402, loss: 0.659248\n",
      "step 3403, loss: 0.659017\n",
      "step 3404, loss: 0.658787\n",
      "step 3405, loss: 0.658556\n",
      "step 3406, loss: 0.658325\n",
      "step 3407, loss: 0.658095\n",
      "step 3408, loss: 0.657864\n",
      "step 3409, loss: 0.657633\n",
      "step 3410, loss: 0.657402\n",
      "step 3411, loss: 0.657171\n",
      "step 3412, loss: 0.656940\n",
      "step 3413, loss: 0.656709\n",
      "step 3414, loss: 0.656478\n",
      "step 3415, loss: 0.656247\n",
      "step 3416, loss: 0.656016\n",
      "step 3417, loss: 0.655785\n",
      "step 3418, loss: 0.655554\n",
      "step 3419, loss: 0.655323\n",
      "step 3420, loss: 0.655092\n",
      "step 3421, loss: 0.654860\n",
      "step 3422, loss: 0.654629\n",
      "step 3423, loss: 0.654398\n",
      "step 3424, loss: 0.654167\n",
      "step 3425, loss: 0.653936\n",
      "step 3426, loss: 0.653705\n",
      "step 3427, loss: 0.653473\n",
      "step 3428, loss: 0.653242\n",
      "step 3429, loss: 0.653010\n",
      "step 3430, loss: 0.652779\n",
      "step 3431, loss: 0.652548\n",
      "step 3432, loss: 0.652316\n",
      "step 3433, loss: 0.652085\n",
      "step 3434, loss: 0.651853\n",
      "step 3435, loss: 0.651621\n",
      "step 3436, loss: 0.651390\n",
      "step 3437, loss: 0.651158\n",
      "step 3438, loss: 0.650926\n",
      "step 3439, loss: 0.650695\n",
      "step 3440, loss: 0.650463\n",
      "step 3441, loss: 0.650231\n",
      "step 3442, loss: 0.650000\n",
      "step 3443, loss: 0.649768\n",
      "step 3444, loss: 0.649536\n",
      "step 3445, loss: 0.649304\n",
      "step 3446, loss: 0.649072\n",
      "step 3447, loss: 0.648840\n",
      "step 3448, loss: 0.648609\n",
      "step 3449, loss: 0.648377\n",
      "step 3450, loss: 0.648145\n",
      "step 3451, loss: 0.647913\n",
      "step 3452, loss: 0.647681\n",
      "step 3453, loss: 0.647449\n",
      "step 3454, loss: 0.647216\n",
      "step 3455, loss: 0.646984\n",
      "step 3456, loss: 0.646752\n",
      "step 3457, loss: 0.646520\n",
      "step 3458, loss: 0.646288\n",
      "step 3459, loss: 0.646056\n",
      "step 3460, loss: 0.645824\n",
      "step 3461, loss: 0.645591\n",
      "step 3462, loss: 0.645359\n",
      "step 3463, loss: 0.645126\n",
      "step 3464, loss: 0.644894\n",
      "step 3465, loss: 0.644662\n",
      "step 3466, loss: 0.644429\n",
      "step 3467, loss: 0.644197\n",
      "step 3468, loss: 0.643965\n",
      "step 3469, loss: 0.643732\n",
      "step 3470, loss: 0.643499\n",
      "step 3471, loss: 0.643267\n",
      "step 3472, loss: 0.643034\n",
      "step 3473, loss: 0.642801\n",
      "step 3474, loss: 0.642569\n",
      "step 3475, loss: 0.642336\n",
      "step 3476, loss: 0.642104\n",
      "step 3477, loss: 0.641871\n",
      "step 3478, loss: 0.641638\n",
      "step 3479, loss: 0.641405\n",
      "step 3480, loss: 0.641173\n",
      "step 3481, loss: 0.640940\n",
      "step 3482, loss: 0.640707\n",
      "step 3483, loss: 0.640474\n",
      "step 3484, loss: 0.640241\n",
      "step 3485, loss: 0.640008\n",
      "step 3486, loss: 0.639776\n",
      "step 3487, loss: 0.639542\n",
      "step 3488, loss: 0.639309\n",
      "step 3489, loss: 0.639076\n",
      "step 3490, loss: 0.638843\n",
      "step 3491, loss: 0.638610\n",
      "step 3492, loss: 0.638377\n",
      "step 3493, loss: 0.638144\n",
      "step 3494, loss: 0.637911\n",
      "step 3495, loss: 0.637678\n",
      "step 3496, loss: 0.637444\n",
      "step 3497, loss: 0.637211\n",
      "step 3498, loss: 0.636978\n",
      "step 3499, loss: 0.636745\n",
      "step 3500, loss: 0.636511\n",
      "step 3501, loss: 0.636278\n",
      "step 3502, loss: 0.636045\n",
      "step 3503, loss: 0.635812\n",
      "step 3504, loss: 0.635578\n",
      "step 3505, loss: 0.635345\n",
      "step 3506, loss: 0.635111\n",
      "step 3507, loss: 0.634878\n",
      "step 3508, loss: 0.634644\n",
      "step 3509, loss: 0.634411\n",
      "step 3510, loss: 0.634177\n",
      "step 3511, loss: 0.633943\n",
      "step 3512, loss: 0.633710\n",
      "step 3513, loss: 0.633476\n",
      "step 3514, loss: 0.633243\n",
      "step 3515, loss: 0.633009\n",
      "step 3516, loss: 0.632775\n",
      "step 3517, loss: 0.632541\n",
      "step 3518, loss: 0.632308\n",
      "step 3519, loss: 0.632074\n",
      "step 3520, loss: 0.631840\n",
      "step 3521, loss: 0.631606\n",
      "step 3522, loss: 0.631373\n",
      "step 3523, loss: 0.631139\n",
      "step 3524, loss: 0.630905\n",
      "step 3525, loss: 0.630671\n",
      "step 3526, loss: 0.630437\n",
      "step 3527, loss: 0.630203\n",
      "step 3528, loss: 0.629969\n",
      "step 3529, loss: 0.629735\n",
      "step 3530, loss: 0.629501\n",
      "step 3531, loss: 0.629267\n",
      "step 3532, loss: 0.629033\n",
      "step 3533, loss: 0.628798\n",
      "step 3534, loss: 0.628564\n",
      "step 3535, loss: 0.628330\n",
      "step 3536, loss: 0.628096\n",
      "step 3537, loss: 0.627862\n",
      "step 3538, loss: 0.627628\n",
      "step 3539, loss: 0.627393\n",
      "step 3540, loss: 0.627159\n",
      "step 3541, loss: 0.626924\n",
      "step 3542, loss: 0.626690\n",
      "step 3543, loss: 0.626456\n",
      "step 3544, loss: 0.626222\n",
      "step 3545, loss: 0.625987\n",
      "step 3546, loss: 0.625753\n",
      "step 3547, loss: 0.625518\n",
      "step 3548, loss: 0.625284\n",
      "step 3549, loss: 0.625049\n",
      "step 3550, loss: 0.624815\n",
      "step 3551, loss: 0.624581\n",
      "step 3552, loss: 0.624346\n",
      "step 3553, loss: 0.624111\n",
      "step 3554, loss: 0.623876\n",
      "step 3555, loss: 0.623642\n",
      "step 3556, loss: 0.623407\n",
      "step 3557, loss: 0.623172\n",
      "step 3558, loss: 0.622938\n",
      "step 3559, loss: 0.622703\n",
      "step 3560, loss: 0.622468\n",
      "step 3561, loss: 0.622233\n",
      "step 3562, loss: 0.621998\n",
      "step 3563, loss: 0.621764\n",
      "step 3564, loss: 0.621529\n",
      "step 3565, loss: 0.621294\n",
      "step 3566, loss: 0.621059\n",
      "step 3567, loss: 0.620824\n",
      "step 3568, loss: 0.620589\n",
      "step 3569, loss: 0.620354\n",
      "step 3570, loss: 0.620119\n",
      "step 3571, loss: 0.619884\n",
      "step 3572, loss: 0.619649\n",
      "step 3573, loss: 0.619414\n",
      "step 3574, loss: 0.619179\n",
      "step 3575, loss: 0.618944\n",
      "step 3576, loss: 0.618709\n",
      "step 3577, loss: 0.618474\n",
      "step 3578, loss: 0.618239\n",
      "step 3579, loss: 0.618003\n",
      "step 3580, loss: 0.617768\n",
      "step 3581, loss: 0.617533\n",
      "step 3582, loss: 0.617298\n",
      "step 3583, loss: 0.617062\n",
      "step 3584, loss: 0.616827\n",
      "step 3585, loss: 0.616592\n",
      "step 3586, loss: 0.616357\n",
      "step 3587, loss: 0.616121\n",
      "step 3588, loss: 0.615886\n",
      "step 3589, loss: 0.615650\n",
      "step 3590, loss: 0.615415\n",
      "step 3591, loss: 0.615180\n",
      "step 3592, loss: 0.614944\n",
      "step 3593, loss: 0.614709\n",
      "step 3594, loss: 0.614473\n",
      "step 3595, loss: 0.614238\n",
      "step 3596, loss: 0.614002\n",
      "step 3597, loss: 0.613766\n",
      "step 3598, loss: 0.613531\n",
      "step 3599, loss: 0.613295\n",
      "step 3600, loss: 0.613060\n",
      "step 3601, loss: 0.612824\n",
      "step 3602, loss: 0.612588\n",
      "step 3603, loss: 0.612352\n",
      "step 3604, loss: 0.612117\n",
      "step 3605, loss: 0.611881\n",
      "step 3606, loss: 0.611645\n",
      "step 3607, loss: 0.611410\n",
      "step 3608, loss: 0.611174\n",
      "step 3609, loss: 0.610938\n",
      "step 3610, loss: 0.610702\n",
      "step 3611, loss: 0.610466\n",
      "step 3612, loss: 0.610230\n",
      "step 3613, loss: 0.609994\n",
      "step 3614, loss: 0.609758\n",
      "step 3615, loss: 0.609523\n",
      "step 3616, loss: 0.609287\n",
      "step 3617, loss: 0.609051\n",
      "step 3618, loss: 0.608815\n",
      "step 3619, loss: 0.608579\n",
      "step 3620, loss: 0.608343\n",
      "step 3621, loss: 0.608107\n",
      "step 3622, loss: 0.607870\n",
      "step 3623, loss: 0.607634\n",
      "step 3624, loss: 0.607398\n",
      "step 3625, loss: 0.607162\n",
      "step 3626, loss: 0.606926\n",
      "step 3627, loss: 0.606690\n",
      "step 3628, loss: 0.606453\n",
      "step 3629, loss: 0.606217\n",
      "step 3630, loss: 0.605981\n",
      "step 3631, loss: 0.605745\n",
      "step 3632, loss: 0.605508\n",
      "step 3633, loss: 0.605272\n",
      "step 3634, loss: 0.605036\n",
      "step 3635, loss: 0.604799\n",
      "step 3636, loss: 0.604563\n",
      "step 3637, loss: 0.604327\n",
      "step 3638, loss: 0.604090\n",
      "step 3639, loss: 0.603854\n",
      "step 3640, loss: 0.603617\n",
      "step 3641, loss: 0.603381\n",
      "step 3642, loss: 0.603145\n",
      "step 3643, loss: 0.602908\n",
      "step 3644, loss: 0.602672\n",
      "step 3645, loss: 0.602435\n",
      "step 3646, loss: 0.602198\n",
      "step 3647, loss: 0.601962\n",
      "step 3648, loss: 0.601725\n",
      "step 3649, loss: 0.601489\n",
      "step 3650, loss: 0.601252\n",
      "step 3651, loss: 0.601015\n",
      "step 3652, loss: 0.600779\n",
      "step 3653, loss: 0.600542\n",
      "step 3654, loss: 0.600305\n",
      "step 3655, loss: 0.600068\n",
      "step 3656, loss: 0.599832\n",
      "step 3657, loss: 0.599595\n",
      "step 3658, loss: 0.599358\n",
      "step 3659, loss: 0.599122\n",
      "step 3660, loss: 0.598885\n",
      "step 3661, loss: 0.598648\n",
      "step 3662, loss: 0.598411\n",
      "step 3663, loss: 0.598174\n",
      "step 3664, loss: 0.597937\n",
      "step 3665, loss: 0.597700\n",
      "step 3666, loss: 0.597463\n",
      "step 3667, loss: 0.597227\n",
      "step 3668, loss: 0.596990\n",
      "step 3669, loss: 0.596753\n",
      "step 3670, loss: 0.596516\n",
      "step 3671, loss: 0.596279\n",
      "step 3672, loss: 0.596041\n",
      "step 3673, loss: 0.595805\n",
      "step 3674, loss: 0.595567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3675, loss: 0.595330\n",
      "step 3676, loss: 0.595093\n",
      "step 3677, loss: 0.594856\n",
      "step 3678, loss: 0.594619\n",
      "step 3679, loss: 0.594382\n",
      "step 3680, loss: 0.594145\n",
      "step 3681, loss: 0.593908\n",
      "step 3682, loss: 0.593670\n",
      "step 3683, loss: 0.593433\n",
      "step 3684, loss: 0.593196\n",
      "step 3685, loss: 0.592959\n",
      "step 3686, loss: 0.592722\n",
      "step 3687, loss: 0.592484\n",
      "step 3688, loss: 0.592247\n",
      "step 3689, loss: 0.592009\n",
      "step 3690, loss: 0.591772\n",
      "step 3691, loss: 0.591535\n",
      "step 3692, loss: 0.591297\n",
      "step 3693, loss: 0.591060\n",
      "step 3694, loss: 0.590823\n",
      "step 3695, loss: 0.590585\n",
      "step 3696, loss: 0.590348\n",
      "step 3697, loss: 0.590111\n",
      "step 3698, loss: 0.589873\n",
      "step 3699, loss: 0.589635\n",
      "step 3700, loss: 0.589398\n",
      "step 3701, loss: 0.589160\n",
      "step 3702, loss: 0.588923\n",
      "step 3703, loss: 0.588685\n",
      "step 3704, loss: 0.588448\n",
      "step 3705, loss: 0.588210\n",
      "step 3706, loss: 0.587973\n",
      "step 3707, loss: 0.587735\n",
      "step 3708, loss: 0.587497\n",
      "step 3709, loss: 0.587260\n",
      "step 3710, loss: 0.587022\n",
      "step 3711, loss: 0.586784\n",
      "step 3712, loss: 0.586547\n",
      "step 3713, loss: 0.586309\n",
      "step 3714, loss: 0.586071\n",
      "step 3715, loss: 0.585834\n",
      "step 3716, loss: 0.585596\n",
      "step 3717, loss: 0.585358\n",
      "step 3718, loss: 0.585120\n",
      "step 3719, loss: 0.584882\n",
      "step 3720, loss: 0.584645\n",
      "step 3721, loss: 0.584407\n",
      "step 3722, loss: 0.584169\n",
      "step 3723, loss: 0.583931\n",
      "step 3724, loss: 0.583693\n",
      "step 3725, loss: 0.583456\n",
      "step 3726, loss: 0.583218\n",
      "step 3727, loss: 0.582980\n",
      "step 3728, loss: 0.582742\n",
      "step 3729, loss: 0.582504\n",
      "step 3730, loss: 0.582266\n",
      "step 3731, loss: 0.582028\n",
      "step 3732, loss: 0.581790\n",
      "step 3733, loss: 0.581552\n",
      "step 3734, loss: 0.581314\n",
      "step 3735, loss: 0.581076\n",
      "step 3736, loss: 0.580838\n",
      "step 3737, loss: 0.580600\n",
      "step 3738, loss: 0.580362\n",
      "step 3739, loss: 0.580124\n",
      "step 3740, loss: 0.579886\n",
      "step 3741, loss: 0.579647\n",
      "step 3742, loss: 0.579409\n",
      "step 3743, loss: 0.579171\n",
      "step 3744, loss: 0.578933\n",
      "step 3745, loss: 0.578695\n",
      "step 3746, loss: 0.578457\n",
      "step 3747, loss: 0.578219\n",
      "step 3748, loss: 0.577980\n",
      "step 3749, loss: 0.577742\n",
      "step 3750, loss: 0.577504\n",
      "step 3751, loss: 0.577266\n",
      "step 3752, loss: 0.577027\n",
      "step 3753, loss: 0.576789\n",
      "step 3754, loss: 0.576551\n",
      "step 3755, loss: 0.576312\n",
      "step 3756, loss: 0.576074\n",
      "step 3757, loss: 0.575836\n",
      "step 3758, loss: 0.575598\n",
      "step 3759, loss: 0.575359\n",
      "step 3760, loss: 0.575121\n",
      "step 3761, loss: 0.574883\n",
      "step 3762, loss: 0.574644\n",
      "step 3763, loss: 0.574406\n",
      "step 3764, loss: 0.574167\n",
      "step 3765, loss: 0.573929\n",
      "step 3766, loss: 0.573691\n",
      "step 3767, loss: 0.573452\n",
      "step 3768, loss: 0.573214\n",
      "step 3769, loss: 0.572975\n",
      "step 3770, loss: 0.572737\n",
      "step 3771, loss: 0.572498\n",
      "step 3772, loss: 0.572260\n",
      "step 3773, loss: 0.572021\n",
      "step 3774, loss: 0.571783\n",
      "step 3775, loss: 0.571544\n",
      "step 3776, loss: 0.571305\n",
      "step 3777, loss: 0.571067\n",
      "step 3778, loss: 0.570829\n",
      "step 3779, loss: 0.570590\n",
      "step 3780, loss: 0.570351\n",
      "step 3781, loss: 0.570113\n",
      "step 3782, loss: 0.569874\n",
      "step 3783, loss: 0.569636\n",
      "step 3784, loss: 0.569397\n",
      "step 3785, loss: 0.569158\n",
      "step 3786, loss: 0.568919\n",
      "step 3787, loss: 0.568681\n",
      "step 3788, loss: 0.568442\n",
      "step 3789, loss: 0.568203\n",
      "step 3790, loss: 0.567964\n",
      "step 3791, loss: 0.567726\n",
      "step 3792, loss: 0.567487\n",
      "step 3793, loss: 0.567248\n",
      "step 3794, loss: 0.567010\n",
      "step 3795, loss: 0.566771\n",
      "step 3796, loss: 0.566532\n",
      "step 3797, loss: 0.566293\n",
      "step 3798, loss: 0.566054\n",
      "step 3799, loss: 0.565815\n",
      "step 3800, loss: 0.565577\n",
      "step 3801, loss: 0.565338\n",
      "step 3802, loss: 0.565099\n",
      "step 3803, loss: 0.564860\n",
      "step 3804, loss: 0.564621\n",
      "step 3805, loss: 0.564382\n",
      "step 3806, loss: 0.564143\n",
      "step 3807, loss: 0.563905\n",
      "step 3808, loss: 0.563666\n",
      "step 3809, loss: 0.563427\n",
      "step 3810, loss: 0.563188\n",
      "step 3811, loss: 0.562949\n",
      "step 3812, loss: 0.562710\n",
      "step 3813, loss: 0.562471\n",
      "step 3814, loss: 0.562232\n",
      "step 3815, loss: 0.561993\n",
      "step 3816, loss: 0.561754\n",
      "step 3817, loss: 0.561515\n",
      "step 3818, loss: 0.561276\n",
      "step 3819, loss: 0.561037\n",
      "step 3820, loss: 0.560798\n",
      "step 3821, loss: 0.560559\n",
      "step 3822, loss: 0.560320\n",
      "step 3823, loss: 0.560081\n",
      "step 3824, loss: 0.559842\n",
      "step 3825, loss: 0.559602\n",
      "step 3826, loss: 0.559363\n",
      "step 3827, loss: 0.559124\n",
      "step 3828, loss: 0.558885\n",
      "step 3829, loss: 0.558646\n",
      "step 3830, loss: 0.558407\n",
      "step 3831, loss: 0.558168\n",
      "step 3832, loss: 0.557929\n",
      "step 3833, loss: 0.557690\n",
      "step 3834, loss: 0.557451\n",
      "step 3835, loss: 0.557211\n",
      "step 3836, loss: 0.556972\n",
      "step 3837, loss: 0.556733\n",
      "step 3838, loss: 0.556494\n",
      "step 3839, loss: 0.556255\n",
      "step 3840, loss: 0.556015\n",
      "step 3841, loss: 0.555776\n",
      "step 3842, loss: 0.555537\n",
      "step 3843, loss: 0.555298\n",
      "step 3844, loss: 0.555059\n",
      "step 3845, loss: 0.554819\n",
      "step 3846, loss: 0.554580\n",
      "step 3847, loss: 0.554341\n",
      "step 3848, loss: 0.554101\n",
      "step 3849, loss: 0.553862\n",
      "step 3850, loss: 0.553623\n",
      "step 3851, loss: 0.553384\n",
      "step 3852, loss: 0.553144\n",
      "step 3853, loss: 0.552905\n",
      "step 3854, loss: 0.552666\n",
      "step 3855, loss: 0.552426\n",
      "step 3856, loss: 0.552187\n",
      "step 3857, loss: 0.551948\n",
      "step 3858, loss: 0.551708\n",
      "step 3859, loss: 0.551469\n",
      "step 3860, loss: 0.551230\n",
      "step 3861, loss: 0.550990\n",
      "step 3862, loss: 0.550751\n",
      "step 3863, loss: 0.550512\n",
      "step 3864, loss: 0.550272\n",
      "step 3865, loss: 0.550033\n",
      "step 3866, loss: 0.549793\n",
      "step 3867, loss: 0.549554\n",
      "step 3868, loss: 0.549315\n",
      "step 3869, loss: 0.549075\n",
      "step 3870, loss: 0.548836\n",
      "step 3871, loss: 0.548596\n",
      "step 3872, loss: 0.548357\n",
      "step 3873, loss: 0.548117\n",
      "step 3874, loss: 0.547878\n",
      "step 3875, loss: 0.547638\n",
      "step 3876, loss: 0.547399\n",
      "step 3877, loss: 0.547160\n",
      "step 3878, loss: 0.546920\n",
      "step 3879, loss: 0.546681\n",
      "step 3880, loss: 0.546441\n",
      "step 3881, loss: 0.546202\n",
      "step 3882, loss: 0.545962\n",
      "step 3883, loss: 0.545722\n",
      "step 3884, loss: 0.545483\n",
      "step 3885, loss: 0.545244\n",
      "step 3886, loss: 0.545004\n",
      "step 3887, loss: 0.544764\n",
      "step 3888, loss: 0.544525\n",
      "step 3889, loss: 0.544285\n",
      "step 3890, loss: 0.544046\n",
      "step 3891, loss: 0.543806\n",
      "step 3892, loss: 0.543567\n",
      "step 3893, loss: 0.543327\n",
      "step 3894, loss: 0.543088\n",
      "step 3895, loss: 0.542848\n",
      "step 3896, loss: 0.542609\n",
      "step 3897, loss: 0.542369\n",
      "step 3898, loss: 0.542129\n",
      "step 3899, loss: 0.541890\n",
      "step 3900, loss: 0.541650\n",
      "step 3901, loss: 0.541411\n",
      "step 3902, loss: 0.541171\n",
      "step 3903, loss: 0.540931\n",
      "step 3904, loss: 0.540692\n",
      "step 3905, loss: 0.540452\n",
      "step 3906, loss: 0.540213\n",
      "step 3907, loss: 0.539973\n",
      "step 3908, loss: 0.539733\n",
      "step 3909, loss: 0.539494\n",
      "step 3910, loss: 0.539254\n",
      "step 3911, loss: 0.539014\n",
      "step 3912, loss: 0.538775\n",
      "step 3913, loss: 0.538535\n",
      "step 3914, loss: 0.538296\n",
      "step 3915, loss: 0.538056\n",
      "step 3916, loss: 0.537816\n",
      "step 3917, loss: 0.537576\n",
      "step 3918, loss: 0.537337\n",
      "step 3919, loss: 0.537097\n",
      "step 3920, loss: 0.536857\n",
      "step 3921, loss: 0.536618\n",
      "step 3922, loss: 0.536378\n",
      "step 3923, loss: 0.536138\n",
      "step 3924, loss: 0.535899\n",
      "step 3925, loss: 0.535659\n",
      "step 3926, loss: 0.535419\n",
      "step 3927, loss: 0.535179\n",
      "step 3928, loss: 0.534940\n",
      "step 3929, loss: 0.534700\n",
      "step 3930, loss: 0.534460\n",
      "step 3931, loss: 0.534221\n",
      "step 3932, loss: 0.533981\n",
      "step 3933, loss: 0.533741\n",
      "step 3934, loss: 0.533501\n",
      "step 3935, loss: 0.533262\n",
      "step 3936, loss: 0.533022\n",
      "step 3937, loss: 0.532782\n",
      "step 3938, loss: 0.532543\n",
      "step 3939, loss: 0.532303\n",
      "step 3940, loss: 0.532063\n",
      "step 3941, loss: 0.531823\n",
      "step 3942, loss: 0.531584\n",
      "step 3943, loss: 0.531344\n",
      "step 3944, loss: 0.531104\n",
      "step 3945, loss: 0.530864\n",
      "step 3946, loss: 0.530625\n",
      "step 3947, loss: 0.530385\n",
      "step 3948, loss: 0.530145\n",
      "step 3949, loss: 0.529905\n",
      "step 3950, loss: 0.529665\n",
      "step 3951, loss: 0.529425\n",
      "step 3952, loss: 0.529186\n",
      "step 3953, loss: 0.528946\n",
      "step 3954, loss: 0.528706\n",
      "step 3955, loss: 0.528466\n",
      "step 3956, loss: 0.528227\n",
      "step 3957, loss: 0.527987\n",
      "step 3958, loss: 0.527747\n",
      "step 3959, loss: 0.527507\n",
      "step 3960, loss: 0.527267\n",
      "step 3961, loss: 0.527028\n",
      "step 3962, loss: 0.526788\n",
      "step 3963, loss: 0.526548\n",
      "step 3964, loss: 0.526308\n",
      "step 3965, loss: 0.526069\n",
      "step 3966, loss: 0.525829\n",
      "step 3967, loss: 0.525589\n",
      "step 3968, loss: 0.525349\n",
      "step 3969, loss: 0.525110\n",
      "step 3970, loss: 0.524869\n",
      "step 3971, loss: 0.524630\n",
      "step 3972, loss: 0.524390\n",
      "step 3973, loss: 0.524150\n",
      "step 3974, loss: 0.523911\n",
      "step 3975, loss: 0.523671\n",
      "step 3976, loss: 0.523431\n",
      "step 3977, loss: 0.523191\n",
      "step 3978, loss: 0.522951\n",
      "step 3979, loss: 0.522711\n",
      "step 3980, loss: 0.522472\n",
      "step 3981, loss: 0.522232\n",
      "step 3982, loss: 0.521992\n",
      "step 3983, loss: 0.521752\n",
      "step 3984, loss: 0.521512\n",
      "step 3985, loss: 0.521273\n",
      "step 3986, loss: 0.521032\n",
      "step 3987, loss: 0.520793\n",
      "step 3988, loss: 0.520553\n",
      "step 3989, loss: 0.520313\n",
      "step 3990, loss: 0.520074\n",
      "step 3991, loss: 0.519834\n",
      "step 3992, loss: 0.519594\n",
      "step 3993, loss: 0.519354\n",
      "step 3994, loss: 0.519114\n",
      "step 3995, loss: 0.518875\n",
      "step 3996, loss: 0.518635\n",
      "step 3997, loss: 0.518395\n",
      "step 3998, loss: 0.518155\n",
      "step 3999, loss: 0.517915\n",
      "step 4000, loss: 0.517676\n",
      "step 4001, loss: 0.517436\n",
      "step 4002, loss: 0.517196\n",
      "step 4003, loss: 0.516956\n",
      "step 4004, loss: 0.516716\n",
      "step 4005, loss: 0.516477\n",
      "step 4006, loss: 0.516237\n",
      "step 4007, loss: 0.515997\n",
      "step 4008, loss: 0.515758\n",
      "step 4009, loss: 0.515518\n",
      "step 4010, loss: 0.515278\n",
      "step 4011, loss: 0.515038\n",
      "step 4012, loss: 0.514798\n",
      "step 4013, loss: 0.514558\n",
      "step 4014, loss: 0.514318\n",
      "step 4015, loss: 0.514079\n",
      "step 4016, loss: 0.513839\n",
      "step 4017, loss: 0.513599\n",
      "step 4018, loss: 0.513359\n",
      "step 4019, loss: 0.513120\n",
      "step 4020, loss: 0.512880\n",
      "step 4021, loss: 0.512640\n",
      "step 4022, loss: 0.512401\n",
      "step 4023, loss: 0.512161\n",
      "step 4024, loss: 0.511921\n",
      "step 4025, loss: 0.511681\n",
      "step 4026, loss: 0.511442\n",
      "step 4027, loss: 0.511202\n",
      "step 4028, loss: 0.510962\n",
      "step 4029, loss: 0.510722\n",
      "step 4030, loss: 0.510483\n",
      "step 4031, loss: 0.510243\n",
      "step 4032, loss: 0.510003\n",
      "step 4033, loss: 0.509764\n",
      "step 4034, loss: 0.509524\n",
      "step 4035, loss: 0.509284\n",
      "step 4036, loss: 0.509044\n",
      "step 4037, loss: 0.508804\n",
      "step 4038, loss: 0.508565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4039, loss: 0.508325\n",
      "step 4040, loss: 0.508085\n",
      "step 4041, loss: 0.507846\n",
      "step 4042, loss: 0.507606\n",
      "step 4043, loss: 0.507366\n",
      "step 4044, loss: 0.507127\n",
      "step 4045, loss: 0.506887\n",
      "step 4046, loss: 0.506648\n",
      "step 4047, loss: 0.506408\n",
      "step 4048, loss: 0.506168\n",
      "step 4049, loss: 0.505928\n",
      "step 4050, loss: 0.505689\n",
      "step 4051, loss: 0.505449\n",
      "step 4052, loss: 0.505209\n",
      "step 4053, loss: 0.504970\n",
      "step 4054, loss: 0.504730\n",
      "step 4055, loss: 0.504491\n",
      "step 4056, loss: 0.504251\n",
      "step 4057, loss: 0.504011\n",
      "step 4058, loss: 0.503772\n",
      "step 4059, loss: 0.503532\n",
      "step 4060, loss: 0.503292\n",
      "step 4061, loss: 0.503053\n",
      "step 4062, loss: 0.502813\n",
      "step 4063, loss: 0.502573\n",
      "step 4064, loss: 0.502334\n",
      "step 4065, loss: 0.502094\n",
      "step 4066, loss: 0.501855\n",
      "step 4067, loss: 0.501615\n",
      "step 4068, loss: 0.501376\n",
      "step 4069, loss: 0.501136\n",
      "step 4070, loss: 0.500896\n",
      "step 4071, loss: 0.500657\n",
      "step 4072, loss: 0.500417\n",
      "step 4073, loss: 0.500178\n",
      "step 4074, loss: 0.499938\n",
      "step 4075, loss: 0.499699\n",
      "step 4076, loss: 0.499459\n",
      "step 4077, loss: 0.499220\n",
      "step 4078, loss: 0.498980\n",
      "step 4079, loss: 0.498741\n",
      "step 4080, loss: 0.498501\n",
      "step 4081, loss: 0.498262\n",
      "step 4082, loss: 0.498022\n",
      "step 4083, loss: 0.497783\n",
      "step 4084, loss: 0.497543\n",
      "step 4085, loss: 0.497304\n",
      "step 4086, loss: 0.497064\n",
      "step 4087, loss: 0.496825\n",
      "step 4088, loss: 0.496585\n",
      "step 4089, loss: 0.496346\n",
      "step 4090, loss: 0.496107\n",
      "step 4091, loss: 0.495867\n",
      "step 4092, loss: 0.495628\n",
      "step 4093, loss: 0.495388\n",
      "step 4094, loss: 0.495149\n",
      "step 4095, loss: 0.494909\n",
      "step 4096, loss: 0.494670\n",
      "step 4097, loss: 0.494431\n",
      "step 4098, loss: 0.494191\n",
      "step 4099, loss: 0.493952\n",
      "step 4100, loss: 0.493712\n",
      "step 4101, loss: 0.493473\n",
      "step 4102, loss: 0.493234\n",
      "step 4103, loss: 0.492994\n",
      "step 4104, loss: 0.492755\n",
      "step 4105, loss: 0.492516\n",
      "step 4106, loss: 0.492277\n",
      "step 4107, loss: 0.492037\n",
      "step 4108, loss: 0.491798\n",
      "step 4109, loss: 0.491558\n",
      "step 4110, loss: 0.491319\n",
      "step 4111, loss: 0.491080\n",
      "step 4112, loss: 0.490841\n",
      "step 4113, loss: 0.490601\n",
      "step 4114, loss: 0.490362\n",
      "step 4115, loss: 0.490123\n",
      "step 4116, loss: 0.489884\n",
      "step 4117, loss: 0.489644\n",
      "step 4118, loss: 0.489405\n",
      "step 4119, loss: 0.489166\n",
      "step 4120, loss: 0.488927\n",
      "step 4121, loss: 0.488687\n",
      "step 4122, loss: 0.488448\n",
      "step 4123, loss: 0.488209\n",
      "step 4124, loss: 0.487970\n",
      "step 4125, loss: 0.487731\n",
      "step 4126, loss: 0.487492\n",
      "step 4127, loss: 0.487252\n",
      "step 4128, loss: 0.487013\n",
      "step 4129, loss: 0.486774\n",
      "step 4130, loss: 0.486535\n",
      "step 4131, loss: 0.486296\n",
      "step 4132, loss: 0.486057\n",
      "step 4133, loss: 0.485818\n",
      "step 4134, loss: 0.485578\n",
      "step 4135, loss: 0.485340\n",
      "step 4136, loss: 0.485100\n",
      "step 4137, loss: 0.484861\n",
      "step 4138, loss: 0.484622\n",
      "step 4139, loss: 0.484383\n",
      "step 4140, loss: 0.484144\n",
      "step 4141, loss: 0.483905\n",
      "step 4142, loss: 0.483666\n",
      "step 4143, loss: 0.483427\n",
      "step 4144, loss: 0.483188\n",
      "step 4145, loss: 0.482949\n",
      "step 4146, loss: 0.482710\n",
      "step 4147, loss: 0.482471\n",
      "step 4148, loss: 0.482232\n",
      "step 4149, loss: 0.481993\n",
      "step 4150, loss: 0.481755\n",
      "step 4151, loss: 0.481516\n",
      "step 4152, loss: 0.481277\n",
      "step 4153, loss: 0.481038\n",
      "step 4154, loss: 0.480799\n",
      "step 4155, loss: 0.480560\n",
      "step 4156, loss: 0.480321\n",
      "step 4157, loss: 0.480083\n",
      "step 4158, loss: 0.479843\n",
      "step 4159, loss: 0.479605\n",
      "step 4160, loss: 0.479366\n",
      "step 4161, loss: 0.479127\n",
      "step 4162, loss: 0.478888\n",
      "step 4163, loss: 0.478650\n",
      "step 4164, loss: 0.478411\n",
      "step 4165, loss: 0.478172\n",
      "step 4166, loss: 0.477933\n",
      "step 4167, loss: 0.477695\n",
      "step 4168, loss: 0.477456\n",
      "step 4169, loss: 0.477217\n",
      "step 4170, loss: 0.476978\n",
      "step 4171, loss: 0.476740\n",
      "step 4172, loss: 0.476501\n",
      "step 4173, loss: 0.476263\n",
      "step 4174, loss: 0.476024\n",
      "step 4175, loss: 0.475785\n",
      "step 4176, loss: 0.475546\n",
      "step 4177, loss: 0.475308\n",
      "step 4178, loss: 0.475069\n",
      "step 4179, loss: 0.474831\n",
      "step 4180, loss: 0.474592\n",
      "step 4181, loss: 0.474353\n",
      "step 4182, loss: 0.474115\n",
      "step 4183, loss: 0.473877\n",
      "step 4184, loss: 0.473638\n",
      "step 4185, loss: 0.473399\n",
      "step 4186, loss: 0.473161\n",
      "step 4187, loss: 0.472923\n",
      "step 4188, loss: 0.472684\n",
      "step 4189, loss: 0.472446\n",
      "step 4190, loss: 0.472207\n",
      "step 4191, loss: 0.471969\n",
      "step 4192, loss: 0.471730\n",
      "step 4193, loss: 0.471492\n",
      "step 4194, loss: 0.471254\n",
      "step 4195, loss: 0.471015\n",
      "step 4196, loss: 0.470777\n",
      "step 4197, loss: 0.470538\n",
      "step 4198, loss: 0.470300\n",
      "step 4199, loss: 0.470062\n",
      "step 4200, loss: 0.469823\n",
      "step 4201, loss: 0.469585\n",
      "step 4202, loss: 0.469347\n",
      "step 4203, loss: 0.469108\n",
      "step 4204, loss: 0.468870\n",
      "step 4205, loss: 0.468632\n",
      "step 4206, loss: 0.468394\n",
      "step 4207, loss: 0.468156\n",
      "step 4208, loss: 0.467917\n",
      "step 4209, loss: 0.467679\n",
      "step 4210, loss: 0.467441\n",
      "step 4211, loss: 0.467203\n",
      "step 4212, loss: 0.466965\n",
      "step 4213, loss: 0.466726\n",
      "step 4214, loss: 0.466488\n",
      "step 4215, loss: 0.466250\n",
      "step 4216, loss: 0.466012\n",
      "step 4217, loss: 0.465774\n",
      "step 4218, loss: 0.465536\n",
      "step 4219, loss: 0.465298\n",
      "step 4220, loss: 0.465060\n",
      "step 4221, loss: 0.464822\n",
      "step 4222, loss: 0.464584\n",
      "step 4223, loss: 0.464346\n",
      "step 4224, loss: 0.464108\n",
      "step 4225, loss: 0.463870\n",
      "step 4226, loss: 0.463632\n",
      "step 4227, loss: 0.463394\n",
      "step 4228, loss: 0.463156\n",
      "step 4229, loss: 0.462918\n",
      "step 4230, loss: 0.462680\n",
      "step 4231, loss: 0.462443\n",
      "step 4232, loss: 0.462205\n",
      "step 4233, loss: 0.461967\n",
      "step 4234, loss: 0.461729\n",
      "step 4235, loss: 0.461491\n",
      "step 4236, loss: 0.461253\n",
      "step 4237, loss: 0.461016\n",
      "step 4238, loss: 0.460778\n",
      "step 4239, loss: 0.460540\n",
      "step 4240, loss: 0.460302\n",
      "step 4241, loss: 0.460065\n",
      "step 4242, loss: 0.459827\n",
      "step 4243, loss: 0.459590\n",
      "step 4244, loss: 0.459352\n",
      "step 4245, loss: 0.459114\n",
      "step 4246, loss: 0.458877\n",
      "step 4247, loss: 0.458639\n",
      "step 4248, loss: 0.458401\n",
      "step 4249, loss: 0.458164\n",
      "step 4250, loss: 0.457926\n",
      "step 4251, loss: 0.457689\n",
      "step 4252, loss: 0.457451\n",
      "step 4253, loss: 0.457214\n",
      "step 4254, loss: 0.456976\n",
      "step 4255, loss: 0.456739\n",
      "step 4256, loss: 0.456501\n",
      "step 4257, loss: 0.456264\n",
      "step 4258, loss: 0.456026\n",
      "step 4259, loss: 0.455789\n",
      "step 4260, loss: 0.455552\n",
      "step 4261, loss: 0.455314\n",
      "step 4262, loss: 0.455077\n",
      "step 4263, loss: 0.454840\n",
      "step 4264, loss: 0.454602\n",
      "step 4265, loss: 0.454365\n",
      "step 4266, loss: 0.454128\n",
      "step 4267, loss: 0.453891\n",
      "step 4268, loss: 0.453653\n",
      "step 4269, loss: 0.453416\n",
      "step 4270, loss: 0.453179\n",
      "step 4271, loss: 0.452942\n",
      "step 4272, loss: 0.452705\n",
      "step 4273, loss: 0.452468\n",
      "step 4274, loss: 0.452231\n",
      "step 4275, loss: 0.451993\n",
      "step 4276, loss: 0.451756\n",
      "step 4277, loss: 0.451519\n",
      "step 4278, loss: 0.451282\n",
      "step 4279, loss: 0.451045\n",
      "step 4280, loss: 0.450808\n",
      "step 4281, loss: 0.450571\n",
      "step 4282, loss: 0.450334\n",
      "step 4283, loss: 0.450097\n",
      "step 4284, loss: 0.449860\n",
      "step 4285, loss: 0.449623\n",
      "step 4286, loss: 0.449386\n",
      "step 4287, loss: 0.449149\n",
      "step 4288, loss: 0.448913\n",
      "step 4289, loss: 0.448676\n",
      "step 4290, loss: 0.448439\n",
      "step 4291, loss: 0.448202\n",
      "step 4292, loss: 0.447965\n",
      "step 4293, loss: 0.447729\n",
      "step 4294, loss: 0.447492\n",
      "step 4295, loss: 0.447255\n",
      "step 4296, loss: 0.447019\n",
      "step 4297, loss: 0.446782\n",
      "step 4298, loss: 0.446545\n",
      "step 4299, loss: 0.446308\n",
      "step 4300, loss: 0.446072\n",
      "step 4301, loss: 0.445835\n",
      "step 4302, loss: 0.445599\n",
      "step 4303, loss: 0.445362\n",
      "step 4304, loss: 0.445126\n",
      "step 4305, loss: 0.444889\n",
      "step 4306, loss: 0.444653\n",
      "step 4307, loss: 0.444416\n",
      "step 4308, loss: 0.444180\n",
      "step 4309, loss: 0.443943\n",
      "step 4310, loss: 0.443707\n",
      "step 4311, loss: 0.443471\n",
      "step 4312, loss: 0.443234\n",
      "step 4313, loss: 0.442998\n",
      "step 4314, loss: 0.442761\n",
      "step 4315, loss: 0.442525\n",
      "step 4316, loss: 0.442289\n",
      "step 4317, loss: 0.442053\n",
      "step 4318, loss: 0.441816\n",
      "step 4319, loss: 0.441580\n",
      "step 4320, loss: 0.441344\n",
      "step 4321, loss: 0.441108\n",
      "step 4322, loss: 0.440872\n",
      "step 4323, loss: 0.440636\n",
      "step 4324, loss: 0.440399\n",
      "step 4325, loss: 0.440163\n",
      "step 4326, loss: 0.439927\n",
      "step 4327, loss: 0.439691\n",
      "step 4328, loss: 0.439455\n",
      "step 4329, loss: 0.439219\n",
      "step 4330, loss: 0.438983\n",
      "step 4331, loss: 0.438747\n",
      "step 4332, loss: 0.438511\n",
      "step 4333, loss: 0.438275\n",
      "step 4334, loss: 0.438039\n",
      "step 4335, loss: 0.437804\n",
      "step 4336, loss: 0.437568\n",
      "step 4337, loss: 0.437332\n",
      "step 4338, loss: 0.437096\n",
      "step 4339, loss: 0.436860\n",
      "step 4340, loss: 0.436624\n",
      "step 4341, loss: 0.436389\n",
      "step 4342, loss: 0.436153\n",
      "step 4343, loss: 0.435917\n",
      "step 4344, loss: 0.435682\n",
      "step 4345, loss: 0.435446\n",
      "step 4346, loss: 0.435211\n",
      "step 4347, loss: 0.434975\n",
      "step 4348, loss: 0.434739\n",
      "step 4349, loss: 0.434504\n",
      "step 4350, loss: 0.434268\n",
      "step 4351, loss: 0.434033\n",
      "step 4352, loss: 0.433797\n",
      "step 4353, loss: 0.433562\n",
      "step 4354, loss: 0.433326\n",
      "step 4355, loss: 0.433091\n",
      "step 4356, loss: 0.432856\n",
      "step 4357, loss: 0.432620\n",
      "step 4358, loss: 0.432385\n",
      "step 4359, loss: 0.432150\n",
      "step 4360, loss: 0.431914\n",
      "step 4361, loss: 0.431679\n",
      "step 4362, loss: 0.431444\n",
      "step 4363, loss: 0.431209\n",
      "step 4364, loss: 0.430973\n",
      "step 4365, loss: 0.430738\n",
      "step 4366, loss: 0.430503\n",
      "step 4367, loss: 0.430268\n",
      "step 4368, loss: 0.430033\n",
      "step 4369, loss: 0.429798\n",
      "step 4370, loss: 0.429563\n",
      "step 4371, loss: 0.429328\n",
      "step 4372, loss: 0.429093\n",
      "step 4373, loss: 0.428858\n",
      "step 4374, loss: 0.428623\n",
      "step 4375, loss: 0.428388\n",
      "step 4376, loss: 0.428153\n",
      "step 4377, loss: 0.427918\n",
      "step 4378, loss: 0.427683\n",
      "step 4379, loss: 0.427449\n",
      "step 4380, loss: 0.427214\n",
      "step 4381, loss: 0.426979\n",
      "step 4382, loss: 0.426744\n",
      "step 4383, loss: 0.426509\n",
      "step 4384, loss: 0.426275\n",
      "step 4385, loss: 0.426040\n",
      "step 4386, loss: 0.425806\n",
      "step 4387, loss: 0.425571\n",
      "step 4388, loss: 0.425336\n",
      "step 4389, loss: 0.425102\n",
      "step 4390, loss: 0.424867\n",
      "step 4391, loss: 0.424633\n",
      "step 4392, loss: 0.424398\n",
      "step 4393, loss: 0.424164\n",
      "step 4394, loss: 0.423930\n",
      "step 4395, loss: 0.423695\n",
      "step 4396, loss: 0.423461\n",
      "step 4397, loss: 0.423226\n",
      "step 4398, loss: 0.422992\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4399, loss: 0.422758\n",
      "step 4400, loss: 0.422523\n",
      "step 4401, loss: 0.422289\n",
      "step 4402, loss: 0.422055\n",
      "step 4403, loss: 0.421821\n",
      "step 4404, loss: 0.421587\n",
      "step 4405, loss: 0.421353\n",
      "step 4406, loss: 0.421118\n",
      "step 4407, loss: 0.420884\n",
      "step 4408, loss: 0.420650\n",
      "step 4409, loss: 0.420416\n",
      "step 4410, loss: 0.420182\n",
      "step 4411, loss: 0.419948\n",
      "step 4412, loss: 0.419714\n",
      "step 4413, loss: 0.419480\n",
      "step 4414, loss: 0.419247\n",
      "step 4415, loss: 0.419013\n",
      "step 4416, loss: 0.418779\n",
      "step 4417, loss: 0.418545\n",
      "step 4418, loss: 0.418311\n",
      "step 4419, loss: 0.418078\n",
      "step 4420, loss: 0.417844\n",
      "step 4421, loss: 0.417610\n",
      "step 4422, loss: 0.417377\n",
      "step 4423, loss: 0.417143\n",
      "step 4424, loss: 0.416909\n",
      "step 4425, loss: 0.416676\n",
      "step 4426, loss: 0.416442\n",
      "step 4427, loss: 0.416209\n",
      "step 4428, loss: 0.415975\n",
      "step 4429, loss: 0.415742\n",
      "step 4430, loss: 0.415508\n",
      "step 4431, loss: 0.415275\n",
      "step 4432, loss: 0.415042\n",
      "step 4433, loss: 0.414808\n",
      "step 4434, loss: 0.414575\n",
      "step 4435, loss: 0.414342\n",
      "step 4436, loss: 0.414109\n",
      "step 4437, loss: 0.413875\n",
      "step 4438, loss: 0.413642\n",
      "step 4439, loss: 0.413409\n",
      "step 4440, loss: 0.413176\n",
      "step 4441, loss: 0.412943\n",
      "step 4442, loss: 0.412710\n",
      "step 4443, loss: 0.412477\n",
      "step 4444, loss: 0.412243\n",
      "step 4445, loss: 0.412010\n",
      "step 4446, loss: 0.411778\n",
      "step 4447, loss: 0.411545\n",
      "step 4448, loss: 0.411312\n",
      "step 4449, loss: 0.411079\n",
      "step 4450, loss: 0.410846\n",
      "step 4451, loss: 0.410613\n",
      "step 4452, loss: 0.410381\n",
      "step 4453, loss: 0.410148\n",
      "step 4454, loss: 0.409915\n",
      "step 4455, loss: 0.409682\n",
      "step 4456, loss: 0.409450\n",
      "step 4457, loss: 0.409217\n",
      "step 4458, loss: 0.408984\n",
      "step 4459, loss: 0.408752\n",
      "step 4460, loss: 0.408519\n",
      "step 4461, loss: 0.408287\n",
      "step 4462, loss: 0.408054\n",
      "step 4463, loss: 0.407822\n",
      "step 4464, loss: 0.407590\n",
      "step 4465, loss: 0.407357\n",
      "step 4466, loss: 0.407125\n",
      "step 4467, loss: 0.406892\n",
      "step 4468, loss: 0.406660\n",
      "step 4469, loss: 0.406428\n",
      "step 4470, loss: 0.406196\n",
      "step 4471, loss: 0.405963\n",
      "step 4472, loss: 0.405731\n",
      "step 4473, loss: 0.405499\n",
      "step 4474, loss: 0.405267\n",
      "step 4475, loss: 0.405035\n",
      "step 4476, loss: 0.404803\n",
      "step 4477, loss: 0.404571\n",
      "step 4478, loss: 0.404339\n",
      "step 4479, loss: 0.404107\n",
      "step 4480, loss: 0.403875\n",
      "step 4481, loss: 0.403643\n",
      "step 4482, loss: 0.403412\n",
      "step 4483, loss: 0.403180\n",
      "step 4484, loss: 0.402948\n",
      "step 4485, loss: 0.402716\n",
      "step 4486, loss: 0.402485\n",
      "step 4487, loss: 0.402253\n",
      "step 4488, loss: 0.402021\n",
      "step 4489, loss: 0.401790\n",
      "step 4490, loss: 0.401558\n",
      "step 4491, loss: 0.401327\n",
      "step 4492, loss: 0.401095\n",
      "step 4493, loss: 0.400864\n",
      "step 4494, loss: 0.400632\n",
      "step 4495, loss: 0.400401\n",
      "step 4496, loss: 0.400169\n",
      "step 4497, loss: 0.399938\n",
      "step 4498, loss: 0.399707\n",
      "step 4499, loss: 0.399475\n",
      "step 4500, loss: 0.399244\n",
      "step 4501, loss: 0.399013\n",
      "step 4502, loss: 0.398782\n",
      "step 4503, loss: 0.398551\n",
      "step 4504, loss: 0.398319\n",
      "step 4505, loss: 0.398088\n",
      "step 4506, loss: 0.397857\n",
      "step 4507, loss: 0.397626\n",
      "step 4508, loss: 0.397395\n",
      "step 4509, loss: 0.397164\n",
      "step 4510, loss: 0.396933\n",
      "step 4511, loss: 0.396703\n",
      "step 4512, loss: 0.396472\n",
      "step 4513, loss: 0.396241\n",
      "step 4514, loss: 0.396010\n",
      "step 4515, loss: 0.395780\n",
      "step 4516, loss: 0.395549\n",
      "step 4517, loss: 0.395318\n",
      "step 4518, loss: 0.395087\n",
      "step 4519, loss: 0.394857\n",
      "step 4520, loss: 0.394626\n",
      "step 4521, loss: 0.394396\n",
      "step 4522, loss: 0.394165\n",
      "step 4523, loss: 0.393935\n",
      "step 4524, loss: 0.393705\n",
      "step 4525, loss: 0.393474\n",
      "step 4526, loss: 0.393244\n",
      "step 4527, loss: 0.393013\n",
      "step 4528, loss: 0.392783\n",
      "step 4529, loss: 0.392553\n",
      "step 4530, loss: 0.392323\n",
      "step 4531, loss: 0.392093\n",
      "step 4532, loss: 0.391863\n",
      "step 4533, loss: 0.391632\n",
      "step 4534, loss: 0.391402\n",
      "step 4535, loss: 0.391172\n",
      "step 4536, loss: 0.390942\n",
      "step 4537, loss: 0.390712\n",
      "step 4538, loss: 0.390482\n",
      "step 4539, loss: 0.390252\n",
      "step 4540, loss: 0.390023\n",
      "step 4541, loss: 0.389793\n",
      "step 4542, loss: 0.389563\n",
      "step 4543, loss: 0.389333\n",
      "step 4544, loss: 0.389104\n",
      "step 4545, loss: 0.388874\n",
      "step 4546, loss: 0.388644\n",
      "step 4547, loss: 0.388415\n",
      "step 4548, loss: 0.388185\n",
      "step 4549, loss: 0.387956\n",
      "step 4550, loss: 0.387726\n",
      "step 4551, loss: 0.387497\n",
      "step 4552, loss: 0.387267\n",
      "step 4553, loss: 0.387038\n",
      "step 4554, loss: 0.386808\n",
      "step 4555, loss: 0.386579\n",
      "step 4556, loss: 0.386350\n",
      "step 4557, loss: 0.386121\n",
      "step 4558, loss: 0.385892\n",
      "step 4559, loss: 0.385662\n",
      "step 4560, loss: 0.385433\n",
      "step 4561, loss: 0.385204\n",
      "step 4562, loss: 0.384975\n",
      "step 4563, loss: 0.384746\n",
      "step 4564, loss: 0.384517\n",
      "step 4565, loss: 0.384288\n",
      "step 4566, loss: 0.384059\n",
      "step 4567, loss: 0.383831\n",
      "step 4568, loss: 0.383602\n",
      "step 4569, loss: 0.383373\n",
      "step 4570, loss: 0.383144\n",
      "step 4571, loss: 0.382915\n",
      "step 4572, loss: 0.382687\n",
      "step 4573, loss: 0.382458\n",
      "step 4574, loss: 0.382230\n",
      "step 4575, loss: 0.382001\n",
      "step 4576, loss: 0.381773\n",
      "step 4577, loss: 0.381544\n",
      "step 4578, loss: 0.381316\n",
      "step 4579, loss: 0.381087\n",
      "step 4580, loss: 0.380859\n",
      "step 4581, loss: 0.380631\n",
      "step 4582, loss: 0.380402\n",
      "step 4583, loss: 0.380174\n",
      "step 4584, loss: 0.379946\n",
      "step 4585, loss: 0.379718\n",
      "step 4586, loss: 0.379490\n",
      "step 4587, loss: 0.379261\n",
      "step 4588, loss: 0.379034\n",
      "step 4589, loss: 0.378806\n",
      "step 4590, loss: 0.378578\n",
      "step 4591, loss: 0.378350\n",
      "step 4592, loss: 0.378122\n",
      "step 4593, loss: 0.377894\n",
      "step 4594, loss: 0.377666\n",
      "step 4595, loss: 0.377438\n",
      "step 4596, loss: 0.377211\n",
      "step 4597, loss: 0.376983\n",
      "step 4598, loss: 0.376755\n",
      "step 4599, loss: 0.376528\n",
      "step 4600, loss: 0.376300\n",
      "step 4601, loss: 0.376073\n",
      "step 4602, loss: 0.375845\n",
      "step 4603, loss: 0.375618\n",
      "step 4604, loss: 0.375390\n",
      "step 4605, loss: 0.375163\n",
      "step 4606, loss: 0.374936\n",
      "step 4607, loss: 0.374708\n",
      "step 4608, loss: 0.374481\n",
      "step 4609, loss: 0.374254\n",
      "step 4610, loss: 0.374027\n",
      "step 4611, loss: 0.373800\n",
      "step 4612, loss: 0.373573\n",
      "step 4613, loss: 0.373346\n",
      "step 4614, loss: 0.373118\n",
      "step 4615, loss: 0.372891\n",
      "step 4616, loss: 0.372665\n",
      "step 4617, loss: 0.372438\n",
      "step 4618, loss: 0.372211\n",
      "step 4619, loss: 0.371984\n",
      "step 4620, loss: 0.371757\n",
      "step 4621, loss: 0.371531\n",
      "step 4622, loss: 0.371304\n",
      "step 4623, loss: 0.371078\n",
      "step 4624, loss: 0.370851\n",
      "step 4625, loss: 0.370624\n",
      "step 4626, loss: 0.370398\n",
      "step 4627, loss: 0.370171\n",
      "step 4628, loss: 0.369945\n",
      "step 4629, loss: 0.369719\n",
      "step 4630, loss: 0.369492\n",
      "step 4631, loss: 0.369266\n",
      "step 4632, loss: 0.369040\n",
      "step 4633, loss: 0.368813\n",
      "step 4634, loss: 0.368587\n",
      "step 4635, loss: 0.368361\n",
      "step 4636, loss: 0.368135\n",
      "step 4637, loss: 0.367909\n",
      "step 4638, loss: 0.367683\n",
      "step 4639, loss: 0.367457\n",
      "step 4640, loss: 0.367231\n",
      "step 4641, loss: 0.367005\n",
      "step 4642, loss: 0.366780\n",
      "step 4643, loss: 0.366554\n",
      "step 4644, loss: 0.366328\n",
      "step 4645, loss: 0.366102\n",
      "step 4646, loss: 0.365877\n",
      "step 4647, loss: 0.365651\n",
      "step 4648, loss: 0.365426\n",
      "step 4649, loss: 0.365200\n",
      "step 4650, loss: 0.364974\n",
      "step 4651, loss: 0.364749\n",
      "step 4652, loss: 0.364524\n",
      "step 4653, loss: 0.364298\n",
      "step 4654, loss: 0.364073\n",
      "step 4655, loss: 0.363848\n",
      "step 4656, loss: 0.363623\n",
      "step 4657, loss: 0.363398\n",
      "step 4658, loss: 0.363172\n",
      "step 4659, loss: 0.362947\n",
      "step 4660, loss: 0.362722\n",
      "step 4661, loss: 0.362497\n",
      "step 4662, loss: 0.362272\n",
      "step 4663, loss: 0.362048\n",
      "step 4664, loss: 0.361823\n",
      "step 4665, loss: 0.361598\n",
      "step 4666, loss: 0.361373\n",
      "step 4667, loss: 0.361148\n",
      "step 4668, loss: 0.360924\n",
      "step 4669, loss: 0.360699\n",
      "step 4670, loss: 0.360474\n",
      "step 4671, loss: 0.360250\n",
      "step 4672, loss: 0.360025\n",
      "step 4673, loss: 0.359801\n",
      "step 4674, loss: 0.359576\n",
      "step 4675, loss: 0.359352\n",
      "step 4676, loss: 0.359128\n",
      "step 4677, loss: 0.358903\n",
      "step 4678, loss: 0.358679\n",
      "step 4679, loss: 0.358455\n",
      "step 4680, loss: 0.358231\n",
      "step 4681, loss: 0.358007\n",
      "step 4682, loss: 0.357783\n",
      "step 4683, loss: 0.357559\n",
      "step 4684, loss: 0.357335\n",
      "step 4685, loss: 0.357111\n",
      "step 4686, loss: 0.356887\n",
      "step 4687, loss: 0.356663\n",
      "step 4688, loss: 0.356439\n",
      "step 4689, loss: 0.356216\n",
      "step 4690, loss: 0.355992\n",
      "step 4691, loss: 0.355768\n",
      "step 4692, loss: 0.355545\n",
      "step 4693, loss: 0.355321\n",
      "step 4694, loss: 0.355098\n",
      "step 4695, loss: 0.354874\n",
      "step 4696, loss: 0.354651\n",
      "step 4697, loss: 0.354427\n",
      "step 4698, loss: 0.354204\n",
      "step 4699, loss: 0.353981\n",
      "step 4700, loss: 0.353758\n",
      "step 4701, loss: 0.353534\n",
      "step 4702, loss: 0.353311\n",
      "step 4703, loss: 0.353088\n",
      "step 4704, loss: 0.352865\n",
      "step 4705, loss: 0.352642\n",
      "step 4706, loss: 0.352419\n",
      "step 4707, loss: 0.352196\n",
      "step 4708, loss: 0.351973\n",
      "step 4709, loss: 0.351751\n",
      "step 4710, loss: 0.351528\n",
      "step 4711, loss: 0.351305\n",
      "step 4712, loss: 0.351083\n",
      "step 4713, loss: 0.350860\n",
      "step 4714, loss: 0.350637\n",
      "step 4715, loss: 0.350415\n",
      "step 4716, loss: 0.350192\n",
      "step 4717, loss: 0.349970\n",
      "step 4718, loss: 0.349748\n",
      "step 4719, loss: 0.349525\n",
      "step 4720, loss: 0.349303\n",
      "step 4721, loss: 0.349081\n",
      "step 4722, loss: 0.348859\n",
      "step 4723, loss: 0.348636\n",
      "step 4724, loss: 0.348414\n",
      "step 4725, loss: 0.348192\n",
      "step 4726, loss: 0.347970\n",
      "step 4727, loss: 0.347748\n",
      "step 4728, loss: 0.347526\n",
      "step 4729, loss: 0.347304\n",
      "step 4730, loss: 0.347083\n",
      "step 4731, loss: 0.346861\n",
      "step 4732, loss: 0.346639\n",
      "step 4733, loss: 0.346418\n",
      "step 4734, loss: 0.346196\n",
      "step 4735, loss: 0.345975\n",
      "step 4736, loss: 0.345753\n",
      "step 4737, loss: 0.345532\n",
      "step 4738, loss: 0.345310\n",
      "step 4739, loss: 0.345089\n",
      "step 4740, loss: 0.344868\n",
      "step 4741, loss: 0.344646\n",
      "step 4742, loss: 0.344425\n",
      "step 4743, loss: 0.344204\n",
      "step 4744, loss: 0.343983\n",
      "step 4745, loss: 0.343762\n",
      "step 4746, loss: 0.343541\n",
      "step 4747, loss: 0.343320\n",
      "step 4748, loss: 0.343099\n",
      "step 4749, loss: 0.342878\n",
      "step 4750, loss: 0.342657\n",
      "step 4751, loss: 0.342436\n",
      "step 4752, loss: 0.342216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 4753, loss: 0.341995\n",
      "step 4754, loss: 0.341774\n",
      "step 4755, loss: 0.341554\n",
      "step 4756, loss: 0.341333\n",
      "step 4757, loss: 0.341113\n",
      "step 4758, loss: 0.340892\n",
      "step 4759, loss: 0.340672\n",
      "step 4760, loss: 0.340452\n",
      "step 4761, loss: 0.340231\n",
      "step 4762, loss: 0.340011\n",
      "step 4763, loss: 0.339791\n",
      "step 4764, loss: 0.339571\n",
      "step 4765, loss: 0.339351\n",
      "step 4766, loss: 0.339131\n",
      "step 4767, loss: 0.338911\n",
      "step 4768, loss: 0.338691\n",
      "step 4769, loss: 0.338471\n",
      "step 4770, loss: 0.338251\n",
      "step 4771, loss: 0.338032\n",
      "step 4772, loss: 0.337812\n",
      "step 4773, loss: 0.337592\n",
      "step 4774, loss: 0.337373\n",
      "step 4775, loss: 0.337153\n",
      "step 4776, loss: 0.336934\n",
      "step 4777, loss: 0.336714\n",
      "step 4778, loss: 0.336495\n",
      "step 4779, loss: 0.336276\n",
      "step 4780, loss: 0.336056\n",
      "step 4781, loss: 0.335837\n",
      "step 4782, loss: 0.335618\n",
      "step 4783, loss: 0.335399\n",
      "step 4784, loss: 0.335180\n",
      "step 4785, loss: 0.334961\n",
      "step 4786, loss: 0.334742\n",
      "step 4787, loss: 0.334523\n",
      "step 4788, loss: 0.334304\n",
      "step 4789, loss: 0.334085\n",
      "step 4790, loss: 0.333866\n",
      "step 4791, loss: 0.333648\n",
      "step 4792, loss: 0.333429\n",
      "step 4793, loss: 0.333210\n",
      "step 4794, loss: 0.332992\n",
      "step 4795, loss: 0.332773\n",
      "step 4796, loss: 0.332555\n",
      "step 4797, loss: 0.332336\n",
      "step 4798, loss: 0.332118\n",
      "step 4799, loss: 0.331900\n",
      "step 4800, loss: 0.331681\n",
      "step 4801, loss: 0.331463\n",
      "step 4802, loss: 0.331245\n",
      "step 4803, loss: 0.331027\n",
      "step 4804, loss: 0.330809\n",
      "step 4805, loss: 0.330591\n",
      "step 4806, loss: 0.330373\n",
      "step 4807, loss: 0.330155\n",
      "step 4808, loss: 0.329937\n",
      "step 4809, loss: 0.329720\n",
      "step 4810, loss: 0.329502\n",
      "step 4811, loss: 0.329284\n",
      "step 4812, loss: 0.329067\n",
      "step 4813, loss: 0.328849\n",
      "step 4814, loss: 0.328632\n",
      "step 4815, loss: 0.328414\n",
      "step 4816, loss: 0.328197\n",
      "step 4817, loss: 0.327980\n",
      "step 4818, loss: 0.327762\n",
      "step 4819, loss: 0.327545\n",
      "step 4820, loss: 0.327328\n",
      "step 4821, loss: 0.327111\n",
      "step 4822, loss: 0.326894\n",
      "step 4823, loss: 0.326677\n",
      "step 4824, loss: 0.326460\n",
      "step 4825, loss: 0.326243\n",
      "step 4826, loss: 0.326026\n",
      "step 4827, loss: 0.325809\n",
      "step 4828, loss: 0.325592\n",
      "step 4829, loss: 0.325376\n",
      "step 4830, loss: 0.325159\n",
      "step 4831, loss: 0.324943\n",
      "step 4832, loss: 0.324726\n",
      "step 4833, loss: 0.324510\n",
      "step 4834, loss: 0.324293\n",
      "step 4835, loss: 0.324077\n",
      "step 4836, loss: 0.323861\n",
      "step 4837, loss: 0.323644\n",
      "step 4838, loss: 0.323428\n",
      "step 4839, loss: 0.323212\n",
      "step 4840, loss: 0.322996\n",
      "step 4841, loss: 0.322780\n",
      "step 4842, loss: 0.322564\n",
      "step 4843, loss: 0.322348\n",
      "step 4844, loss: 0.322132\n",
      "step 4845, loss: 0.321916\n",
      "step 4846, loss: 0.321701\n",
      "step 4847, loss: 0.321485\n",
      "step 4848, loss: 0.321269\n",
      "step 4849, loss: 0.321054\n",
      "step 4850, loss: 0.320838\n",
      "step 4851, loss: 0.320623\n",
      "step 4852, loss: 0.320407\n",
      "step 4853, loss: 0.320192\n",
      "step 4854, loss: 0.319977\n",
      "step 4855, loss: 0.319762\n",
      "step 4856, loss: 0.319546\n",
      "step 4857, loss: 0.319331\n",
      "step 4858, loss: 0.319116\n",
      "step 4859, loss: 0.318901\n",
      "step 4860, loss: 0.318686\n",
      "step 4861, loss: 0.318471\n",
      "step 4862, loss: 0.318256\n",
      "step 4863, loss: 0.318042\n",
      "step 4864, loss: 0.317827\n",
      "step 4865, loss: 0.317612\n",
      "step 4866, loss: 0.317398\n",
      "step 4867, loss: 0.317183\n",
      "step 4868, loss: 0.316969\n",
      "step 4869, loss: 0.316754\n",
      "step 4870, loss: 0.316540\n",
      "step 4871, loss: 0.316325\n",
      "step 4872, loss: 0.316111\n",
      "step 4873, loss: 0.315897\n",
      "step 4874, loss: 0.315683\n",
      "step 4875, loss: 0.315469\n",
      "step 4876, loss: 0.315255\n",
      "step 4877, loss: 0.315041\n",
      "step 4878, loss: 0.314827\n",
      "step 4879, loss: 0.314613\n",
      "step 4880, loss: 0.314399\n",
      "step 4881, loss: 0.314185\n",
      "step 4882, loss: 0.313972\n",
      "step 4883, loss: 0.313758\n",
      "step 4884, loss: 0.313545\n",
      "step 4885, loss: 0.313331\n",
      "step 4886, loss: 0.313118\n",
      "step 4887, loss: 0.312904\n",
      "step 4888, loss: 0.312691\n",
      "step 4889, loss: 0.312478\n",
      "step 4890, loss: 0.312264\n",
      "step 4891, loss: 0.312051\n",
      "step 4892, loss: 0.311838\n",
      "step 4893, loss: 0.311625\n",
      "step 4894, loss: 0.311412\n",
      "step 4895, loss: 0.311199\n",
      "step 4896, loss: 0.310986\n",
      "step 4897, loss: 0.310774\n",
      "step 4898, loss: 0.310561\n",
      "step 4899, loss: 0.310348\n",
      "step 4900, loss: 0.310136\n",
      "step 4901, loss: 0.309923\n",
      "step 4902, loss: 0.309710\n",
      "step 4903, loss: 0.309498\n",
      "step 4904, loss: 0.309286\n",
      "step 4905, loss: 0.309073\n",
      "step 4906, loss: 0.308861\n",
      "step 4907, loss: 0.308649\n",
      "step 4908, loss: 0.308437\n",
      "step 4909, loss: 0.308225\n",
      "step 4910, loss: 0.308013\n",
      "step 4911, loss: 0.307801\n",
      "step 4912, loss: 0.307588\n",
      "step 4913, loss: 0.307377\n",
      "step 4914, loss: 0.307165\n",
      "step 4915, loss: 0.306953\n",
      "step 4916, loss: 0.306742\n",
      "step 4917, loss: 0.306530\n",
      "step 4918, loss: 0.306318\n",
      "step 4919, loss: 0.306107\n",
      "step 4920, loss: 0.305895\n",
      "step 4921, loss: 0.305684\n",
      "step 4922, loss: 0.305473\n",
      "step 4923, loss: 0.305261\n",
      "step 4924, loss: 0.305050\n",
      "step 4925, loss: 0.304839\n",
      "step 4926, loss: 0.304628\n",
      "step 4927, loss: 0.304417\n",
      "step 4928, loss: 0.304206\n",
      "step 4929, loss: 0.303995\n",
      "step 4930, loss: 0.303784\n",
      "step 4931, loss: 0.303573\n",
      "step 4932, loss: 0.303363\n",
      "step 4933, loss: 0.303152\n",
      "step 4934, loss: 0.302942\n",
      "step 4935, loss: 0.302731\n",
      "step 4936, loss: 0.302521\n",
      "step 4937, loss: 0.302310\n",
      "step 4938, loss: 0.302100\n",
      "step 4939, loss: 0.301889\n",
      "step 4940, loss: 0.301679\n",
      "step 4941, loss: 0.301469\n",
      "step 4942, loss: 0.301259\n",
      "step 4943, loss: 0.301049\n",
      "step 4944, loss: 0.300839\n",
      "step 4945, loss: 0.300629\n",
      "step 4946, loss: 0.300419\n",
      "step 4947, loss: 0.300210\n",
      "step 4948, loss: 0.300000\n",
      "step 4949, loss: 0.299790\n",
      "step 4950, loss: 0.299581\n",
      "step 4951, loss: 0.299371\n",
      "step 4952, loss: 0.299161\n",
      "step 4953, loss: 0.298952\n",
      "step 4954, loss: 0.298743\n",
      "step 4955, loss: 0.298534\n",
      "step 4956, loss: 0.298324\n",
      "step 4957, loss: 0.298115\n",
      "step 4958, loss: 0.297906\n",
      "step 4959, loss: 0.297697\n",
      "step 4960, loss: 0.297488\n",
      "step 4961, loss: 0.297279\n",
      "step 4962, loss: 0.297070\n",
      "step 4963, loss: 0.296861\n",
      "step 4964, loss: 0.296653\n",
      "step 4965, loss: 0.296444\n",
      "step 4966, loss: 0.296236\n",
      "step 4967, loss: 0.296027\n",
      "step 4968, loss: 0.295818\n",
      "step 4969, loss: 0.295610\n",
      "step 4970, loss: 0.295402\n",
      "step 4971, loss: 0.295193\n",
      "step 4972, loss: 0.294985\n",
      "step 4973, loss: 0.294777\n",
      "step 4974, loss: 0.294569\n",
      "step 4975, loss: 0.294361\n",
      "step 4976, loss: 0.294153\n",
      "step 4977, loss: 0.293945\n",
      "step 4978, loss: 0.293737\n",
      "step 4979, loss: 0.293529\n",
      "step 4980, loss: 0.293322\n",
      "step 4981, loss: 0.293114\n",
      "step 4982, loss: 0.292906\n",
      "step 4983, loss: 0.292699\n",
      "step 4984, loss: 0.292491\n",
      "step 4985, loss: 0.292284\n",
      "step 4986, loss: 0.292077\n",
      "step 4987, loss: 0.291869\n",
      "step 4988, loss: 0.291662\n",
      "step 4989, loss: 0.291455\n",
      "step 4990, loss: 0.291248\n",
      "step 4991, loss: 0.291041\n",
      "step 4992, loss: 0.290834\n",
      "step 4993, loss: 0.290627\n",
      "step 4994, loss: 0.290420\n",
      "step 4995, loss: 0.290213\n",
      "step 4996, loss: 0.290007\n",
      "step 4997, loss: 0.289800\n",
      "step 4998, loss: 0.289594\n",
      "step 4999, loss: 0.289387\n",
      "step 5000, loss: 0.289181\n",
      "step 5001, loss: 0.288974\n",
      "step 5002, loss: 0.288768\n",
      "step 5003, loss: 0.288562\n",
      "step 5004, loss: 0.288356\n",
      "step 5005, loss: 0.288150\n",
      "step 5006, loss: 0.287944\n",
      "step 5007, loss: 0.287738\n",
      "step 5008, loss: 0.287532\n",
      "step 5009, loss: 0.287326\n",
      "step 5010, loss: 0.287120\n",
      "step 5011, loss: 0.286914\n",
      "step 5012, loss: 0.286709\n",
      "step 5013, loss: 0.286503\n",
      "step 5014, loss: 0.286298\n",
      "step 5015, loss: 0.286092\n",
      "step 5016, loss: 0.285887\n",
      "step 5017, loss: 0.285681\n",
      "step 5018, loss: 0.285476\n",
      "step 5019, loss: 0.285271\n",
      "step 5020, loss: 0.285066\n",
      "step 5021, loss: 0.284861\n",
      "step 5022, loss: 0.284656\n",
      "step 5023, loss: 0.284451\n",
      "step 5024, loss: 0.284246\n",
      "step 5025, loss: 0.284042\n",
      "step 5026, loss: 0.283837\n",
      "step 5027, loss: 0.283632\n",
      "step 5028, loss: 0.283428\n",
      "step 5029, loss: 0.283223\n",
      "step 5030, loss: 0.283018\n",
      "step 5031, loss: 0.282814\n",
      "step 5032, loss: 0.282610\n",
      "step 5033, loss: 0.282406\n",
      "step 5034, loss: 0.282201\n",
      "step 5035, loss: 0.281997\n",
      "step 5036, loss: 0.281793\n",
      "step 5037, loss: 0.281589\n",
      "step 5038, loss: 0.281385\n",
      "step 5039, loss: 0.281181\n",
      "step 5040, loss: 0.280978\n",
      "step 5041, loss: 0.280774\n",
      "step 5042, loss: 0.280570\n",
      "step 5043, loss: 0.280367\n",
      "step 5044, loss: 0.280163\n",
      "step 5045, loss: 0.279960\n",
      "step 5046, loss: 0.279756\n",
      "step 5047, loss: 0.279553\n",
      "step 5048, loss: 0.279350\n",
      "step 5049, loss: 0.279147\n",
      "step 5050, loss: 0.278944\n",
      "step 5051, loss: 0.278740\n",
      "step 5052, loss: 0.278538\n",
      "step 5053, loss: 0.278335\n",
      "step 5054, loss: 0.278132\n",
      "step 5055, loss: 0.277929\n",
      "step 5056, loss: 0.277727\n",
      "step 5057, loss: 0.277524\n",
      "step 5058, loss: 0.277321\n",
      "step 5059, loss: 0.277119\n",
      "step 5060, loss: 0.276916\n",
      "step 5061, loss: 0.276714\n",
      "step 5062, loss: 0.276512\n",
      "step 5063, loss: 0.276309\n",
      "step 5064, loss: 0.276107\n",
      "step 5065, loss: 0.275905\n",
      "step 5066, loss: 0.275703\n",
      "step 5067, loss: 0.275501\n",
      "step 5068, loss: 0.275299\n",
      "step 5069, loss: 0.275098\n",
      "step 5070, loss: 0.274896\n",
      "step 5071, loss: 0.274694\n",
      "step 5072, loss: 0.274493\n",
      "step 5073, loss: 0.274291\n",
      "step 5074, loss: 0.274090\n",
      "step 5075, loss: 0.273888\n",
      "step 5076, loss: 0.273687\n",
      "step 5077, loss: 0.273486\n",
      "step 5078, loss: 0.273284\n",
      "step 5079, loss: 0.273083\n",
      "step 5080, loss: 0.272882\n",
      "step 5081, loss: 0.272681\n",
      "step 5082, loss: 0.272480\n",
      "step 5083, loss: 0.272280\n",
      "step 5084, loss: 0.272079\n",
      "step 5085, loss: 0.271878\n",
      "step 5086, loss: 0.271677\n",
      "step 5087, loss: 0.271477\n",
      "step 5088, loss: 0.271276\n",
      "step 5089, loss: 0.271076\n",
      "step 5090, loss: 0.270876\n",
      "step 5091, loss: 0.270675\n",
      "step 5092, loss: 0.270475\n",
      "step 5093, loss: 0.270275\n",
      "step 5094, loss: 0.270075\n",
      "step 5095, loss: 0.269875\n",
      "step 5096, loss: 0.269675\n",
      "step 5097, loss: 0.269475\n",
      "step 5098, loss: 0.269275\n",
      "step 5099, loss: 0.269076\n",
      "step 5100, loss: 0.268876\n",
      "step 5101, loss: 0.268676\n",
      "step 5102, loss: 0.268477\n",
      "step 5103, loss: 0.268277\n",
      "step 5104, loss: 0.268078\n",
      "step 5105, loss: 0.267879\n",
      "step 5106, loss: 0.267680\n",
      "step 5107, loss: 0.267480\n",
      "step 5108, loss: 0.267281\n",
      "step 5109, loss: 0.267082\n",
      "step 5110, loss: 0.266883\n",
      "step 5111, loss: 0.266685\n",
      "step 5112, loss: 0.266486\n",
      "step 5113, loss: 0.266287\n",
      "step 5114, loss: 0.266088\n",
      "step 5115, loss: 0.265890\n",
      "step 5116, loss: 0.265691\n",
      "step 5117, loss: 0.265493\n",
      "step 5118, loss: 0.265294\n",
      "step 5119, loss: 0.265096\n",
      "step 5120, loss: 0.264898\n",
      "step 5121, loss: 0.264700\n",
      "step 5122, loss: 0.264502\n",
      "step 5123, loss: 0.264304\n",
      "step 5124, loss: 0.264106\n",
      "step 5125, loss: 0.263908\n",
      "step 5126, loss: 0.263710\n",
      "step 5127, loss: 0.263512\n",
      "step 5128, loss: 0.263315\n",
      "step 5129, loss: 0.263117\n",
      "step 5130, loss: 0.262919\n",
      "step 5131, loss: 0.262722\n",
      "step 5132, loss: 0.262525\n",
      "step 5133, loss: 0.262327\n",
      "step 5134, loss: 0.262130\n",
      "step 5135, loss: 0.261933\n",
      "step 5136, loss: 0.261736\n",
      "step 5137, loss: 0.261539\n",
      "step 5138, loss: 0.261342\n",
      "step 5139, loss: 0.261145\n",
      "step 5140, loss: 0.260948\n",
      "step 5141, loss: 0.260752\n",
      "step 5142, loss: 0.260555\n",
      "step 5143, loss: 0.260358\n",
      "step 5144, loss: 0.260162\n",
      "step 5145, loss: 0.259965\n",
      "step 5146, loss: 0.259769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5147, loss: 0.259573\n",
      "step 5148, loss: 0.259377\n",
      "step 5149, loss: 0.259181\n",
      "step 5150, loss: 0.258984\n",
      "step 5151, loss: 0.258789\n",
      "step 5152, loss: 0.258593\n",
      "step 5153, loss: 0.258397\n",
      "step 5154, loss: 0.258201\n",
      "step 5155, loss: 0.258005\n",
      "step 5156, loss: 0.257810\n",
      "step 5157, loss: 0.257614\n",
      "step 5158, loss: 0.257419\n",
      "step 5159, loss: 0.257223\n",
      "step 5160, loss: 0.257028\n",
      "step 5161, loss: 0.256833\n",
      "step 5162, loss: 0.256638\n",
      "step 5163, loss: 0.256442\n",
      "step 5164, loss: 0.256247\n",
      "step 5165, loss: 0.256052\n",
      "step 5166, loss: 0.255858\n",
      "step 5167, loss: 0.255663\n",
      "step 5168, loss: 0.255468\n",
      "step 5169, loss: 0.255273\n",
      "step 5170, loss: 0.255079\n",
      "step 5171, loss: 0.254884\n",
      "step 5172, loss: 0.254690\n",
      "step 5173, loss: 0.254495\n",
      "step 5174, loss: 0.254301\n",
      "step 5175, loss: 0.254107\n",
      "step 5176, loss: 0.253913\n",
      "step 5177, loss: 0.253719\n",
      "step 5178, loss: 0.253525\n",
      "step 5179, loss: 0.253331\n",
      "step 5180, loss: 0.253137\n",
      "step 5181, loss: 0.252943\n",
      "step 5182, loss: 0.252750\n",
      "step 5183, loss: 0.252556\n",
      "step 5184, loss: 0.252362\n",
      "step 5185, loss: 0.252169\n",
      "step 5186, loss: 0.251976\n",
      "step 5187, loss: 0.251782\n",
      "step 5188, loss: 0.251589\n",
      "step 5189, loss: 0.251396\n",
      "step 5190, loss: 0.251203\n",
      "step 5191, loss: 0.251010\n",
      "step 5192, loss: 0.250817\n",
      "step 5193, loss: 0.250624\n",
      "step 5194, loss: 0.250431\n",
      "step 5195, loss: 0.250238\n",
      "step 5196, loss: 0.250046\n",
      "step 5197, loss: 0.249853\n",
      "step 5198, loss: 0.249661\n",
      "step 5199, loss: 0.249468\n",
      "step 5200, loss: 0.249276\n",
      "step 5201, loss: 0.249084\n",
      "step 5202, loss: 0.248892\n",
      "step 5203, loss: 0.248699\n",
      "step 5204, loss: 0.248507\n",
      "step 5205, loss: 0.248315\n",
      "step 5206, loss: 0.248123\n",
      "step 5207, loss: 0.247931\n",
      "step 5208, loss: 0.247740\n",
      "step 5209, loss: 0.247548\n",
      "step 5210, loss: 0.247357\n",
      "step 5211, loss: 0.247165\n",
      "step 5212, loss: 0.246974\n",
      "step 5213, loss: 0.246782\n",
      "step 5214, loss: 0.246591\n",
      "step 5215, loss: 0.246400\n",
      "step 5216, loss: 0.246209\n",
      "step 5217, loss: 0.246018\n",
      "step 5218, loss: 0.245827\n",
      "step 5219, loss: 0.245636\n",
      "step 5220, loss: 0.245445\n",
      "step 5221, loss: 0.245254\n",
      "step 5222, loss: 0.245063\n",
      "step 5223, loss: 0.244873\n",
      "step 5224, loss: 0.244683\n",
      "step 5225, loss: 0.244492\n",
      "step 5226, loss: 0.244302\n",
      "step 5227, loss: 0.244112\n",
      "step 5228, loss: 0.243921\n",
      "step 5229, loss: 0.243731\n",
      "step 5230, loss: 0.243541\n",
      "step 5231, loss: 0.243351\n",
      "step 5232, loss: 0.243161\n",
      "step 5233, loss: 0.242971\n",
      "step 5234, loss: 0.242782\n",
      "step 5235, loss: 0.242592\n",
      "step 5236, loss: 0.242402\n",
      "step 5237, loss: 0.242213\n",
      "step 5238, loss: 0.242023\n",
      "step 5239, loss: 0.241834\n",
      "step 5240, loss: 0.241645\n",
      "step 5241, loss: 0.241455\n",
      "step 5242, loss: 0.241266\n",
      "step 5243, loss: 0.241077\n",
      "step 5244, loss: 0.240889\n",
      "step 5245, loss: 0.240700\n",
      "step 5246, loss: 0.240511\n",
      "step 5247, loss: 0.240322\n",
      "step 5248, loss: 0.240133\n",
      "step 5249, loss: 0.239945\n",
      "step 5250, loss: 0.239756\n",
      "step 5251, loss: 0.239568\n",
      "step 5252, loss: 0.239379\n",
      "step 5253, loss: 0.239191\n",
      "step 5254, loss: 0.239003\n",
      "step 5255, loss: 0.238815\n",
      "step 5256, loss: 0.238627\n",
      "step 5257, loss: 0.238439\n",
      "step 5258, loss: 0.238251\n",
      "step 5259, loss: 0.238063\n",
      "step 5260, loss: 0.237875\n",
      "step 5261, loss: 0.237688\n",
      "step 5262, loss: 0.237500\n",
      "step 5263, loss: 0.237313\n",
      "step 5264, loss: 0.237125\n",
      "step 5265, loss: 0.236938\n",
      "step 5266, loss: 0.236751\n",
      "step 5267, loss: 0.236564\n",
      "step 5268, loss: 0.236377\n",
      "step 5269, loss: 0.236190\n",
      "step 5270, loss: 0.236003\n",
      "step 5271, loss: 0.235816\n",
      "step 5272, loss: 0.235629\n",
      "step 5273, loss: 0.235442\n",
      "step 5274, loss: 0.235256\n",
      "step 5275, loss: 0.235069\n",
      "step 5276, loss: 0.234883\n",
      "step 5277, loss: 0.234696\n",
      "step 5278, loss: 0.234510\n",
      "step 5279, loss: 0.234324\n",
      "step 5280, loss: 0.234138\n",
      "step 5281, loss: 0.233952\n",
      "step 5282, loss: 0.233766\n",
      "step 5283, loss: 0.233580\n",
      "step 5284, loss: 0.233394\n",
      "step 5285, loss: 0.233208\n",
      "step 5286, loss: 0.233022\n",
      "step 5287, loss: 0.232837\n",
      "step 5288, loss: 0.232651\n",
      "step 5289, loss: 0.232466\n",
      "step 5290, loss: 0.232280\n",
      "step 5291, loss: 0.232095\n",
      "step 5292, loss: 0.231910\n",
      "step 5293, loss: 0.231725\n",
      "step 5294, loss: 0.231540\n",
      "step 5295, loss: 0.231355\n",
      "step 5296, loss: 0.231170\n",
      "step 5297, loss: 0.230985\n",
      "step 5298, loss: 0.230801\n",
      "step 5299, loss: 0.230616\n",
      "step 5300, loss: 0.230431\n",
      "step 5301, loss: 0.230247\n",
      "step 5302, loss: 0.230062\n",
      "step 5303, loss: 0.229878\n",
      "step 5304, loss: 0.229694\n",
      "step 5305, loss: 0.229510\n",
      "step 5306, loss: 0.229326\n",
      "step 5307, loss: 0.229142\n",
      "step 5308, loss: 0.228958\n",
      "step 5309, loss: 0.228774\n",
      "step 5310, loss: 0.228590\n",
      "step 5311, loss: 0.228407\n",
      "step 5312, loss: 0.228223\n",
      "step 5313, loss: 0.228040\n",
      "step 5314, loss: 0.227856\n",
      "step 5315, loss: 0.227673\n",
      "step 5316, loss: 0.227490\n",
      "step 5317, loss: 0.227306\n",
      "step 5318, loss: 0.227123\n",
      "step 5319, loss: 0.226940\n",
      "step 5320, loss: 0.226757\n",
      "step 5321, loss: 0.226574\n",
      "step 5322, loss: 0.226392\n",
      "step 5323, loss: 0.226209\n",
      "step 5324, loss: 0.226026\n",
      "step 5325, loss: 0.225844\n",
      "step 5326, loss: 0.225661\n",
      "step 5327, loss: 0.225479\n",
      "step 5328, loss: 0.225297\n",
      "step 5329, loss: 0.225114\n",
      "step 5330, loss: 0.224932\n",
      "step 5331, loss: 0.224750\n",
      "step 5332, loss: 0.224568\n",
      "step 5333, loss: 0.224387\n",
      "step 5334, loss: 0.224205\n",
      "step 5335, loss: 0.224023\n",
      "step 5336, loss: 0.223841\n",
      "step 5337, loss: 0.223660\n",
      "step 5338, loss: 0.223478\n",
      "step 5339, loss: 0.223297\n",
      "step 5340, loss: 0.223116\n",
      "step 5341, loss: 0.222934\n",
      "step 5342, loss: 0.222753\n",
      "step 5343, loss: 0.222572\n",
      "step 5344, loss: 0.222391\n",
      "step 5345, loss: 0.222210\n",
      "step 5346, loss: 0.222029\n",
      "step 5347, loss: 0.221849\n",
      "step 5348, loss: 0.221668\n",
      "step 5349, loss: 0.221488\n",
      "step 5350, loss: 0.221307\n",
      "step 5351, loss: 0.221126\n",
      "step 5352, loss: 0.220946\n",
      "step 5353, loss: 0.220766\n",
      "step 5354, loss: 0.220586\n",
      "step 5355, loss: 0.220406\n",
      "step 5356, loss: 0.220226\n",
      "step 5357, loss: 0.220046\n",
      "step 5358, loss: 0.219866\n",
      "step 5359, loss: 0.219686\n",
      "step 5360, loss: 0.219507\n",
      "step 5361, loss: 0.219327\n",
      "step 5362, loss: 0.219147\n",
      "step 5363, loss: 0.218968\n",
      "step 5364, loss: 0.218789\n",
      "step 5365, loss: 0.218610\n",
      "step 5366, loss: 0.218430\n",
      "step 5367, loss: 0.218251\n",
      "step 5368, loss: 0.218072\n",
      "step 5369, loss: 0.217893\n",
      "step 5370, loss: 0.217714\n",
      "step 5371, loss: 0.217536\n",
      "step 5372, loss: 0.217357\n",
      "step 5373, loss: 0.217179\n",
      "step 5374, loss: 0.217000\n",
      "step 5375, loss: 0.216822\n",
      "step 5376, loss: 0.216643\n",
      "step 5377, loss: 0.216465\n",
      "step 5378, loss: 0.216287\n",
      "step 5379, loss: 0.216109\n",
      "step 5380, loss: 0.215931\n",
      "step 5381, loss: 0.215753\n",
      "step 5382, loss: 0.215575\n",
      "step 5383, loss: 0.215397\n",
      "step 5384, loss: 0.215220\n",
      "step 5385, loss: 0.215042\n",
      "step 5386, loss: 0.214864\n",
      "step 5387, loss: 0.214687\n",
      "step 5388, loss: 0.214510\n",
      "step 5389, loss: 0.214332\n",
      "step 5390, loss: 0.214155\n",
      "step 5391, loss: 0.213978\n",
      "step 5392, loss: 0.213801\n",
      "step 5393, loss: 0.213624\n",
      "step 5394, loss: 0.213447\n",
      "step 5395, loss: 0.213271\n",
      "step 5396, loss: 0.213094\n",
      "step 5397, loss: 0.212918\n",
      "step 5398, loss: 0.212741\n",
      "step 5399, loss: 0.212565\n",
      "step 5400, loss: 0.212388\n",
      "step 5401, loss: 0.212212\n",
      "step 5402, loss: 0.212036\n",
      "step 5403, loss: 0.211860\n",
      "step 5404, loss: 0.211684\n",
      "step 5405, loss: 0.211508\n",
      "step 5406, loss: 0.211332\n",
      "step 5407, loss: 0.211156\n",
      "step 5408, loss: 0.210981\n",
      "step 5409, loss: 0.210805\n",
      "step 5410, loss: 0.210630\n",
      "step 5411, loss: 0.210454\n",
      "step 5412, loss: 0.210279\n",
      "step 5413, loss: 0.210104\n",
      "step 5414, loss: 0.209929\n",
      "step 5415, loss: 0.209754\n",
      "step 5416, loss: 0.209579\n",
      "step 5417, loss: 0.209404\n",
      "step 5418, loss: 0.209229\n",
      "step 5419, loss: 0.209054\n",
      "step 5420, loss: 0.208880\n",
      "step 5421, loss: 0.208705\n",
      "step 5422, loss: 0.208530\n",
      "step 5423, loss: 0.208356\n",
      "step 5424, loss: 0.208182\n",
      "step 5425, loss: 0.208008\n",
      "step 5426, loss: 0.207833\n",
      "step 5427, loss: 0.207659\n",
      "step 5428, loss: 0.207485\n",
      "step 5429, loss: 0.207312\n",
      "step 5430, loss: 0.207138\n",
      "step 5431, loss: 0.206964\n",
      "step 5432, loss: 0.206790\n",
      "step 5433, loss: 0.206617\n",
      "step 5434, loss: 0.206444\n",
      "step 5435, loss: 0.206270\n",
      "step 5436, loss: 0.206097\n",
      "step 5437, loss: 0.205924\n",
      "step 5438, loss: 0.205751\n",
      "step 5439, loss: 0.205578\n",
      "step 5440, loss: 0.205405\n",
      "step 5441, loss: 0.205232\n",
      "step 5442, loss: 0.205059\n",
      "step 5443, loss: 0.204886\n",
      "step 5444, loss: 0.204714\n",
      "step 5445, loss: 0.204541\n",
      "step 5446, loss: 0.204369\n",
      "step 5447, loss: 0.204197\n",
      "step 5448, loss: 0.204024\n",
      "step 5449, loss: 0.203852\n",
      "step 5450, loss: 0.203680\n",
      "step 5451, loss: 0.203508\n",
      "step 5452, loss: 0.203336\n",
      "step 5453, loss: 0.203164\n",
      "step 5454, loss: 0.202993\n",
      "step 5455, loss: 0.202821\n",
      "step 5456, loss: 0.202650\n",
      "step 5457, loss: 0.202478\n",
      "step 5458, loss: 0.202307\n",
      "step 5459, loss: 0.202135\n",
      "step 5460, loss: 0.201964\n",
      "step 5461, loss: 0.201793\n",
      "step 5462, loss: 0.201622\n",
      "step 5463, loss: 0.201451\n",
      "step 5464, loss: 0.201280\n",
      "step 5465, loss: 0.201110\n",
      "step 5466, loss: 0.200939\n",
      "step 5467, loss: 0.200768\n",
      "step 5468, loss: 0.200598\n",
      "step 5469, loss: 0.200427\n",
      "step 5470, loss: 0.200257\n",
      "step 5471, loss: 0.200087\n",
      "step 5472, loss: 0.199916\n",
      "step 5473, loss: 0.199746\n",
      "step 5474, loss: 0.199576\n",
      "step 5475, loss: 0.199406\n",
      "step 5476, loss: 0.199237\n",
      "step 5477, loss: 0.199067\n",
      "step 5478, loss: 0.198897\n",
      "step 5479, loss: 0.198728\n",
      "step 5480, loss: 0.198558\n",
      "step 5481, loss: 0.198389\n",
      "step 5482, loss: 0.198220\n",
      "step 5483, loss: 0.198050\n",
      "step 5484, loss: 0.197881\n",
      "step 5485, loss: 0.197712\n",
      "step 5486, loss: 0.197543\n",
      "step 5487, loss: 0.197374\n",
      "step 5488, loss: 0.197206\n",
      "step 5489, loss: 0.197037\n",
      "step 5490, loss: 0.196868\n",
      "step 5491, loss: 0.196700\n",
      "step 5492, loss: 0.196532\n",
      "step 5493, loss: 0.196363\n",
      "step 5494, loss: 0.196195\n",
      "step 5495, loss: 0.196027\n",
      "step 5496, loss: 0.195859\n",
      "step 5497, loss: 0.195691\n",
      "step 5498, loss: 0.195523\n",
      "step 5499, loss: 0.195355\n",
      "step 5500, loss: 0.195187\n",
      "step 5501, loss: 0.195020\n",
      "step 5502, loss: 0.194852\n",
      "step 5503, loss: 0.194685\n",
      "step 5504, loss: 0.194517\n",
      "step 5505, loss: 0.194350\n",
      "step 5506, loss: 0.194183\n",
      "step 5507, loss: 0.194016\n",
      "step 5508, loss: 0.193849\n",
      "step 5509, loss: 0.193682\n",
      "step 5510, loss: 0.193515\n",
      "step 5511, loss: 0.193348\n",
      "step 5512, loss: 0.193181\n",
      "step 5513, loss: 0.193015\n",
      "step 5514, loss: 0.192848\n",
      "step 5515, loss: 0.192682\n",
      "step 5516, loss: 0.192516\n",
      "step 5517, loss: 0.192349\n",
      "step 5518, loss: 0.192183\n",
      "step 5519, loss: 0.192017\n",
      "step 5520, loss: 0.191851\n",
      "step 5521, loss: 0.191685\n",
      "step 5522, loss: 0.191520\n",
      "step 5523, loss: 0.191354\n",
      "step 5524, loss: 0.191188\n",
      "step 5525, loss: 0.191023\n",
      "step 5526, loss: 0.190857\n",
      "step 5527, loss: 0.190692\n",
      "step 5528, loss: 0.190527\n",
      "step 5529, loss: 0.190362\n",
      "step 5530, loss: 0.190196\n",
      "step 5531, loss: 0.190032\n",
      "step 5532, loss: 0.189867\n",
      "step 5533, loss: 0.189702\n",
      "step 5534, loss: 0.189537\n",
      "step 5535, loss: 0.189372\n",
      "step 5536, loss: 0.189208\n",
      "step 5537, loss: 0.189043\n",
      "step 5538, loss: 0.188879\n",
      "step 5539, loss: 0.188715\n",
      "step 5540, loss: 0.188550\n",
      "step 5541, loss: 0.188386\n",
      "step 5542, loss: 0.188222\n",
      "step 5543, loss: 0.188059\n",
      "step 5544, loss: 0.187895\n",
      "step 5545, loss: 0.187731\n",
      "step 5546, loss: 0.187567\n",
      "step 5547, loss: 0.187404\n",
      "step 5548, loss: 0.187240\n",
      "step 5549, loss: 0.187077\n",
      "step 5550, loss: 0.186914\n",
      "step 5551, loss: 0.186750\n",
      "step 5552, loss: 0.186587\n",
      "step 5553, loss: 0.186424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5554, loss: 0.186261\n",
      "step 5555, loss: 0.186098\n",
      "step 5556, loss: 0.185936\n",
      "step 5557, loss: 0.185773\n",
      "step 5558, loss: 0.185610\n",
      "step 5559, loss: 0.185448\n",
      "step 5560, loss: 0.185285\n",
      "step 5561, loss: 0.185123\n",
      "step 5562, loss: 0.184961\n",
      "step 5563, loss: 0.184799\n",
      "step 5564, loss: 0.184637\n",
      "step 5565, loss: 0.184475\n",
      "step 5566, loss: 0.184313\n",
      "step 5567, loss: 0.184151\n",
      "step 5568, loss: 0.183989\n",
      "step 5569, loss: 0.183828\n",
      "step 5570, loss: 0.183666\n",
      "step 5571, loss: 0.183505\n",
      "step 5572, loss: 0.183344\n",
      "step 5573, loss: 0.183182\n",
      "step 5574, loss: 0.183021\n",
      "step 5575, loss: 0.182860\n",
      "step 5576, loss: 0.182699\n",
      "step 5577, loss: 0.182538\n",
      "step 5578, loss: 0.182377\n",
      "step 5579, loss: 0.182217\n",
      "step 5580, loss: 0.182056\n",
      "step 5581, loss: 0.181896\n",
      "step 5582, loss: 0.181735\n",
      "step 5583, loss: 0.181575\n",
      "step 5584, loss: 0.181415\n",
      "step 5585, loss: 0.181254\n",
      "step 5586, loss: 0.181095\n",
      "step 5587, loss: 0.180934\n",
      "step 5588, loss: 0.180774\n",
      "step 5589, loss: 0.180615\n",
      "step 5590, loss: 0.180455\n",
      "step 5591, loss: 0.180295\n",
      "step 5592, loss: 0.180136\n",
      "step 5593, loss: 0.179976\n",
      "step 5594, loss: 0.179817\n",
      "step 5595, loss: 0.179658\n",
      "step 5596, loss: 0.179499\n",
      "step 5597, loss: 0.179339\n",
      "step 5598, loss: 0.179180\n",
      "step 5599, loss: 0.179022\n",
      "step 5600, loss: 0.178863\n",
      "step 5601, loss: 0.178704\n",
      "step 5602, loss: 0.178546\n",
      "step 5603, loss: 0.178387\n",
      "step 5604, loss: 0.178229\n",
      "step 5605, loss: 0.178070\n",
      "step 5606, loss: 0.177912\n",
      "step 5607, loss: 0.177754\n",
      "step 5608, loss: 0.177596\n",
      "step 5609, loss: 0.177438\n",
      "step 5610, loss: 0.177280\n",
      "step 5611, loss: 0.177122\n",
      "step 5612, loss: 0.176964\n",
      "step 5613, loss: 0.176807\n",
      "step 5614, loss: 0.176649\n",
      "step 5615, loss: 0.176492\n",
      "step 5616, loss: 0.176334\n",
      "step 5617, loss: 0.176177\n",
      "step 5618, loss: 0.176020\n",
      "step 5619, loss: 0.175863\n",
      "step 5620, loss: 0.175706\n",
      "step 5621, loss: 0.175549\n",
      "step 5622, loss: 0.175392\n",
      "step 5623, loss: 0.175235\n",
      "step 5624, loss: 0.175079\n",
      "step 5625, loss: 0.174922\n",
      "step 5626, loss: 0.174766\n",
      "step 5627, loss: 0.174609\n",
      "step 5628, loss: 0.174453\n",
      "step 5629, loss: 0.174297\n",
      "step 5630, loss: 0.174141\n",
      "step 5631, loss: 0.173985\n",
      "step 5632, loss: 0.173829\n",
      "step 5633, loss: 0.173673\n",
      "step 5634, loss: 0.173517\n",
      "step 5635, loss: 0.173362\n",
      "step 5636, loss: 0.173206\n",
      "step 5637, loss: 0.173051\n",
      "step 5638, loss: 0.172896\n",
      "step 5639, loss: 0.172740\n",
      "step 5640, loss: 0.172585\n",
      "step 5641, loss: 0.172430\n",
      "step 5642, loss: 0.172275\n",
      "step 5643, loss: 0.172120\n",
      "step 5644, loss: 0.171965\n",
      "step 5645, loss: 0.171811\n",
      "step 5646, loss: 0.171656\n",
      "step 5647, loss: 0.171501\n",
      "step 5648, loss: 0.171347\n",
      "step 5649, loss: 0.171193\n",
      "step 5650, loss: 0.171039\n",
      "step 5651, loss: 0.170884\n",
      "step 5652, loss: 0.170730\n",
      "step 5653, loss: 0.170576\n",
      "step 5654, loss: 0.170422\n",
      "step 5655, loss: 0.170269\n",
      "step 5656, loss: 0.170115\n",
      "step 5657, loss: 0.169961\n",
      "step 5658, loss: 0.169808\n",
      "step 5659, loss: 0.169654\n",
      "step 5660, loss: 0.169501\n",
      "step 5661, loss: 0.169348\n",
      "step 5662, loss: 0.169195\n",
      "step 5663, loss: 0.169042\n",
      "step 5664, loss: 0.168889\n",
      "step 5665, loss: 0.168736\n",
      "step 5666, loss: 0.168583\n",
      "step 5667, loss: 0.168430\n",
      "step 5668, loss: 0.168278\n",
      "step 5669, loss: 0.168125\n",
      "step 5670, loss: 0.167973\n",
      "step 5671, loss: 0.167820\n",
      "step 5672, loss: 0.167668\n",
      "step 5673, loss: 0.167516\n",
      "step 5674, loss: 0.167364\n",
      "step 5675, loss: 0.167212\n",
      "step 5676, loss: 0.167060\n",
      "step 5677, loss: 0.166909\n",
      "step 5678, loss: 0.166757\n",
      "step 5679, loss: 0.166605\n",
      "step 5680, loss: 0.166454\n",
      "step 5681, loss: 0.166302\n",
      "step 5682, loss: 0.166151\n",
      "step 5683, loss: 0.166000\n",
      "step 5684, loss: 0.165849\n",
      "step 5685, loss: 0.165698\n",
      "step 5686, loss: 0.165547\n",
      "step 5687, loss: 0.165396\n",
      "step 5688, loss: 0.165245\n",
      "step 5689, loss: 0.165094\n",
      "step 5690, loss: 0.164944\n",
      "step 5691, loss: 0.164793\n",
      "step 5692, loss: 0.164643\n",
      "step 5693, loss: 0.164493\n",
      "step 5694, loss: 0.164343\n",
      "step 5695, loss: 0.164192\n",
      "step 5696, loss: 0.164043\n",
      "step 5697, loss: 0.163892\n",
      "step 5698, loss: 0.163743\n",
      "step 5699, loss: 0.163593\n",
      "step 5700, loss: 0.163443\n",
      "step 5701, loss: 0.163294\n",
      "step 5702, loss: 0.163144\n",
      "step 5703, loss: 0.162995\n",
      "step 5704, loss: 0.162846\n",
      "step 5705, loss: 0.162697\n",
      "step 5706, loss: 0.162547\n",
      "step 5707, loss: 0.162398\n",
      "step 5708, loss: 0.162250\n",
      "step 5709, loss: 0.162101\n",
      "step 5710, loss: 0.161952\n",
      "step 5711, loss: 0.161803\n",
      "step 5712, loss: 0.161655\n",
      "step 5713, loss: 0.161506\n",
      "step 5714, loss: 0.161358\n",
      "step 5715, loss: 0.161210\n",
      "step 5716, loss: 0.161062\n",
      "step 5717, loss: 0.160914\n",
      "step 5718, loss: 0.160766\n",
      "step 5719, loss: 0.160618\n",
      "step 5720, loss: 0.160470\n",
      "step 5721, loss: 0.160322\n",
      "step 5722, loss: 0.160175\n",
      "step 5723, loss: 0.160027\n",
      "step 5724, loss: 0.159880\n",
      "step 5725, loss: 0.159733\n",
      "step 5726, loss: 0.159585\n",
      "step 5727, loss: 0.159438\n",
      "step 5728, loss: 0.159291\n",
      "step 5729, loss: 0.159144\n",
      "step 5730, loss: 0.158997\n",
      "step 5731, loss: 0.158851\n",
      "step 5732, loss: 0.158704\n",
      "step 5733, loss: 0.158557\n",
      "step 5734, loss: 0.158411\n",
      "step 5735, loss: 0.158265\n",
      "step 5736, loss: 0.158118\n",
      "step 5737, loss: 0.157972\n",
      "step 5738, loss: 0.157826\n",
      "step 5739, loss: 0.157680\n",
      "step 5740, loss: 0.157534\n",
      "step 5741, loss: 0.157388\n",
      "step 5742, loss: 0.157243\n",
      "step 5743, loss: 0.157097\n",
      "step 5744, loss: 0.156951\n",
      "step 5745, loss: 0.156806\n",
      "step 5746, loss: 0.156660\n",
      "step 5747, loss: 0.156515\n",
      "step 5748, loss: 0.156370\n",
      "step 5749, loss: 0.156225\n",
      "step 5750, loss: 0.156080\n",
      "step 5751, loss: 0.155935\n",
      "step 5752, loss: 0.155790\n",
      "step 5753, loss: 0.155646\n",
      "step 5754, loss: 0.155501\n",
      "step 5755, loss: 0.155356\n",
      "step 5756, loss: 0.155212\n",
      "step 5757, loss: 0.155068\n",
      "step 5758, loss: 0.154923\n",
      "step 5759, loss: 0.154779\n",
      "step 5760, loss: 0.154635\n",
      "step 5761, loss: 0.154491\n",
      "step 5762, loss: 0.154347\n",
      "step 5763, loss: 0.154204\n",
      "step 5764, loss: 0.154060\n",
      "step 5765, loss: 0.153916\n",
      "step 5766, loss: 0.153773\n",
      "step 5767, loss: 0.153630\n",
      "step 5768, loss: 0.153486\n",
      "step 5769, loss: 0.153343\n",
      "step 5770, loss: 0.153200\n",
      "step 5771, loss: 0.153057\n",
      "step 5772, loss: 0.152914\n",
      "step 5773, loss: 0.152771\n",
      "step 5774, loss: 0.152629\n",
      "step 5775, loss: 0.152486\n",
      "step 5776, loss: 0.152343\n",
      "step 5777, loss: 0.152201\n",
      "step 5778, loss: 0.152059\n",
      "step 5779, loss: 0.151916\n",
      "step 5780, loss: 0.151774\n",
      "step 5781, loss: 0.151632\n",
      "step 5782, loss: 0.151490\n",
      "step 5783, loss: 0.151348\n",
      "step 5784, loss: 0.151206\n",
      "step 5785, loss: 0.151065\n",
      "step 5786, loss: 0.150923\n",
      "step 5787, loss: 0.150781\n",
      "step 5788, loss: 0.150640\n",
      "step 5789, loss: 0.150499\n",
      "step 5790, loss: 0.150358\n",
      "step 5791, loss: 0.150216\n",
      "step 5792, loss: 0.150075\n",
      "step 5793, loss: 0.149934\n",
      "step 5794, loss: 0.149794\n",
      "step 5795, loss: 0.149653\n",
      "step 5796, loss: 0.149512\n",
      "step 5797, loss: 0.149372\n",
      "step 5798, loss: 0.149231\n",
      "step 5799, loss: 0.149091\n",
      "step 5800, loss: 0.148950\n",
      "step 5801, loss: 0.148810\n",
      "step 5802, loss: 0.148670\n",
      "step 5803, loss: 0.148530\n",
      "step 5804, loss: 0.148390\n",
      "step 5805, loss: 0.148250\n",
      "step 5806, loss: 0.148111\n",
      "step 5807, loss: 0.147971\n",
      "step 5808, loss: 0.147831\n",
      "step 5809, loss: 0.147692\n",
      "step 5810, loss: 0.147553\n",
      "step 5811, loss: 0.147413\n",
      "step 5812, loss: 0.147274\n",
      "step 5813, loss: 0.147135\n",
      "step 5814, loss: 0.146996\n",
      "step 5815, loss: 0.146857\n",
      "step 5816, loss: 0.146719\n",
      "step 5817, loss: 0.146580\n",
      "step 5818, loss: 0.146441\n",
      "step 5819, loss: 0.146303\n",
      "step 5820, loss: 0.146164\n",
      "step 5821, loss: 0.146026\n",
      "step 5822, loss: 0.145888\n",
      "step 5823, loss: 0.145750\n",
      "step 5824, loss: 0.145612\n",
      "step 5825, loss: 0.145474\n",
      "step 5826, loss: 0.145336\n",
      "step 5827, loss: 0.145198\n",
      "step 5828, loss: 0.145060\n",
      "step 5829, loss: 0.144923\n",
      "step 5830, loss: 0.144785\n",
      "step 5831, loss: 0.144648\n",
      "step 5832, loss: 0.144511\n",
      "step 5833, loss: 0.144374\n",
      "step 5834, loss: 0.144237\n",
      "step 5835, loss: 0.144100\n",
      "step 5836, loss: 0.143963\n",
      "step 5837, loss: 0.143826\n",
      "step 5838, loss: 0.143689\n",
      "step 5839, loss: 0.143553\n",
      "step 5840, loss: 0.143416\n",
      "step 5841, loss: 0.143280\n",
      "step 5842, loss: 0.143143\n",
      "step 5843, loss: 0.143007\n",
      "step 5844, loss: 0.142871\n",
      "step 5845, loss: 0.142735\n",
      "step 5846, loss: 0.142599\n",
      "step 5847, loss: 0.142463\n",
      "step 5848, loss: 0.142327\n",
      "step 5849, loss: 0.142192\n",
      "step 5850, loss: 0.142056\n",
      "step 5851, loss: 0.141921\n",
      "step 5852, loss: 0.141785\n",
      "step 5853, loss: 0.141650\n",
      "step 5854, loss: 0.141515\n",
      "step 5855, loss: 0.141380\n",
      "step 5856, loss: 0.141245\n",
      "step 5857, loss: 0.141110\n",
      "step 5858, loss: 0.140975\n",
      "step 5859, loss: 0.140840\n",
      "step 5860, loss: 0.140706\n",
      "step 5861, loss: 0.140571\n",
      "step 5862, loss: 0.140437\n",
      "step 5863, loss: 0.140302\n",
      "step 5864, loss: 0.140168\n",
      "step 5865, loss: 0.140034\n",
      "step 5866, loss: 0.139900\n",
      "step 5867, loss: 0.139766\n",
      "step 5868, loss: 0.139632\n",
      "step 5869, loss: 0.139498\n",
      "step 5870, loss: 0.139365\n",
      "step 5871, loss: 0.139231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 5872, loss: 0.139098\n",
      "step 5873, loss: 0.138964\n",
      "step 5874, loss: 0.138831\n",
      "step 5875, loss: 0.138698\n",
      "step 5876, loss: 0.138565\n",
      "step 5877, loss: 0.138432\n",
      "step 5878, loss: 0.138299\n",
      "step 5879, loss: 0.138166\n",
      "step 5880, loss: 0.138033\n",
      "step 5881, loss: 0.137900\n",
      "step 5882, loss: 0.137768\n",
      "step 5883, loss: 0.137635\n",
      "step 5884, loss: 0.137503\n",
      "step 5885, loss: 0.137371\n",
      "step 5886, loss: 0.137239\n",
      "step 5887, loss: 0.137107\n",
      "step 5888, loss: 0.136975\n",
      "step 5889, loss: 0.136843\n",
      "step 5890, loss: 0.136711\n",
      "step 5891, loss: 0.136579\n",
      "step 5892, loss: 0.136448\n",
      "step 5893, loss: 0.136316\n",
      "step 5894, loss: 0.136185\n",
      "step 5895, loss: 0.136054\n",
      "step 5896, loss: 0.135922\n",
      "step 5897, loss: 0.135791\n",
      "step 5898, loss: 0.135660\n",
      "step 5899, loss: 0.135529\n",
      "step 5900, loss: 0.135398\n",
      "step 5901, loss: 0.135268\n",
      "step 5902, loss: 0.135137\n",
      "step 5903, loss: 0.135007\n",
      "step 5904, loss: 0.134876\n",
      "step 5905, loss: 0.134746\n",
      "step 5906, loss: 0.134615\n",
      "step 5907, loss: 0.134485\n",
      "step 5908, loss: 0.134355\n",
      "step 5909, loss: 0.134225\n",
      "step 5910, loss: 0.134095\n",
      "step 5911, loss: 0.133965\n",
      "step 5912, loss: 0.133836\n",
      "step 5913, loss: 0.133706\n",
      "step 5914, loss: 0.133577\n",
      "step 5915, loss: 0.133447\n",
      "step 5916, loss: 0.133318\n",
      "step 5917, loss: 0.133189\n",
      "step 5918, loss: 0.133060\n",
      "step 5919, loss: 0.132930\n",
      "step 5920, loss: 0.132802\n",
      "step 5921, loss: 0.132673\n",
      "step 5922, loss: 0.132544\n",
      "step 5923, loss: 0.132415\n",
      "step 5924, loss: 0.132287\n",
      "step 5925, loss: 0.132158\n",
      "step 5926, loss: 0.132030\n",
      "step 5927, loss: 0.131902\n",
      "step 5928, loss: 0.131774\n",
      "step 5929, loss: 0.131645\n",
      "step 5930, loss: 0.131517\n",
      "step 5931, loss: 0.131390\n",
      "step 5932, loss: 0.131262\n",
      "step 5933, loss: 0.131134\n",
      "step 5934, loss: 0.131006\n",
      "step 5935, loss: 0.130879\n",
      "step 5936, loss: 0.130751\n",
      "step 5937, loss: 0.130624\n",
      "step 5938, loss: 0.130497\n",
      "step 5939, loss: 0.130370\n",
      "step 5940, loss: 0.130243\n",
      "step 5941, loss: 0.130116\n",
      "step 5942, loss: 0.129989\n",
      "step 5943, loss: 0.129862\n",
      "step 5944, loss: 0.129736\n",
      "step 5945, loss: 0.129609\n",
      "step 5946, loss: 0.129483\n",
      "step 5947, loss: 0.129356\n",
      "step 5948, loss: 0.129230\n",
      "step 5949, loss: 0.129104\n",
      "step 5950, loss: 0.128978\n",
      "step 5951, loss: 0.128852\n",
      "step 5952, loss: 0.128726\n",
      "step 5953, loss: 0.128600\n",
      "step 5954, loss: 0.128474\n",
      "step 5955, loss: 0.128349\n",
      "step 5956, loss: 0.128223\n",
      "step 5957, loss: 0.128098\n",
      "step 5958, loss: 0.127972\n",
      "step 5959, loss: 0.127847\n",
      "step 5960, loss: 0.127722\n",
      "step 5961, loss: 0.127597\n",
      "step 5962, loss: 0.127472\n",
      "step 5963, loss: 0.127347\n",
      "step 5964, loss: 0.127222\n",
      "step 5965, loss: 0.127098\n",
      "step 5966, loss: 0.126973\n",
      "step 5967, loss: 0.126849\n",
      "step 5968, loss: 0.126724\n",
      "step 5969, loss: 0.126600\n",
      "step 5970, loss: 0.126476\n",
      "step 5971, loss: 0.126352\n",
      "step 5972, loss: 0.126228\n",
      "step 5973, loss: 0.126104\n",
      "step 5974, loss: 0.125980\n",
      "step 5975, loss: 0.125856\n",
      "step 5976, loss: 0.125733\n",
      "step 5977, loss: 0.125609\n",
      "step 5978, loss: 0.125486\n",
      "step 5979, loss: 0.125363\n",
      "step 5980, loss: 0.125239\n",
      "step 5981, loss: 0.125116\n",
      "step 5982, loss: 0.124993\n",
      "step 5983, loss: 0.124870\n",
      "step 5984, loss: 0.124747\n",
      "step 5985, loss: 0.124624\n",
      "step 5986, loss: 0.124502\n",
      "step 5987, loss: 0.124379\n",
      "step 5988, loss: 0.124257\n",
      "step 5989, loss: 0.124134\n",
      "step 5990, loss: 0.124012\n",
      "step 5991, loss: 0.123890\n",
      "step 5992, loss: 0.123768\n",
      "step 5993, loss: 0.123646\n",
      "step 5994, loss: 0.123524\n",
      "step 5995, loss: 0.123402\n",
      "step 5996, loss: 0.123280\n",
      "step 5997, loss: 0.123159\n",
      "step 5998, loss: 0.123037\n",
      "step 5999, loss: 0.122916\n",
      "step 6000, loss: 0.122794\n",
      "step 6001, loss: 0.122673\n",
      "step 6002, loss: 0.122552\n",
      "step 6003, loss: 0.122431\n",
      "step 6004, loss: 0.122310\n",
      "step 6005, loss: 0.122189\n",
      "step 6006, loss: 0.122068\n",
      "step 6007, loss: 0.121948\n",
      "step 6008, loss: 0.121827\n",
      "step 6009, loss: 0.121707\n",
      "step 6010, loss: 0.121586\n",
      "step 6011, loss: 0.121466\n",
      "step 6012, loss: 0.121346\n",
      "step 6013, loss: 0.121226\n",
      "step 6014, loss: 0.121106\n",
      "step 6015, loss: 0.120986\n",
      "step 6016, loss: 0.120866\n",
      "step 6017, loss: 0.120746\n",
      "step 6018, loss: 0.120626\n",
      "step 6019, loss: 0.120507\n",
      "step 6020, loss: 0.120387\n",
      "step 6021, loss: 0.120268\n",
      "step 6022, loss: 0.120149\n",
      "step 6023, loss: 0.120030\n",
      "step 6024, loss: 0.119911\n",
      "step 6025, loss: 0.119792\n",
      "step 6026, loss: 0.119673\n",
      "step 6027, loss: 0.119554\n",
      "step 6028, loss: 0.119435\n",
      "step 6029, loss: 0.119317\n",
      "step 6030, loss: 0.119198\n",
      "step 6031, loss: 0.119080\n",
      "step 6032, loss: 0.118962\n",
      "step 6033, loss: 0.118843\n",
      "step 6034, loss: 0.118725\n",
      "step 6035, loss: 0.118607\n",
      "step 6036, loss: 0.118489\n",
      "step 6037, loss: 0.118372\n",
      "step 6038, loss: 0.118254\n",
      "step 6039, loss: 0.118136\n",
      "step 6040, loss: 0.118019\n",
      "step 6041, loss: 0.117901\n",
      "step 6042, loss: 0.117784\n",
      "step 6043, loss: 0.117666\n",
      "step 6044, loss: 0.117549\n",
      "step 6045, loss: 0.117432\n",
      "step 6046, loss: 0.117315\n",
      "step 6047, loss: 0.117198\n",
      "step 6048, loss: 0.117082\n",
      "step 6049, loss: 0.116965\n",
      "step 6050, loss: 0.116848\n",
      "step 6051, loss: 0.116732\n",
      "step 6052, loss: 0.116615\n",
      "step 6053, loss: 0.116499\n",
      "step 6054, loss: 0.116383\n",
      "step 6055, loss: 0.116267\n",
      "step 6056, loss: 0.116151\n",
      "step 6057, loss: 0.116035\n",
      "step 6058, loss: 0.115919\n",
      "step 6059, loss: 0.115803\n",
      "step 6060, loss: 0.115687\n",
      "step 6061, loss: 0.115572\n",
      "step 6062, loss: 0.115456\n",
      "step 6063, loss: 0.115341\n",
      "step 6064, loss: 0.115226\n",
      "step 6065, loss: 0.115111\n",
      "step 6066, loss: 0.114995\n",
      "step 6067, loss: 0.114880\n",
      "step 6068, loss: 0.114766\n",
      "step 6069, loss: 0.114651\n",
      "step 6070, loss: 0.114536\n",
      "step 6071, loss: 0.114421\n",
      "step 6072, loss: 0.114307\n",
      "step 6073, loss: 0.114192\n",
      "step 6074, loss: 0.114078\n",
      "step 6075, loss: 0.113964\n",
      "step 6076, loss: 0.113850\n",
      "step 6077, loss: 0.113736\n",
      "step 6078, loss: 0.113622\n",
      "step 6079, loss: 0.113508\n",
      "step 6080, loss: 0.113394\n",
      "step 6081, loss: 0.113280\n",
      "step 6082, loss: 0.113167\n",
      "step 6083, loss: 0.113053\n",
      "step 6084, loss: 0.112940\n",
      "step 6085, loss: 0.112827\n",
      "step 6086, loss: 0.112713\n",
      "step 6087, loss: 0.112600\n",
      "step 6088, loss: 0.112487\n",
      "step 6089, loss: 0.112374\n",
      "step 6090, loss: 0.112262\n",
      "step 6091, loss: 0.112149\n",
      "step 6092, loss: 0.112036\n",
      "step 6093, loss: 0.111924\n",
      "step 6094, loss: 0.111811\n",
      "step 6095, loss: 0.111699\n",
      "step 6096, loss: 0.111587\n",
      "step 6097, loss: 0.111474\n",
      "step 6098, loss: 0.111362\n",
      "step 6099, loss: 0.111250\n",
      "step 6100, loss: 0.111138\n",
      "step 6101, loss: 0.111027\n",
      "step 6102, loss: 0.110915\n",
      "step 6103, loss: 0.110803\n",
      "step 6104, loss: 0.110692\n",
      "step 6105, loss: 0.110580\n",
      "step 6106, loss: 0.110469\n",
      "step 6107, loss: 0.110358\n",
      "step 6108, loss: 0.110247\n",
      "step 6109, loss: 0.110136\n",
      "step 6110, loss: 0.110025\n",
      "step 6111, loss: 0.109914\n",
      "step 6112, loss: 0.109803\n",
      "step 6113, loss: 0.109692\n",
      "step 6114, loss: 0.109582\n",
      "step 6115, loss: 0.109471\n",
      "step 6116, loss: 0.109361\n",
      "step 6117, loss: 0.109251\n",
      "step 6118, loss: 0.109141\n",
      "step 6119, loss: 0.109030\n",
      "step 6120, loss: 0.108921\n",
      "step 6121, loss: 0.108811\n",
      "step 6122, loss: 0.108701\n",
      "step 6123, loss: 0.108591\n",
      "step 6124, loss: 0.108481\n",
      "step 6125, loss: 0.108372\n",
      "step 6126, loss: 0.108262\n",
      "step 6127, loss: 0.108153\n",
      "step 6128, loss: 0.108044\n",
      "step 6129, loss: 0.107935\n",
      "step 6130, loss: 0.107826\n",
      "step 6131, loss: 0.107717\n",
      "step 6132, loss: 0.107608\n",
      "step 6133, loss: 0.107499\n",
      "step 6134, loss: 0.107390\n",
      "step 6135, loss: 0.107282\n",
      "step 6136, loss: 0.107173\n",
      "step 6137, loss: 0.107065\n",
      "step 6138, loss: 0.106956\n",
      "step 6139, loss: 0.106848\n",
      "step 6140, loss: 0.106740\n",
      "step 6141, loss: 0.106632\n",
      "step 6142, loss: 0.106524\n",
      "step 6143, loss: 0.106416\n",
      "step 6144, loss: 0.106308\n",
      "step 6145, loss: 0.106201\n",
      "step 6146, loss: 0.106093\n",
      "step 6147, loss: 0.105986\n",
      "step 6148, loss: 0.105878\n",
      "step 6149, loss: 0.105771\n",
      "step 6150, loss: 0.105664\n",
      "step 6151, loss: 0.105557\n",
      "step 6152, loss: 0.105450\n",
      "step 6153, loss: 0.105343\n",
      "step 6154, loss: 0.105236\n",
      "step 6155, loss: 0.105129\n",
      "step 6156, loss: 0.105023\n",
      "step 6157, loss: 0.104916\n",
      "step 6158, loss: 0.104810\n",
      "step 6159, loss: 0.104703\n",
      "step 6160, loss: 0.104597\n",
      "step 6161, loss: 0.104491\n",
      "step 6162, loss: 0.104385\n",
      "step 6163, loss: 0.104279\n",
      "step 6164, loss: 0.104173\n",
      "step 6165, loss: 0.104067\n",
      "step 6166, loss: 0.103962\n",
      "step 6167, loss: 0.103856\n",
      "step 6168, loss: 0.103750\n",
      "step 6169, loss: 0.103645\n",
      "step 6170, loss: 0.103540\n",
      "step 6171, loss: 0.103434\n",
      "step 6172, loss: 0.103329\n",
      "step 6173, loss: 0.103224\n",
      "step 6174, loss: 0.103119\n",
      "step 6175, loss: 0.103014\n",
      "step 6176, loss: 0.102910\n",
      "step 6177, loss: 0.102805\n",
      "step 6178, loss: 0.102700\n",
      "step 6179, loss: 0.102596\n",
      "step 6180, loss: 0.102491\n",
      "step 6181, loss: 0.102387\n",
      "step 6182, loss: 0.102283\n",
      "step 6183, loss: 0.102179\n",
      "step 6184, loss: 0.102075\n",
      "step 6185, loss: 0.101971\n",
      "step 6186, loss: 0.101867\n",
      "step 6187, loss: 0.101763\n",
      "step 6188, loss: 0.101660\n",
      "step 6189, loss: 0.101556\n",
      "step 6190, loss: 0.101453\n",
      "step 6191, loss: 0.101349\n",
      "step 6192, loss: 0.101246\n",
      "step 6193, loss: 0.101143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6194, loss: 0.101040\n",
      "step 6195, loss: 0.100937\n",
      "step 6196, loss: 0.100834\n",
      "step 6197, loss: 0.100731\n",
      "step 6198, loss: 0.100628\n",
      "step 6199, loss: 0.100526\n",
      "step 6200, loss: 0.100423\n",
      "step 6201, loss: 0.100321\n",
      "step 6202, loss: 0.100218\n",
      "step 6203, loss: 0.100116\n",
      "step 6204, loss: 0.100014\n",
      "step 6205, loss: 0.099912\n",
      "step 6206, loss: 0.099810\n",
      "step 6207, loss: 0.099708\n",
      "step 6208, loss: 0.099606\n",
      "step 6209, loss: 0.099504\n",
      "step 6210, loss: 0.099403\n",
      "step 6211, loss: 0.099301\n",
      "step 6212, loss: 0.099200\n",
      "step 6213, loss: 0.099098\n",
      "step 6214, loss: 0.098997\n",
      "step 6215, loss: 0.098896\n",
      "step 6216, loss: 0.098795\n",
      "step 6217, loss: 0.098694\n",
      "step 6218, loss: 0.098593\n",
      "step 6219, loss: 0.098492\n",
      "step 6220, loss: 0.098392\n",
      "step 6221, loss: 0.098291\n",
      "step 6222, loss: 0.098191\n",
      "step 6223, loss: 0.098090\n",
      "step 6224, loss: 0.097990\n",
      "step 6225, loss: 0.097890\n",
      "step 6226, loss: 0.097789\n",
      "step 6227, loss: 0.097689\n",
      "step 6228, loss: 0.097589\n",
      "step 6229, loss: 0.097490\n",
      "step 6230, loss: 0.097390\n",
      "step 6231, loss: 0.097290\n",
      "step 6232, loss: 0.097191\n",
      "step 6233, loss: 0.097091\n",
      "step 6234, loss: 0.096992\n",
      "step 6235, loss: 0.096892\n",
      "step 6236, loss: 0.096793\n",
      "step 6237, loss: 0.096694\n",
      "step 6238, loss: 0.096595\n",
      "step 6239, loss: 0.096496\n",
      "step 6240, loss: 0.096397\n",
      "step 6241, loss: 0.096298\n",
      "step 6242, loss: 0.096200\n",
      "step 6243, loss: 0.096101\n",
      "step 6244, loss: 0.096003\n",
      "step 6245, loss: 0.095904\n",
      "step 6246, loss: 0.095806\n",
      "step 6247, loss: 0.095708\n",
      "step 6248, loss: 0.095610\n",
      "step 6249, loss: 0.095512\n",
      "step 6250, loss: 0.095414\n",
      "step 6251, loss: 0.095316\n",
      "step 6252, loss: 0.095218\n",
      "step 6253, loss: 0.095120\n",
      "step 6254, loss: 0.095023\n",
      "step 6255, loss: 0.094925\n",
      "step 6256, loss: 0.094828\n",
      "step 6257, loss: 0.094731\n",
      "step 6258, loss: 0.094634\n",
      "step 6259, loss: 0.094536\n",
      "step 6260, loss: 0.094439\n",
      "step 6261, loss: 0.094342\n",
      "step 6262, loss: 0.094246\n",
      "step 6263, loss: 0.094149\n",
      "step 6264, loss: 0.094052\n",
      "step 6265, loss: 0.093956\n",
      "step 6266, loss: 0.093859\n",
      "step 6267, loss: 0.093763\n",
      "step 6268, loss: 0.093667\n",
      "step 6269, loss: 0.093570\n",
      "step 6270, loss: 0.093474\n",
      "step 6271, loss: 0.093378\n",
      "step 6272, loss: 0.093282\n",
      "step 6273, loss: 0.093186\n",
      "step 6274, loss: 0.093091\n",
      "step 6275, loss: 0.092995\n",
      "step 6276, loss: 0.092900\n",
      "step 6277, loss: 0.092804\n",
      "step 6278, loss: 0.092709\n",
      "step 6279, loss: 0.092613\n",
      "step 6280, loss: 0.092518\n",
      "step 6281, loss: 0.092423\n",
      "step 6282, loss: 0.092328\n",
      "step 6283, loss: 0.092233\n",
      "step 6284, loss: 0.092138\n",
      "step 6285, loss: 0.092044\n",
      "step 6286, loss: 0.091949\n",
      "step 6287, loss: 0.091854\n",
      "step 6288, loss: 0.091760\n",
      "step 6289, loss: 0.091666\n",
      "step 6290, loss: 0.091571\n",
      "step 6291, loss: 0.091477\n",
      "step 6292, loss: 0.091383\n",
      "step 6293, loss: 0.091289\n",
      "step 6294, loss: 0.091195\n",
      "step 6295, loss: 0.091101\n",
      "step 6296, loss: 0.091007\n",
      "step 6297, loss: 0.090914\n",
      "step 6298, loss: 0.090820\n",
      "step 6299, loss: 0.090727\n",
      "step 6300, loss: 0.090633\n",
      "step 6301, loss: 0.090540\n",
      "step 6302, loss: 0.090447\n",
      "step 6303, loss: 0.090354\n",
      "step 6304, loss: 0.090261\n",
      "step 6305, loss: 0.090168\n",
      "step 6306, loss: 0.090075\n",
      "step 6307, loss: 0.089982\n",
      "step 6308, loss: 0.089890\n",
      "step 6309, loss: 0.089797\n",
      "step 6310, loss: 0.089705\n",
      "step 6311, loss: 0.089612\n",
      "step 6312, loss: 0.089520\n",
      "step 6313, loss: 0.089428\n",
      "step 6314, loss: 0.089336\n",
      "step 6315, loss: 0.089244\n",
      "step 6316, loss: 0.089152\n",
      "step 6317, loss: 0.089060\n",
      "step 6318, loss: 0.088968\n",
      "step 6319, loss: 0.088876\n",
      "step 6320, loss: 0.088785\n",
      "step 6321, loss: 0.088693\n",
      "step 6322, loss: 0.088602\n",
      "step 6323, loss: 0.088511\n",
      "step 6324, loss: 0.088419\n",
      "step 6325, loss: 0.088328\n",
      "step 6326, loss: 0.088237\n",
      "step 6327, loss: 0.088146\n",
      "step 6328, loss: 0.088055\n",
      "step 6329, loss: 0.087965\n",
      "step 6330, loss: 0.087874\n",
      "step 6331, loss: 0.087783\n",
      "step 6332, loss: 0.087693\n",
      "step 6333, loss: 0.087603\n",
      "step 6334, loss: 0.087512\n",
      "step 6335, loss: 0.087422\n",
      "step 6336, loss: 0.087332\n",
      "step 6337, loss: 0.087242\n",
      "step 6338, loss: 0.087152\n",
      "step 6339, loss: 0.087062\n",
      "step 6340, loss: 0.086972\n",
      "step 6341, loss: 0.086882\n",
      "step 6342, loss: 0.086793\n",
      "step 6343, loss: 0.086703\n",
      "step 6344, loss: 0.086614\n",
      "step 6345, loss: 0.086525\n",
      "step 6346, loss: 0.086435\n",
      "step 6347, loss: 0.086346\n",
      "step 6348, loss: 0.086257\n",
      "step 6349, loss: 0.086168\n",
      "step 6350, loss: 0.086079\n",
      "step 6351, loss: 0.085990\n",
      "step 6352, loss: 0.085902\n",
      "step 6353, loss: 0.085813\n",
      "step 6354, loss: 0.085725\n",
      "step 6355, loss: 0.085636\n",
      "step 6356, loss: 0.085548\n",
      "step 6357, loss: 0.085460\n",
      "step 6358, loss: 0.085371\n",
      "step 6359, loss: 0.085283\n",
      "step 6360, loss: 0.085195\n",
      "step 6361, loss: 0.085107\n",
      "step 6362, loss: 0.085020\n",
      "step 6363, loss: 0.084932\n",
      "step 6364, loss: 0.084844\n",
      "step 6365, loss: 0.084757\n",
      "step 6366, loss: 0.084669\n",
      "step 6367, loss: 0.084582\n",
      "step 6368, loss: 0.084495\n",
      "step 6369, loss: 0.084407\n",
      "step 6370, loss: 0.084320\n",
      "step 6371, loss: 0.084233\n",
      "step 6372, loss: 0.084146\n",
      "step 6373, loss: 0.084060\n",
      "step 6374, loss: 0.083973\n",
      "step 6375, loss: 0.083886\n",
      "step 6376, loss: 0.083799\n",
      "step 6377, loss: 0.083713\n",
      "step 6378, loss: 0.083627\n",
      "step 6379, loss: 0.083540\n",
      "step 6380, loss: 0.083454\n",
      "step 6381, loss: 0.083368\n",
      "step 6382, loss: 0.083282\n",
      "step 6383, loss: 0.083196\n",
      "step 6384, loss: 0.083110\n",
      "step 6385, loss: 0.083024\n",
      "step 6386, loss: 0.082939\n",
      "step 6387, loss: 0.082853\n",
      "step 6388, loss: 0.082767\n",
      "step 6389, loss: 0.082682\n",
      "step 6390, loss: 0.082597\n",
      "step 6391, loss: 0.082511\n",
      "step 6392, loss: 0.082426\n",
      "step 6393, loss: 0.082341\n",
      "step 6394, loss: 0.082256\n",
      "step 6395, loss: 0.082171\n",
      "step 6396, loss: 0.082086\n",
      "step 6397, loss: 0.082002\n",
      "step 6398, loss: 0.081917\n",
      "step 6399, loss: 0.081833\n",
      "step 6400, loss: 0.081748\n",
      "step 6401, loss: 0.081664\n",
      "step 6402, loss: 0.081579\n",
      "step 6403, loss: 0.081495\n",
      "step 6404, loss: 0.081411\n",
      "step 6405, loss: 0.081327\n",
      "step 6406, loss: 0.081243\n",
      "step 6407, loss: 0.081159\n",
      "step 6408, loss: 0.081075\n",
      "step 6409, loss: 0.080992\n",
      "step 6410, loss: 0.080908\n",
      "step 6411, loss: 0.080825\n",
      "step 6412, loss: 0.080741\n",
      "step 6413, loss: 0.080658\n",
      "step 6414, loss: 0.080575\n",
      "step 6415, loss: 0.080492\n",
      "step 6416, loss: 0.080409\n",
      "step 6417, loss: 0.080326\n",
      "step 6418, loss: 0.080243\n",
      "step 6419, loss: 0.080160\n",
      "step 6420, loss: 0.080077\n",
      "step 6421, loss: 0.079995\n",
      "step 6422, loss: 0.079912\n",
      "step 6423, loss: 0.079829\n",
      "step 6424, loss: 0.079747\n",
      "step 6425, loss: 0.079665\n",
      "step 6426, loss: 0.079583\n",
      "step 6427, loss: 0.079501\n",
      "step 6428, loss: 0.079419\n",
      "step 6429, loss: 0.079337\n",
      "step 6430, loss: 0.079255\n",
      "step 6431, loss: 0.079173\n",
      "step 6432, loss: 0.079091\n",
      "step 6433, loss: 0.079010\n",
      "step 6434, loss: 0.078928\n",
      "step 6435, loss: 0.078847\n",
      "step 6436, loss: 0.078766\n",
      "step 6437, loss: 0.078684\n",
      "step 6438, loss: 0.078603\n",
      "step 6439, loss: 0.078522\n",
      "step 6440, loss: 0.078441\n",
      "step 6441, loss: 0.078360\n",
      "step 6442, loss: 0.078279\n",
      "step 6443, loss: 0.078199\n",
      "step 6444, loss: 0.078118\n",
      "step 6445, loss: 0.078037\n",
      "step 6446, loss: 0.077957\n",
      "step 6447, loss: 0.077877\n",
      "step 6448, loss: 0.077796\n",
      "step 6449, loss: 0.077716\n",
      "step 6450, loss: 0.077636\n",
      "step 6451, loss: 0.077556\n",
      "step 6452, loss: 0.077476\n",
      "step 6453, loss: 0.077396\n",
      "step 6454, loss: 0.077316\n",
      "step 6455, loss: 0.077237\n",
      "step 6456, loss: 0.077157\n",
      "step 6457, loss: 0.077078\n",
      "step 6458, loss: 0.076998\n",
      "step 6459, loss: 0.076919\n",
      "step 6460, loss: 0.076839\n",
      "step 6461, loss: 0.076760\n",
      "step 6462, loss: 0.076681\n",
      "step 6463, loss: 0.076602\n",
      "step 6464, loss: 0.076523\n",
      "step 6465, loss: 0.076444\n",
      "step 6466, loss: 0.076366\n",
      "step 6467, loss: 0.076287\n",
      "step 6468, loss: 0.076208\n",
      "step 6469, loss: 0.076130\n",
      "step 6470, loss: 0.076052\n",
      "step 6471, loss: 0.075973\n",
      "step 6472, loss: 0.075895\n",
      "step 6473, loss: 0.075817\n",
      "step 6474, loss: 0.075739\n",
      "step 6475, loss: 0.075661\n",
      "step 6476, loss: 0.075583\n",
      "step 6477, loss: 0.075505\n",
      "step 6478, loss: 0.075427\n",
      "step 6479, loss: 0.075350\n",
      "step 6480, loss: 0.075272\n",
      "step 6481, loss: 0.075195\n",
      "step 6482, loss: 0.075117\n",
      "step 6483, loss: 0.075040\n",
      "step 6484, loss: 0.074963\n",
      "step 6485, loss: 0.074886\n",
      "step 6486, loss: 0.074809\n",
      "step 6487, loss: 0.074732\n",
      "step 6488, loss: 0.074655\n",
      "step 6489, loss: 0.074578\n",
      "step 6490, loss: 0.074501\n",
      "step 6491, loss: 0.074425\n",
      "step 6492, loss: 0.074348\n",
      "step 6493, loss: 0.074272\n",
      "step 6494, loss: 0.074195\n",
      "step 6495, loss: 0.074119\n",
      "step 6496, loss: 0.074043\n",
      "step 6497, loss: 0.073967\n",
      "step 6498, loss: 0.073891\n",
      "step 6499, loss: 0.073815\n",
      "step 6500, loss: 0.073739\n",
      "step 6501, loss: 0.073663\n",
      "step 6502, loss: 0.073587\n",
      "step 6503, loss: 0.073512\n",
      "step 6504, loss: 0.073436\n",
      "step 6505, loss: 0.073361\n",
      "step 6506, loss: 0.073286\n",
      "step 6507, loss: 0.073210\n",
      "step 6508, loss: 0.073135\n",
      "step 6509, loss: 0.073060\n",
      "step 6510, loss: 0.072985\n",
      "step 6511, loss: 0.072910\n",
      "step 6512, loss: 0.072835\n",
      "step 6513, loss: 0.072760\n",
      "step 6514, loss: 0.072686\n",
      "step 6515, loss: 0.072611\n",
      "step 6516, loss: 0.072537\n",
      "step 6517, loss: 0.072462\n",
      "step 6518, loss: 0.072388\n",
      "step 6519, loss: 0.072313\n",
      "step 6520, loss: 0.072239\n",
      "step 6521, loss: 0.072165\n",
      "step 6522, loss: 0.072091\n",
      "step 6523, loss: 0.072017\n",
      "step 6524, loss: 0.071943\n",
      "step 6525, loss: 0.071870\n",
      "step 6526, loss: 0.071796\n",
      "step 6527, loss: 0.071722\n",
      "step 6528, loss: 0.071649\n",
      "step 6529, loss: 0.071575\n",
      "step 6530, loss: 0.071502\n",
      "step 6531, loss: 0.071429\n",
      "step 6532, loss: 0.071356\n",
      "step 6533, loss: 0.071282\n",
      "step 6534, loss: 0.071209\n",
      "step 6535, loss: 0.071137\n",
      "step 6536, loss: 0.071064\n",
      "step 6537, loss: 0.070991\n",
      "step 6538, loss: 0.070918\n",
      "step 6539, loss: 0.070846\n",
      "step 6540, loss: 0.070773\n",
      "step 6541, loss: 0.070701\n",
      "step 6542, loss: 0.070628\n",
      "step 6543, loss: 0.070556\n",
      "step 6544, loss: 0.070484\n",
      "step 6545, loss: 0.070412\n",
      "step 6546, loss: 0.070340\n",
      "step 6547, loss: 0.070268\n",
      "step 6548, loss: 0.070196\n",
      "step 6549, loss: 0.070124\n",
      "step 6550, loss: 0.070052\n",
      "step 6551, loss: 0.069981\n",
      "step 6552, loss: 0.069909\n",
      "step 6553, loss: 0.069838\n",
      "step 6554, loss: 0.069766\n",
      "step 6555, loss: 0.069695\n",
      "step 6556, loss: 0.069624\n",
      "step 6557, loss: 0.069553\n",
      "step 6558, loss: 0.069482\n",
      "step 6559, loss: 0.069411\n",
      "step 6560, loss: 0.069340\n",
      "step 6561, loss: 0.069269\n",
      "step 6562, loss: 0.069198\n",
      "step 6563, loss: 0.069128\n",
      "step 6564, loss: 0.069057\n",
      "step 6565, loss: 0.068987\n",
      "step 6566, loss: 0.068916\n",
      "step 6567, loss: 0.068846\n",
      "step 6568, loss: 0.068776\n",
      "step 6569, loss: 0.068706\n",
      "step 6570, loss: 0.068636\n",
      "step 6571, loss: 0.068566\n",
      "step 6572, loss: 0.068496\n",
      "step 6573, loss: 0.068426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6574, loss: 0.068356\n",
      "step 6575, loss: 0.068287\n",
      "step 6576, loss: 0.068217\n",
      "step 6577, loss: 0.068148\n",
      "step 6578, loss: 0.068078\n",
      "step 6579, loss: 0.068009\n",
      "step 6580, loss: 0.067940\n",
      "step 6581, loss: 0.067870\n",
      "step 6582, loss: 0.067801\n",
      "step 6583, loss: 0.067732\n",
      "step 6584, loss: 0.067663\n",
      "step 6585, loss: 0.067595\n",
      "step 6586, loss: 0.067526\n",
      "step 6587, loss: 0.067457\n",
      "step 6588, loss: 0.067389\n",
      "step 6589, loss: 0.067320\n",
      "step 6590, loss: 0.067251\n",
      "step 6591, loss: 0.067183\n",
      "step 6592, loss: 0.067115\n",
      "step 6593, loss: 0.067047\n",
      "step 6594, loss: 0.066979\n",
      "step 6595, loss: 0.066911\n",
      "step 6596, loss: 0.066843\n",
      "step 6597, loss: 0.066775\n",
      "step 6598, loss: 0.066707\n",
      "step 6599, loss: 0.066639\n",
      "step 6600, loss: 0.066572\n",
      "step 6601, loss: 0.066504\n",
      "step 6602, loss: 0.066437\n",
      "step 6603, loss: 0.066369\n",
      "step 6604, loss: 0.066302\n",
      "step 6605, loss: 0.066235\n",
      "step 6606, loss: 0.066167\n",
      "step 6607, loss: 0.066100\n",
      "step 6608, loss: 0.066033\n",
      "step 6609, loss: 0.065967\n",
      "step 6610, loss: 0.065900\n",
      "step 6611, loss: 0.065833\n",
      "step 6612, loss: 0.065766\n",
      "step 6613, loss: 0.065700\n",
      "step 6614, loss: 0.065633\n",
      "step 6615, loss: 0.065567\n",
      "step 6616, loss: 0.065500\n",
      "step 6617, loss: 0.065434\n",
      "step 6618, loss: 0.065368\n",
      "step 6619, loss: 0.065302\n",
      "step 6620, loss: 0.065236\n",
      "step 6621, loss: 0.065170\n",
      "step 6622, loss: 0.065104\n",
      "step 6623, loss: 0.065038\n",
      "step 6624, loss: 0.064972\n",
      "step 6625, loss: 0.064907\n",
      "step 6626, loss: 0.064841\n",
      "step 6627, loss: 0.064776\n",
      "step 6628, loss: 0.064710\n",
      "step 6629, loss: 0.064645\n",
      "step 6630, loss: 0.064580\n",
      "step 6631, loss: 0.064515\n",
      "step 6632, loss: 0.064450\n",
      "step 6633, loss: 0.064385\n",
      "step 6634, loss: 0.064320\n",
      "step 6635, loss: 0.064255\n",
      "step 6636, loss: 0.064190\n",
      "step 6637, loss: 0.064125\n",
      "step 6638, loss: 0.064061\n",
      "step 6639, loss: 0.063996\n",
      "step 6640, loss: 0.063932\n",
      "step 6641, loss: 0.063867\n",
      "step 6642, loss: 0.063803\n",
      "step 6643, loss: 0.063739\n",
      "step 6644, loss: 0.063675\n",
      "step 6645, loss: 0.063611\n",
      "step 6646, loss: 0.063547\n",
      "step 6647, loss: 0.063483\n",
      "step 6648, loss: 0.063419\n",
      "step 6649, loss: 0.063355\n",
      "step 6650, loss: 0.063292\n",
      "step 6651, loss: 0.063228\n",
      "step 6652, loss: 0.063164\n",
      "step 6653, loss: 0.063101\n",
      "step 6654, loss: 0.063038\n",
      "step 6655, loss: 0.062974\n",
      "step 6656, loss: 0.062911\n",
      "step 6657, loss: 0.062848\n",
      "step 6658, loss: 0.062785\n",
      "step 6659, loss: 0.062722\n",
      "step 6660, loss: 0.062659\n",
      "step 6661, loss: 0.062596\n",
      "step 6662, loss: 0.062533\n",
      "step 6663, loss: 0.062471\n",
      "step 6664, loss: 0.062408\n",
      "step 6665, loss: 0.062346\n",
      "step 6666, loss: 0.062283\n",
      "step 6667, loss: 0.062221\n",
      "step 6668, loss: 0.062159\n",
      "step 6669, loss: 0.062097\n",
      "step 6670, loss: 0.062034\n",
      "step 6671, loss: 0.061972\n",
      "step 6672, loss: 0.061910\n",
      "step 6673, loss: 0.061849\n",
      "step 6674, loss: 0.061787\n",
      "step 6675, loss: 0.061725\n",
      "step 6676, loss: 0.061663\n",
      "step 6677, loss: 0.061602\n",
      "step 6678, loss: 0.061540\n",
      "step 6679, loss: 0.061479\n",
      "step 6680, loss: 0.061418\n",
      "step 6681, loss: 0.061356\n",
      "step 6682, loss: 0.061295\n",
      "step 6683, loss: 0.061234\n",
      "step 6684, loss: 0.061173\n",
      "step 6685, loss: 0.061112\n",
      "step 6686, loss: 0.061051\n",
      "step 6687, loss: 0.060990\n",
      "step 6688, loss: 0.060929\n",
      "step 6689, loss: 0.060869\n",
      "step 6690, loss: 0.060808\n",
      "step 6691, loss: 0.060748\n",
      "step 6692, loss: 0.060687\n",
      "step 6693, loss: 0.060627\n",
      "step 6694, loss: 0.060567\n",
      "step 6695, loss: 0.060506\n",
      "step 6696, loss: 0.060446\n",
      "step 6697, loss: 0.060386\n",
      "step 6698, loss: 0.060326\n",
      "step 6699, loss: 0.060266\n",
      "step 6700, loss: 0.060207\n",
      "step 6701, loss: 0.060147\n",
      "step 6702, loss: 0.060087\n",
      "step 6703, loss: 0.060028\n",
      "step 6704, loss: 0.059968\n",
      "step 6705, loss: 0.059909\n",
      "step 6706, loss: 0.059849\n",
      "step 6707, loss: 0.059790\n",
      "step 6708, loss: 0.059731\n",
      "step 6709, loss: 0.059672\n",
      "step 6710, loss: 0.059613\n",
      "step 6711, loss: 0.059554\n",
      "step 6712, loss: 0.059495\n",
      "step 6713, loss: 0.059436\n",
      "step 6714, loss: 0.059377\n",
      "step 6715, loss: 0.059318\n",
      "step 6716, loss: 0.059260\n",
      "step 6717, loss: 0.059201\n",
      "step 6718, loss: 0.059143\n",
      "step 6719, loss: 0.059084\n",
      "step 6720, loss: 0.059026\n",
      "step 6721, loss: 0.058968\n",
      "step 6722, loss: 0.058910\n",
      "step 6723, loss: 0.058852\n",
      "step 6724, loss: 0.058793\n",
      "step 6725, loss: 0.058736\n",
      "step 6726, loss: 0.058678\n",
      "step 6727, loss: 0.058620\n",
      "step 6728, loss: 0.058562\n",
      "step 6729, loss: 0.058504\n",
      "step 6730, loss: 0.058447\n",
      "step 6731, loss: 0.058390\n",
      "step 6732, loss: 0.058332\n",
      "step 6733, loss: 0.058275\n",
      "step 6734, loss: 0.058217\n",
      "step 6735, loss: 0.058160\n",
      "step 6736, loss: 0.058103\n",
      "step 6737, loss: 0.058046\n",
      "step 6738, loss: 0.057989\n",
      "step 6739, loss: 0.057932\n",
      "step 6740, loss: 0.057875\n",
      "step 6741, loss: 0.057819\n",
      "step 6742, loss: 0.057762\n",
      "step 6743, loss: 0.057705\n",
      "step 6744, loss: 0.057649\n",
      "step 6745, loss: 0.057592\n",
      "step 6746, loss: 0.057536\n",
      "step 6747, loss: 0.057480\n",
      "step 6748, loss: 0.057423\n",
      "step 6749, loss: 0.057367\n",
      "step 6750, loss: 0.057311\n",
      "step 6751, loss: 0.057255\n",
      "step 6752, loss: 0.057199\n",
      "step 6753, loss: 0.057143\n",
      "step 6754, loss: 0.057087\n",
      "step 6755, loss: 0.057032\n",
      "step 6756, loss: 0.056976\n",
      "step 6757, loss: 0.056921\n",
      "step 6758, loss: 0.056865\n",
      "step 6759, loss: 0.056810\n",
      "step 6760, loss: 0.056754\n",
      "step 6761, loss: 0.056699\n",
      "step 6762, loss: 0.056644\n",
      "step 6763, loss: 0.056589\n",
      "step 6764, loss: 0.056534\n",
      "step 6765, loss: 0.056479\n",
      "step 6766, loss: 0.056424\n",
      "step 6767, loss: 0.056369\n",
      "step 6768, loss: 0.056314\n",
      "step 6769, loss: 0.056259\n",
      "step 6770, loss: 0.056205\n",
      "step 6771, loss: 0.056150\n",
      "step 6772, loss: 0.056096\n",
      "step 6773, loss: 0.056041\n",
      "step 6774, loss: 0.055987\n",
      "step 6775, loss: 0.055933\n",
      "step 6776, loss: 0.055878\n",
      "step 6777, loss: 0.055824\n",
      "step 6778, loss: 0.055770\n",
      "step 6779, loss: 0.055716\n",
      "step 6780, loss: 0.055662\n",
      "step 6781, loss: 0.055609\n",
      "step 6782, loss: 0.055555\n",
      "step 6783, loss: 0.055501\n",
      "step 6784, loss: 0.055447\n",
      "step 6785, loss: 0.055394\n",
      "step 6786, loss: 0.055340\n",
      "step 6787, loss: 0.055287\n",
      "step 6788, loss: 0.055234\n",
      "step 6789, loss: 0.055180\n",
      "step 6790, loss: 0.055127\n",
      "step 6791, loss: 0.055074\n",
      "step 6792, loss: 0.055021\n",
      "step 6793, loss: 0.054968\n",
      "step 6794, loss: 0.054915\n",
      "step 6795, loss: 0.054862\n",
      "step 6796, loss: 0.054810\n",
      "step 6797, loss: 0.054757\n",
      "step 6798, loss: 0.054704\n",
      "step 6799, loss: 0.054652\n",
      "step 6800, loss: 0.054599\n",
      "step 6801, loss: 0.054547\n",
      "step 6802, loss: 0.054495\n",
      "step 6803, loss: 0.054442\n",
      "step 6804, loss: 0.054390\n",
      "step 6805, loss: 0.054338\n",
      "step 6806, loss: 0.054286\n",
      "step 6807, loss: 0.054234\n",
      "step 6808, loss: 0.054182\n",
      "step 6809, loss: 0.054130\n",
      "step 6810, loss: 0.054078\n",
      "step 6811, loss: 0.054027\n",
      "step 6812, loss: 0.053975\n",
      "step 6813, loss: 0.053923\n",
      "step 6814, loss: 0.053872\n",
      "step 6815, loss: 0.053821\n",
      "step 6816, loss: 0.053769\n",
      "step 6817, loss: 0.053718\n",
      "step 6818, loss: 0.053667\n",
      "step 6819, loss: 0.053616\n",
      "step 6820, loss: 0.053565\n",
      "step 6821, loss: 0.053514\n",
      "step 6822, loss: 0.053463\n",
      "step 6823, loss: 0.053412\n",
      "step 6824, loss: 0.053361\n",
      "step 6825, loss: 0.053310\n",
      "step 6826, loss: 0.053260\n",
      "step 6827, loss: 0.053209\n",
      "step 6828, loss: 0.053158\n",
      "step 6829, loss: 0.053108\n",
      "step 6830, loss: 0.053058\n",
      "step 6831, loss: 0.053007\n",
      "step 6832, loss: 0.052957\n",
      "step 6833, loss: 0.052907\n",
      "step 6834, loss: 0.052857\n",
      "step 6835, loss: 0.052807\n",
      "step 6836, loss: 0.052757\n",
      "step 6837, loss: 0.052707\n",
      "step 6838, loss: 0.052657\n",
      "step 6839, loss: 0.052607\n",
      "step 6840, loss: 0.052558\n",
      "step 6841, loss: 0.052508\n",
      "step 6842, loss: 0.052459\n",
      "step 6843, loss: 0.052409\n",
      "step 6844, loss: 0.052360\n",
      "step 6845, loss: 0.052310\n",
      "step 6846, loss: 0.052261\n",
      "step 6847, loss: 0.052212\n",
      "step 6848, loss: 0.052163\n",
      "step 6849, loss: 0.052114\n",
      "step 6850, loss: 0.052065\n",
      "step 6851, loss: 0.052016\n",
      "step 6852, loss: 0.051967\n",
      "step 6853, loss: 0.051918\n",
      "step 6854, loss: 0.051870\n",
      "step 6855, loss: 0.051821\n",
      "step 6856, loss: 0.051772\n",
      "step 6857, loss: 0.051724\n",
      "step 6858, loss: 0.051675\n",
      "step 6859, loss: 0.051627\n",
      "step 6860, loss: 0.051579\n",
      "step 6861, loss: 0.051530\n",
      "step 6862, loss: 0.051482\n",
      "step 6863, loss: 0.051434\n",
      "step 6864, loss: 0.051386\n",
      "step 6865, loss: 0.051338\n",
      "step 6866, loss: 0.051290\n",
      "step 6867, loss: 0.051242\n",
      "step 6868, loss: 0.051195\n",
      "step 6869, loss: 0.051147\n",
      "step 6870, loss: 0.051099\n",
      "step 6871, loss: 0.051052\n",
      "step 6872, loss: 0.051004\n",
      "step 6873, loss: 0.050957\n",
      "step 6874, loss: 0.050909\n",
      "step 6875, loss: 0.050862\n",
      "step 6876, loss: 0.050815\n",
      "step 6877, loss: 0.050768\n",
      "step 6878, loss: 0.050721\n",
      "step 6879, loss: 0.050674\n",
      "step 6880, loss: 0.050627\n",
      "step 6881, loss: 0.050580\n",
      "step 6882, loss: 0.050533\n",
      "step 6883, loss: 0.050486\n",
      "step 6884, loss: 0.050439\n",
      "step 6885, loss: 0.050393\n",
      "step 6886, loss: 0.050346\n",
      "step 6887, loss: 0.050300\n",
      "step 6888, loss: 0.050253\n",
      "step 6889, loss: 0.050207\n",
      "step 6890, loss: 0.050161\n",
      "step 6891, loss: 0.050114\n",
      "step 6892, loss: 0.050068\n",
      "step 6893, loss: 0.050022\n",
      "step 6894, loss: 0.049976\n",
      "step 6895, loss: 0.049930\n",
      "step 6896, loss: 0.049884\n",
      "step 6897, loss: 0.049838\n",
      "step 6898, loss: 0.049793\n",
      "step 6899, loss: 0.049747\n",
      "step 6900, loss: 0.049701\n",
      "step 6901, loss: 0.049656\n",
      "step 6902, loss: 0.049610\n",
      "step 6903, loss: 0.049565\n",
      "step 6904, loss: 0.049519\n",
      "step 6905, loss: 0.049474\n",
      "step 6906, loss: 0.049429\n",
      "step 6907, loss: 0.049384\n",
      "step 6908, loss: 0.049339\n",
      "step 6909, loss: 0.049293\n",
      "step 6910, loss: 0.049249\n",
      "step 6911, loss: 0.049204\n",
      "step 6912, loss: 0.049159\n",
      "step 6913, loss: 0.049114\n",
      "step 6914, loss: 0.049069\n",
      "step 6915, loss: 0.049025\n",
      "step 6916, loss: 0.048980\n",
      "step 6917, loss: 0.048935\n",
      "step 6918, loss: 0.048891\n",
      "step 6919, loss: 0.048847\n",
      "step 6920, loss: 0.048802\n",
      "step 6921, loss: 0.048758\n",
      "step 6922, loss: 0.048714\n",
      "step 6923, loss: 0.048670\n",
      "step 6924, loss: 0.048626\n",
      "step 6925, loss: 0.048582\n",
      "step 6926, loss: 0.048538\n",
      "step 6927, loss: 0.048494\n",
      "step 6928, loss: 0.048450\n",
      "step 6929, loss: 0.048406\n",
      "step 6930, loss: 0.048363\n",
      "step 6931, loss: 0.048319\n",
      "step 6932, loss: 0.048275\n",
      "step 6933, loss: 0.048232\n",
      "step 6934, loss: 0.048188\n",
      "step 6935, loss: 0.048145\n",
      "step 6936, loss: 0.048102\n",
      "step 6937, loss: 0.048059\n",
      "step 6938, loss: 0.048015\n",
      "step 6939, loss: 0.047972\n",
      "step 6940, loss: 0.047929\n",
      "step 6941, loss: 0.047886\n",
      "step 6942, loss: 0.047843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6943, loss: 0.047800\n",
      "step 6944, loss: 0.047758\n",
      "step 6945, loss: 0.047715\n",
      "step 6946, loss: 0.047672\n",
      "step 6947, loss: 0.047630\n",
      "step 6948, loss: 0.047587\n",
      "step 6949, loss: 0.047545\n",
      "step 6950, loss: 0.047502\n",
      "step 6951, loss: 0.047460\n",
      "step 6952, loss: 0.047418\n",
      "step 6953, loss: 0.047375\n",
      "step 6954, loss: 0.047333\n",
      "step 6955, loss: 0.047291\n",
      "step 6956, loss: 0.047249\n",
      "step 6957, loss: 0.047207\n",
      "step 6958, loss: 0.047165\n",
      "step 6959, loss: 0.047123\n",
      "step 6960, loss: 0.047082\n",
      "step 6961, loss: 0.047040\n",
      "step 6962, loss: 0.046998\n",
      "step 6963, loss: 0.046957\n",
      "step 6964, loss: 0.046915\n",
      "step 6965, loss: 0.046874\n",
      "step 6966, loss: 0.046832\n",
      "step 6967, loss: 0.046791\n",
      "step 6968, loss: 0.046750\n",
      "step 6969, loss: 0.046708\n",
      "step 6970, loss: 0.046667\n",
      "step 6971, loss: 0.046626\n",
      "step 6972, loss: 0.046585\n",
      "step 6973, loss: 0.046544\n",
      "step 6974, loss: 0.046503\n",
      "step 6975, loss: 0.046462\n",
      "step 6976, loss: 0.046422\n",
      "step 6977, loss: 0.046381\n",
      "step 6978, loss: 0.046340\n",
      "step 6979, loss: 0.046300\n",
      "step 6980, loss: 0.046259\n",
      "step 6981, loss: 0.046219\n",
      "step 6982, loss: 0.046178\n",
      "step 6983, loss: 0.046138\n",
      "step 6984, loss: 0.046097\n",
      "step 6985, loss: 0.046057\n",
      "step 6986, loss: 0.046017\n",
      "step 6987, loss: 0.045977\n",
      "step 6988, loss: 0.045937\n",
      "step 6989, loss: 0.045897\n",
      "step 6990, loss: 0.045857\n",
      "step 6991, loss: 0.045817\n",
      "step 6992, loss: 0.045777\n",
      "step 6993, loss: 0.045738\n",
      "step 6994, loss: 0.045698\n",
      "step 6995, loss: 0.045658\n",
      "step 6996, loss: 0.045619\n",
      "step 6997, loss: 0.045579\n",
      "step 6998, loss: 0.045540\n",
      "step 6999, loss: 0.045500\n",
      "step 7000, loss: 0.045461\n",
      "step 7001, loss: 0.045422\n",
      "step 7002, loss: 0.045383\n",
      "step 7003, loss: 0.045343\n",
      "step 7004, loss: 0.045304\n",
      "step 7005, loss: 0.045265\n",
      "step 7006, loss: 0.045226\n",
      "step 7007, loss: 0.045187\n",
      "step 7008, loss: 0.045149\n",
      "step 7009, loss: 0.045110\n",
      "step 7010, loss: 0.045071\n",
      "step 7011, loss: 0.045032\n",
      "step 7012, loss: 0.044994\n",
      "step 7013, loss: 0.044955\n",
      "step 7014, loss: 0.044917\n",
      "step 7015, loss: 0.044878\n",
      "step 7016, loss: 0.044840\n",
      "step 7017, loss: 0.044802\n",
      "step 7018, loss: 0.044764\n",
      "step 7019, loss: 0.044725\n",
      "step 7020, loss: 0.044687\n",
      "step 7021, loss: 0.044649\n",
      "step 7022, loss: 0.044611\n",
      "step 7023, loss: 0.044573\n",
      "step 7024, loss: 0.044535\n",
      "step 7025, loss: 0.044497\n",
      "step 7026, loss: 0.044460\n",
      "step 7027, loss: 0.044422\n",
      "step 7028, loss: 0.044384\n",
      "step 7029, loss: 0.044347\n",
      "step 7030, loss: 0.044309\n",
      "step 7031, loss: 0.044272\n",
      "step 7032, loss: 0.044234\n",
      "step 7033, loss: 0.044197\n",
      "step 7034, loss: 0.044160\n",
      "step 7035, loss: 0.044122\n",
      "step 7036, loss: 0.044085\n",
      "step 7037, loss: 0.044048\n",
      "step 7038, loss: 0.044011\n",
      "step 7039, loss: 0.043974\n",
      "step 7040, loss: 0.043937\n",
      "step 7041, loss: 0.043900\n",
      "step 7042, loss: 0.043863\n",
      "step 7043, loss: 0.043827\n",
      "step 7044, loss: 0.043790\n",
      "step 7045, loss: 0.043753\n",
      "step 7046, loss: 0.043717\n",
      "step 7047, loss: 0.043680\n",
      "step 7048, loss: 0.043644\n",
      "step 7049, loss: 0.043607\n",
      "step 7050, loss: 0.043571\n",
      "step 7051, loss: 0.043534\n",
      "step 7052, loss: 0.043498\n",
      "step 7053, loss: 0.043462\n",
      "step 7054, loss: 0.043426\n",
      "step 7055, loss: 0.043390\n",
      "step 7056, loss: 0.043354\n",
      "step 7057, loss: 0.043318\n",
      "step 7058, loss: 0.043282\n",
      "step 7059, loss: 0.043246\n",
      "step 7060, loss: 0.043210\n",
      "step 7061, loss: 0.043175\n",
      "step 7062, loss: 0.043139\n",
      "step 7063, loss: 0.043103\n",
      "step 7064, loss: 0.043068\n",
      "step 7065, loss: 0.043032\n",
      "step 7066, loss: 0.042997\n",
      "step 7067, loss: 0.042961\n",
      "step 7068, loss: 0.042926\n",
      "step 7069, loss: 0.042891\n",
      "step 7070, loss: 0.042855\n",
      "step 7071, loss: 0.042820\n",
      "step 7072, loss: 0.042785\n",
      "step 7073, loss: 0.042750\n",
      "step 7074, loss: 0.042715\n",
      "step 7075, loss: 0.042680\n",
      "step 7076, loss: 0.042645\n",
      "step 7077, loss: 0.042610\n",
      "step 7078, loss: 0.042576\n",
      "step 7079, loss: 0.042541\n",
      "step 7080, loss: 0.042506\n",
      "step 7081, loss: 0.042472\n",
      "step 7082, loss: 0.042437\n",
      "step 7083, loss: 0.042403\n",
      "step 7084, loss: 0.042368\n",
      "step 7085, loss: 0.042334\n",
      "step 7086, loss: 0.042299\n",
      "step 7087, loss: 0.042265\n",
      "step 7088, loss: 0.042231\n",
      "step 7089, loss: 0.042197\n",
      "step 7090, loss: 0.042163\n",
      "step 7091, loss: 0.042129\n",
      "step 7092, loss: 0.042095\n",
      "step 7093, loss: 0.042061\n",
      "step 7094, loss: 0.042027\n",
      "step 7095, loss: 0.041993\n",
      "step 7096, loss: 0.041959\n",
      "step 7097, loss: 0.041926\n",
      "step 7098, loss: 0.041892\n",
      "step 7099, loss: 0.041858\n",
      "step 7100, loss: 0.041825\n",
      "step 7101, loss: 0.041791\n",
      "step 7102, loss: 0.041758\n",
      "step 7103, loss: 0.041724\n",
      "step 7104, loss: 0.041691\n",
      "step 7105, loss: 0.041658\n",
      "step 7106, loss: 0.041625\n",
      "step 7107, loss: 0.041591\n",
      "step 7108, loss: 0.041558\n",
      "step 7109, loss: 0.041525\n",
      "step 7110, loss: 0.041492\n",
      "step 7111, loss: 0.041459\n",
      "step 7112, loss: 0.041426\n",
      "step 7113, loss: 0.041393\n",
      "step 7114, loss: 0.041361\n",
      "step 7115, loss: 0.041328\n",
      "step 7116, loss: 0.041295\n",
      "step 7117, loss: 0.041263\n",
      "step 7118, loss: 0.041230\n",
      "step 7119, loss: 0.041198\n",
      "step 7120, loss: 0.041165\n",
      "step 7121, loss: 0.041133\n",
      "step 7122, loss: 0.041100\n",
      "step 7123, loss: 0.041068\n",
      "step 7124, loss: 0.041036\n",
      "step 7125, loss: 0.041004\n",
      "step 7126, loss: 0.040971\n",
      "step 7127, loss: 0.040939\n",
      "step 7128, loss: 0.040907\n",
      "step 7129, loss: 0.040875\n",
      "step 7130, loss: 0.040843\n",
      "step 7131, loss: 0.040812\n",
      "step 7132, loss: 0.040780\n",
      "step 7133, loss: 0.040748\n",
      "step 7134, loss: 0.040716\n",
      "step 7135, loss: 0.040684\n",
      "step 7136, loss: 0.040653\n",
      "step 7137, loss: 0.040621\n",
      "step 7138, loss: 0.040590\n",
      "step 7139, loss: 0.040558\n",
      "step 7140, loss: 0.040527\n",
      "step 7141, loss: 0.040496\n",
      "step 7142, loss: 0.040464\n",
      "step 7143, loss: 0.040433\n",
      "step 7144, loss: 0.040402\n",
      "step 7145, loss: 0.040371\n",
      "step 7146, loss: 0.040340\n",
      "step 7147, loss: 0.040309\n",
      "step 7148, loss: 0.040278\n",
      "step 7149, loss: 0.040247\n",
      "step 7150, loss: 0.040216\n",
      "step 7151, loss: 0.040185\n",
      "step 7152, loss: 0.040154\n",
      "step 7153, loss: 0.040124\n",
      "step 7154, loss: 0.040093\n",
      "step 7155, loss: 0.040062\n",
      "step 7156, loss: 0.040032\n",
      "step 7157, loss: 0.040001\n",
      "step 7158, loss: 0.039971\n",
      "step 7159, loss: 0.039940\n",
      "step 7160, loss: 0.039910\n",
      "step 7161, loss: 0.039880\n",
      "step 7162, loss: 0.039849\n",
      "step 7163, loss: 0.039819\n",
      "step 7164, loss: 0.039789\n",
      "step 7165, loss: 0.039759\n",
      "step 7166, loss: 0.039729\n",
      "step 7167, loss: 0.039699\n",
      "step 7168, loss: 0.039669\n",
      "step 7169, loss: 0.039639\n",
      "step 7170, loss: 0.039609\n",
      "step 7171, loss: 0.039579\n",
      "step 7172, loss: 0.039550\n",
      "step 7173, loss: 0.039520\n",
      "step 7174, loss: 0.039490\n",
      "step 7175, loss: 0.039461\n",
      "step 7176, loss: 0.039431\n",
      "step 7177, loss: 0.039402\n",
      "step 7178, loss: 0.039372\n",
      "step 7179, loss: 0.039343\n",
      "step 7180, loss: 0.039314\n",
      "step 7181, loss: 0.039284\n",
      "step 7182, loss: 0.039255\n",
      "step 7183, loss: 0.039226\n",
      "step 7184, loss: 0.039197\n",
      "step 7185, loss: 0.039168\n",
      "step 7186, loss: 0.039139\n",
      "step 7187, loss: 0.039110\n",
      "step 7188, loss: 0.039081\n",
      "step 7189, loss: 0.039052\n",
      "step 7190, loss: 0.039023\n",
      "step 7191, loss: 0.038994\n",
      "step 7192, loss: 0.038965\n",
      "step 7193, loss: 0.038937\n",
      "step 7194, loss: 0.038908\n",
      "step 7195, loss: 0.038880\n",
      "step 7196, loss: 0.038851\n",
      "step 7197, loss: 0.038823\n",
      "step 7198, loss: 0.038794\n",
      "step 7199, loss: 0.038766\n",
      "step 7200, loss: 0.038737\n",
      "step 7201, loss: 0.038709\n",
      "step 7202, loss: 0.038681\n",
      "step 7203, loss: 0.038653\n",
      "step 7204, loss: 0.038624\n",
      "step 7205, loss: 0.038596\n",
      "step 7206, loss: 0.038568\n",
      "step 7207, loss: 0.038540\n",
      "step 7208, loss: 0.038512\n",
      "step 7209, loss: 0.038484\n",
      "step 7210, loss: 0.038457\n",
      "step 7211, loss: 0.038429\n",
      "step 7212, loss: 0.038401\n",
      "step 7213, loss: 0.038373\n",
      "step 7214, loss: 0.038346\n",
      "step 7215, loss: 0.038318\n",
      "step 7216, loss: 0.038291\n",
      "step 7217, loss: 0.038263\n",
      "step 7218, loss: 0.038236\n",
      "step 7219, loss: 0.038208\n",
      "step 7220, loss: 0.038181\n",
      "step 7221, loss: 0.038153\n",
      "step 7222, loss: 0.038126\n",
      "step 7223, loss: 0.038099\n",
      "step 7224, loss: 0.038072\n",
      "step 7225, loss: 0.038045\n",
      "step 7226, loss: 0.038018\n",
      "step 7227, loss: 0.037991\n",
      "step 7228, loss: 0.037964\n",
      "step 7229, loss: 0.037937\n",
      "step 7230, loss: 0.037910\n",
      "step 7231, loss: 0.037883\n",
      "step 7232, loss: 0.037856\n",
      "step 7233, loss: 0.037830\n",
      "step 7234, loss: 0.037803\n",
      "step 7235, loss: 0.037776\n",
      "step 7236, loss: 0.037750\n",
      "step 7237, loss: 0.037723\n",
      "step 7238, loss: 0.037697\n",
      "step 7239, loss: 0.037670\n",
      "step 7240, loss: 0.037644\n",
      "step 7241, loss: 0.037617\n",
      "step 7242, loss: 0.037591\n",
      "step 7243, loss: 0.037565\n",
      "step 7244, loss: 0.037539\n",
      "step 7245, loss: 0.037512\n",
      "step 7246, loss: 0.037486\n",
      "step 7247, loss: 0.037460\n",
      "step 7248, loss: 0.037434\n",
      "step 7249, loss: 0.037408\n",
      "step 7250, loss: 0.037382\n",
      "step 7251, loss: 0.037356\n",
      "step 7252, loss: 0.037330\n",
      "step 7253, loss: 0.037305\n",
      "step 7254, loss: 0.037279\n",
      "step 7255, loss: 0.037253\n",
      "step 7256, loss: 0.037227\n",
      "step 7257, loss: 0.037202\n",
      "step 7258, loss: 0.037176\n",
      "step 7259, loss: 0.037151\n",
      "step 7260, loss: 0.037125\n",
      "step 7261, loss: 0.037100\n",
      "step 7262, loss: 0.037074\n",
      "step 7263, loss: 0.037049\n",
      "step 7264, loss: 0.037024\n",
      "step 7265, loss: 0.036999\n",
      "step 7266, loss: 0.036973\n",
      "step 7267, loss: 0.036948\n",
      "step 7268, loss: 0.036923\n",
      "step 7269, loss: 0.036898\n",
      "step 7270, loss: 0.036873\n",
      "step 7271, loss: 0.036848\n",
      "step 7272, loss: 0.036823\n",
      "step 7273, loss: 0.036798\n",
      "step 7274, loss: 0.036774\n",
      "step 7275, loss: 0.036749\n",
      "step 7276, loss: 0.036724\n",
      "step 7277, loss: 0.036699\n",
      "step 7278, loss: 0.036675\n",
      "step 7279, loss: 0.036650\n",
      "step 7280, loss: 0.036625\n",
      "step 7281, loss: 0.036601\n",
      "step 7282, loss: 0.036576\n",
      "step 7283, loss: 0.036552\n",
      "step 7284, loss: 0.036528\n",
      "step 7285, loss: 0.036503\n",
      "step 7286, loss: 0.036479\n",
      "step 7287, loss: 0.036455\n",
      "step 7288, loss: 0.036431\n",
      "step 7289, loss: 0.036406\n",
      "step 7290, loss: 0.036382\n",
      "step 7291, loss: 0.036358\n",
      "step 7292, loss: 0.036334\n",
      "step 7293, loss: 0.036310\n",
      "step 7294, loss: 0.036286\n",
      "step 7295, loss: 0.036262\n",
      "step 7296, loss: 0.036238\n",
      "step 7297, loss: 0.036215\n",
      "step 7298, loss: 0.036191\n",
      "step 7299, loss: 0.036167\n",
      "step 7300, loss: 0.036144\n",
      "step 7301, loss: 0.036120\n",
      "step 7302, loss: 0.036096\n",
      "step 7303, loss: 0.036073\n",
      "step 7304, loss: 0.036049\n",
      "step 7305, loss: 0.036026\n",
      "step 7306, loss: 0.036002\n",
      "step 7307, loss: 0.035979\n",
      "step 7308, loss: 0.035956\n",
      "step 7309, loss: 0.035932\n",
      "step 7310, loss: 0.035909\n",
      "step 7311, loss: 0.035886\n",
      "step 7312, loss: 0.035863\n",
      "step 7313, loss: 0.035840\n",
      "step 7314, loss: 0.035817\n",
      "step 7315, loss: 0.035794\n",
      "step 7316, loss: 0.035771\n",
      "step 7317, loss: 0.035748\n",
      "step 7318, loss: 0.035725\n",
      "step 7319, loss: 0.035702\n",
      "step 7320, loss: 0.035679\n",
      "step 7321, loss: 0.035656\n",
      "step 7322, loss: 0.035634\n",
      "step 7323, loss: 0.035611\n",
      "step 7324, loss: 0.035588\n",
      "step 7325, loss: 0.035566\n",
      "step 7326, loss: 0.035543\n",
      "step 7327, loss: 0.035521\n",
      "step 7328, loss: 0.035498\n",
      "step 7329, loss: 0.035476\n",
      "step 7330, loss: 0.035453\n",
      "step 7331, loss: 0.035431\n",
      "step 7332, loss: 0.035409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7333, loss: 0.035386\n",
      "step 7334, loss: 0.035364\n",
      "step 7335, loss: 0.035342\n",
      "step 7336, loss: 0.035320\n",
      "step 7337, loss: 0.035298\n",
      "step 7338, loss: 0.035276\n",
      "step 7339, loss: 0.035254\n",
      "step 7340, loss: 0.035232\n",
      "step 7341, loss: 0.035210\n",
      "step 7342, loss: 0.035188\n",
      "step 7343, loss: 0.035166\n",
      "step 7344, loss: 0.035144\n",
      "step 7345, loss: 0.035123\n",
      "step 7346, loss: 0.035101\n",
      "step 7347, loss: 0.035079\n",
      "step 7348, loss: 0.035058\n",
      "step 7349, loss: 0.035036\n",
      "step 7350, loss: 0.035015\n",
      "step 7351, loss: 0.034993\n",
      "step 7352, loss: 0.034972\n",
      "step 7353, loss: 0.034950\n",
      "step 7354, loss: 0.034929\n",
      "step 7355, loss: 0.034907\n",
      "step 7356, loss: 0.034886\n",
      "step 7357, loss: 0.034865\n",
      "step 7358, loss: 0.034844\n",
      "step 7359, loss: 0.034822\n",
      "step 7360, loss: 0.034801\n",
      "step 7361, loss: 0.034780\n",
      "step 7362, loss: 0.034759\n",
      "step 7363, loss: 0.034738\n",
      "step 7364, loss: 0.034717\n",
      "step 7365, loss: 0.034696\n",
      "step 7366, loss: 0.034675\n",
      "step 7367, loss: 0.034654\n",
      "step 7368, loss: 0.034634\n",
      "step 7369, loss: 0.034613\n",
      "step 7370, loss: 0.034592\n",
      "step 7371, loss: 0.034571\n",
      "step 7372, loss: 0.034551\n",
      "step 7373, loss: 0.034530\n",
      "step 7374, loss: 0.034510\n",
      "step 7375, loss: 0.034489\n",
      "step 7376, loss: 0.034469\n",
      "step 7377, loss: 0.034448\n",
      "step 7378, loss: 0.034428\n",
      "step 7379, loss: 0.034407\n",
      "step 7380, loss: 0.034387\n",
      "step 7381, loss: 0.034367\n",
      "step 7382, loss: 0.034346\n",
      "step 7383, loss: 0.034326\n",
      "step 7384, loss: 0.034306\n",
      "step 7385, loss: 0.034286\n",
      "step 7386, loss: 0.034266\n",
      "step 7387, loss: 0.034246\n",
      "step 7388, loss: 0.034226\n",
      "step 7389, loss: 0.034206\n",
      "step 7390, loss: 0.034186\n",
      "step 7391, loss: 0.034166\n",
      "step 7392, loss: 0.034146\n",
      "step 7393, loss: 0.034126\n",
      "step 7394, loss: 0.034106\n",
      "step 7395, loss: 0.034087\n",
      "step 7396, loss: 0.034067\n",
      "step 7397, loss: 0.034047\n",
      "step 7398, loss: 0.034028\n",
      "step 7399, loss: 0.034008\n",
      "step 7400, loss: 0.033988\n",
      "step 7401, loss: 0.033969\n",
      "step 7402, loss: 0.033949\n",
      "step 7403, loss: 0.033930\n",
      "step 7404, loss: 0.033911\n",
      "step 7405, loss: 0.033891\n",
      "step 7406, loss: 0.033872\n",
      "step 7407, loss: 0.033853\n",
      "step 7408, loss: 0.033833\n",
      "step 7409, loss: 0.033814\n",
      "step 7410, loss: 0.033795\n",
      "step 7411, loss: 0.033776\n",
      "step 7412, loss: 0.033757\n",
      "step 7413, loss: 0.033738\n",
      "step 7414, loss: 0.033719\n",
      "step 7415, loss: 0.033700\n",
      "step 7416, loss: 0.033681\n",
      "step 7417, loss: 0.033662\n",
      "step 7418, loss: 0.033643\n",
      "step 7419, loss: 0.033624\n",
      "step 7420, loss: 0.033605\n",
      "step 7421, loss: 0.033587\n",
      "step 7422, loss: 0.033568\n",
      "step 7423, loss: 0.033549\n",
      "step 7424, loss: 0.033531\n",
      "step 7425, loss: 0.033512\n",
      "step 7426, loss: 0.033493\n",
      "step 7427, loss: 0.033475\n",
      "step 7428, loss: 0.033456\n",
      "step 7429, loss: 0.033438\n",
      "step 7430, loss: 0.033419\n",
      "step 7431, loss: 0.033401\n",
      "step 7432, loss: 0.033383\n",
      "step 7433, loss: 0.033364\n",
      "step 7434, loss: 0.033346\n",
      "step 7435, loss: 0.033328\n",
      "step 7436, loss: 0.033310\n",
      "step 7437, loss: 0.033292\n",
      "step 7438, loss: 0.033273\n",
      "step 7439, loss: 0.033255\n",
      "step 7440, loss: 0.033237\n",
      "step 7441, loss: 0.033219\n",
      "step 7442, loss: 0.033201\n",
      "step 7443, loss: 0.033183\n",
      "step 7444, loss: 0.033165\n",
      "step 7445, loss: 0.033148\n",
      "step 7446, loss: 0.033130\n",
      "step 7447, loss: 0.033112\n",
      "step 7448, loss: 0.033094\n",
      "step 7449, loss: 0.033076\n",
      "step 7450, loss: 0.033059\n",
      "step 7451, loss: 0.033041\n",
      "step 7452, loss: 0.033023\n",
      "step 7453, loss: 0.033006\n",
      "step 7454, loss: 0.032988\n",
      "step 7455, loss: 0.032971\n",
      "step 7456, loss: 0.032953\n",
      "step 7457, loss: 0.032936\n",
      "step 7458, loss: 0.032918\n",
      "step 7459, loss: 0.032901\n",
      "step 7460, loss: 0.032884\n",
      "step 7461, loss: 0.032866\n",
      "step 7462, loss: 0.032849\n",
      "step 7463, loss: 0.032832\n",
      "step 7464, loss: 0.032815\n",
      "step 7465, loss: 0.032798\n",
      "step 7466, loss: 0.032780\n",
      "step 7467, loss: 0.032763\n",
      "step 7468, loss: 0.032746\n",
      "step 7469, loss: 0.032729\n",
      "step 7470, loss: 0.032712\n",
      "step 7471, loss: 0.032695\n",
      "step 7472, loss: 0.032678\n",
      "step 7473, loss: 0.032662\n",
      "step 7474, loss: 0.032645\n",
      "step 7475, loss: 0.032628\n",
      "step 7476, loss: 0.032611\n",
      "step 7477, loss: 0.032594\n",
      "step 7478, loss: 0.032578\n",
      "step 7479, loss: 0.032561\n",
      "step 7480, loss: 0.032544\n",
      "step 7481, loss: 0.032528\n",
      "step 7482, loss: 0.032511\n",
      "step 7483, loss: 0.032495\n",
      "step 7484, loss: 0.032478\n",
      "step 7485, loss: 0.032462\n",
      "step 7486, loss: 0.032445\n",
      "step 7487, loss: 0.032429\n",
      "step 7488, loss: 0.032413\n",
      "step 7489, loss: 0.032396\n",
      "step 7490, loss: 0.032380\n",
      "step 7491, loss: 0.032364\n",
      "step 7492, loss: 0.032347\n",
      "step 7493, loss: 0.032331\n",
      "step 7494, loss: 0.032315\n",
      "step 7495, loss: 0.032299\n",
      "step 7496, loss: 0.032283\n",
      "step 7497, loss: 0.032267\n",
      "step 7498, loss: 0.032251\n",
      "step 7499, loss: 0.032235\n",
      "step 7500, loss: 0.032219\n",
      "step 7501, loss: 0.032203\n",
      "step 7502, loss: 0.032187\n",
      "step 7503, loss: 0.032171\n",
      "step 7504, loss: 0.032155\n",
      "step 7505, loss: 0.032140\n",
      "step 7506, loss: 0.032124\n",
      "step 7507, loss: 0.032108\n",
      "step 7508, loss: 0.032092\n",
      "step 7509, loss: 0.032077\n",
      "step 7510, loss: 0.032061\n",
      "step 7511, loss: 0.032046\n",
      "step 7512, loss: 0.032030\n",
      "step 7513, loss: 0.032014\n",
      "step 7514, loss: 0.031999\n",
      "step 7515, loss: 0.031983\n",
      "step 7516, loss: 0.031968\n",
      "step 7517, loss: 0.031953\n",
      "step 7518, loss: 0.031937\n",
      "step 7519, loss: 0.031922\n",
      "step 7520, loss: 0.031907\n",
      "step 7521, loss: 0.031891\n",
      "step 7522, loss: 0.031876\n",
      "step 7523, loss: 0.031861\n",
      "step 7524, loss: 0.031846\n",
      "step 7525, loss: 0.031831\n",
      "step 7526, loss: 0.031816\n",
      "step 7527, loss: 0.031800\n",
      "step 7528, loss: 0.031785\n",
      "step 7529, loss: 0.031770\n",
      "step 7530, loss: 0.031755\n",
      "step 7531, loss: 0.031740\n",
      "step 7532, loss: 0.031726\n",
      "step 7533, loss: 0.031711\n",
      "step 7534, loss: 0.031696\n",
      "step 7535, loss: 0.031681\n",
      "step 7536, loss: 0.031666\n",
      "step 7537, loss: 0.031651\n",
      "step 7538, loss: 0.031637\n",
      "step 7539, loss: 0.031622\n",
      "step 7540, loss: 0.031607\n",
      "step 7541, loss: 0.031593\n",
      "step 7542, loss: 0.031578\n",
      "step 7543, loss: 0.031564\n",
      "step 7544, loss: 0.031549\n",
      "step 7545, loss: 0.031535\n",
      "step 7546, loss: 0.031520\n",
      "step 7547, loss: 0.031506\n",
      "step 7548, loss: 0.031491\n",
      "step 7549, loss: 0.031477\n",
      "step 7550, loss: 0.031463\n",
      "step 7551, loss: 0.031448\n",
      "step 7552, loss: 0.031434\n",
      "step 7553, loss: 0.031420\n",
      "step 7554, loss: 0.031406\n",
      "step 7555, loss: 0.031391\n",
      "step 7556, loss: 0.031377\n",
      "step 7557, loss: 0.031363\n",
      "step 7558, loss: 0.031349\n",
      "step 7559, loss: 0.031335\n",
      "step 7560, loss: 0.031321\n",
      "step 7561, loss: 0.031307\n",
      "step 7562, loss: 0.031293\n",
      "step 7563, loss: 0.031279\n",
      "step 7564, loss: 0.031265\n",
      "step 7565, loss: 0.031251\n",
      "step 7566, loss: 0.031237\n",
      "step 7567, loss: 0.031223\n",
      "step 7568, loss: 0.031210\n",
      "step 7569, loss: 0.031196\n",
      "step 7570, loss: 0.031182\n",
      "step 7571, loss: 0.031168\n",
      "step 7572, loss: 0.031155\n",
      "step 7573, loss: 0.031141\n",
      "step 7574, loss: 0.031127\n",
      "step 7575, loss: 0.031114\n",
      "step 7576, loss: 0.031100\n",
      "step 7577, loss: 0.031087\n",
      "step 7578, loss: 0.031073\n",
      "step 7579, loss: 0.031060\n",
      "step 7580, loss: 0.031046\n",
      "step 7581, loss: 0.031033\n",
      "step 7582, loss: 0.031020\n",
      "step 7583, loss: 0.031006\n",
      "step 7584, loss: 0.030993\n",
      "step 7585, loss: 0.030980\n",
      "step 7586, loss: 0.030966\n",
      "step 7587, loss: 0.030953\n",
      "step 7588, loss: 0.030940\n",
      "step 7589, loss: 0.030927\n",
      "step 7590, loss: 0.030914\n",
      "step 7591, loss: 0.030900\n",
      "step 7592, loss: 0.030887\n",
      "step 7593, loss: 0.030874\n",
      "step 7594, loss: 0.030861\n",
      "step 7595, loss: 0.030848\n",
      "step 7596, loss: 0.030835\n",
      "step 7597, loss: 0.030822\n",
      "step 7598, loss: 0.030809\n",
      "step 7599, loss: 0.030797\n",
      "step 7600, loss: 0.030784\n",
      "step 7601, loss: 0.030771\n",
      "step 7602, loss: 0.030758\n",
      "step 7603, loss: 0.030745\n",
      "step 7604, loss: 0.030733\n",
      "step 7605, loss: 0.030720\n",
      "step 7606, loss: 0.030707\n",
      "step 7607, loss: 0.030695\n",
      "step 7608, loss: 0.030682\n",
      "step 7609, loss: 0.030669\n",
      "step 7610, loss: 0.030657\n",
      "step 7611, loss: 0.030644\n",
      "step 7612, loss: 0.030632\n",
      "step 7613, loss: 0.030619\n",
      "step 7614, loss: 0.030607\n",
      "step 7615, loss: 0.030594\n",
      "step 7616, loss: 0.030582\n",
      "step 7617, loss: 0.030570\n",
      "step 7618, loss: 0.030557\n",
      "step 7619, loss: 0.030545\n",
      "step 7620, loss: 0.030533\n",
      "step 7621, loss: 0.030520\n",
      "step 7622, loss: 0.030508\n",
      "step 7623, loss: 0.030496\n",
      "step 7624, loss: 0.030484\n",
      "step 7625, loss: 0.030471\n",
      "step 7626, loss: 0.030459\n",
      "step 7627, loss: 0.030447\n",
      "step 7628, loss: 0.030435\n",
      "step 7629, loss: 0.030423\n",
      "step 7630, loss: 0.030411\n",
      "step 7631, loss: 0.030399\n",
      "step 7632, loss: 0.030387\n",
      "step 7633, loss: 0.030375\n",
      "step 7634, loss: 0.030363\n",
      "step 7635, loss: 0.030351\n",
      "step 7636, loss: 0.030340\n",
      "step 7637, loss: 0.030328\n",
      "step 7638, loss: 0.030316\n",
      "step 7639, loss: 0.030304\n",
      "step 7640, loss: 0.030292\n",
      "step 7641, loss: 0.030281\n",
      "step 7642, loss: 0.030269\n",
      "step 7643, loss: 0.030257\n",
      "step 7644, loss: 0.030246\n",
      "step 7645, loss: 0.030234\n",
      "step 7646, loss: 0.030223\n",
      "step 7647, loss: 0.030211\n",
      "step 7648, loss: 0.030199\n",
      "step 7649, loss: 0.030188\n",
      "step 7650, loss: 0.030176\n",
      "step 7651, loss: 0.030165\n",
      "step 7652, loss: 0.030154\n",
      "step 7653, loss: 0.030142\n",
      "step 7654, loss: 0.030131\n",
      "step 7655, loss: 0.030119\n",
      "step 7656, loss: 0.030108\n",
      "step 7657, loss: 0.030097\n",
      "step 7658, loss: 0.030086\n",
      "step 7659, loss: 0.030074\n",
      "step 7660, loss: 0.030063\n",
      "step 7661, loss: 0.030052\n",
      "step 7662, loss: 0.030041\n",
      "step 7663, loss: 0.030030\n",
      "step 7664, loss: 0.030018\n",
      "step 7665, loss: 0.030007\n",
      "step 7666, loss: 0.029996\n",
      "step 7667, loss: 0.029985\n",
      "step 7668, loss: 0.029974\n",
      "step 7669, loss: 0.029963\n",
      "step 7670, loss: 0.029952\n",
      "step 7671, loss: 0.029941\n",
      "step 7672, loss: 0.029930\n",
      "step 7673, loss: 0.029920\n",
      "step 7674, loss: 0.029909\n",
      "step 7675, loss: 0.029898\n",
      "step 7676, loss: 0.029887\n",
      "step 7677, loss: 0.029876\n",
      "step 7678, loss: 0.029865\n",
      "step 7679, loss: 0.029855\n",
      "step 7680, loss: 0.029844\n",
      "step 7681, loss: 0.029833\n",
      "step 7682, loss: 0.029823\n",
      "step 7683, loss: 0.029812\n",
      "step 7684, loss: 0.029801\n",
      "step 7685, loss: 0.029791\n",
      "step 7686, loss: 0.029780\n",
      "step 7687, loss: 0.029770\n",
      "step 7688, loss: 0.029759\n",
      "step 7689, loss: 0.029749\n",
      "step 7690, loss: 0.029738\n",
      "step 7691, loss: 0.029728\n",
      "step 7692, loss: 0.029717\n",
      "step 7693, loss: 0.029707\n",
      "step 7694, loss: 0.029697\n",
      "step 7695, loss: 0.029686\n",
      "step 7696, loss: 0.029676\n",
      "step 7697, loss: 0.029666\n",
      "step 7698, loss: 0.029655\n",
      "step 7699, loss: 0.029645\n",
      "step 7700, loss: 0.029635\n",
      "step 7701, loss: 0.029625\n",
      "step 7702, loss: 0.029615\n",
      "step 7703, loss: 0.029605\n",
      "step 7704, loss: 0.029594\n",
      "step 7705, loss: 0.029584\n",
      "step 7706, loss: 0.029574\n",
      "step 7707, loss: 0.029564\n",
      "step 7708, loss: 0.029554\n",
      "step 7709, loss: 0.029544\n",
      "step 7710, loss: 0.029534\n",
      "step 7711, loss: 0.029524\n",
      "step 7712, loss: 0.029514\n",
      "step 7713, loss: 0.029504\n",
      "step 7714, loss: 0.029495\n",
      "step 7715, loss: 0.029485\n",
      "step 7716, loss: 0.029475\n",
      "step 7717, loss: 0.029465\n",
      "step 7718, loss: 0.029455\n",
      "step 7719, loss: 0.029445\n",
      "step 7720, loss: 0.029436\n",
      "step 7721, loss: 0.029426\n",
      "step 7722, loss: 0.029416\n",
      "step 7723, loss: 0.029407\n",
      "step 7724, loss: 0.029397\n",
      "step 7725, loss: 0.029387\n",
      "step 7726, loss: 0.029378\n",
      "step 7727, loss: 0.029368\n",
      "step 7728, loss: 0.029359\n",
      "step 7729, loss: 0.029349\n",
      "step 7730, loss: 0.029339\n",
      "step 7731, loss: 0.029330\n",
      "step 7732, loss: 0.029321\n",
      "step 7733, loss: 0.029311\n",
      "step 7734, loss: 0.029302\n",
      "step 7735, loss: 0.029292\n",
      "step 7736, loss: 0.029283\n",
      "step 7737, loss: 0.029274\n",
      "step 7738, loss: 0.029264\n",
      "step 7739, loss: 0.029255\n",
      "step 7740, loss: 0.029246\n",
      "step 7741, loss: 0.029236\n",
      "step 7742, loss: 0.029227\n",
      "step 7743, loss: 0.029218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 7744, loss: 0.029209\n",
      "step 7745, loss: 0.029200\n",
      "step 7746, loss: 0.029190\n",
      "step 7747, loss: 0.029181\n",
      "step 7748, loss: 0.029172\n",
      "step 7749, loss: 0.029163\n",
      "step 7750, loss: 0.029154\n",
      "step 7751, loss: 0.029145\n",
      "step 7752, loss: 0.029136\n",
      "step 7753, loss: 0.029127\n",
      "step 7754, loss: 0.029118\n",
      "step 7755, loss: 0.029109\n",
      "step 7756, loss: 0.029100\n",
      "step 7757, loss: 0.029091\n",
      "step 7758, loss: 0.029082\n",
      "step 7759, loss: 0.029073\n",
      "step 7760, loss: 0.029065\n",
      "step 7761, loss: 0.029056\n",
      "step 7762, loss: 0.029047\n",
      "step 7763, loss: 0.029038\n",
      "step 7764, loss: 0.029029\n",
      "step 7765, loss: 0.029021\n",
      "step 7766, loss: 0.029012\n",
      "step 7767, loss: 0.029003\n",
      "step 7768, loss: 0.028995\n",
      "step 7769, loss: 0.028986\n",
      "step 7770, loss: 0.028977\n",
      "step 7771, loss: 0.028969\n",
      "step 7772, loss: 0.028960\n",
      "step 7773, loss: 0.028952\n",
      "step 7774, loss: 0.028943\n",
      "step 7775, loss: 0.028935\n",
      "step 7776, loss: 0.028926\n",
      "step 7777, loss: 0.028918\n",
      "step 7778, loss: 0.028909\n",
      "step 7779, loss: 0.028901\n",
      "step 7780, loss: 0.028892\n",
      "step 7781, loss: 0.028884\n",
      "step 7782, loss: 0.028876\n",
      "step 7783, loss: 0.028867\n",
      "step 7784, loss: 0.028859\n",
      "step 7785, loss: 0.028850\n",
      "step 7786, loss: 0.028842\n",
      "step 7787, loss: 0.028834\n",
      "step 7788, loss: 0.028826\n",
      "step 7789, loss: 0.028817\n",
      "step 7790, loss: 0.028809\n",
      "step 7791, loss: 0.028801\n",
      "step 7792, loss: 0.028793\n",
      "step 7793, loss: 0.028785\n",
      "step 7794, loss: 0.028777\n",
      "step 7795, loss: 0.028768\n",
      "step 7796, loss: 0.028760\n",
      "step 7797, loss: 0.028752\n",
      "step 7798, loss: 0.028744\n",
      "step 7799, loss: 0.028736\n",
      "step 7800, loss: 0.028728\n",
      "step 7801, loss: 0.028720\n",
      "step 7802, loss: 0.028712\n",
      "step 7803, loss: 0.028704\n",
      "step 7804, loss: 0.028696\n",
      "step 7805, loss: 0.028688\n",
      "step 7806, loss: 0.028681\n",
      "step 7807, loss: 0.028673\n",
      "step 7808, loss: 0.028665\n",
      "step 7809, loss: 0.028657\n",
      "step 7810, loss: 0.028649\n",
      "step 7811, loss: 0.028641\n",
      "step 7812, loss: 0.028634\n",
      "step 7813, loss: 0.028626\n",
      "step 7814, loss: 0.028618\n",
      "step 7815, loss: 0.028611\n",
      "step 7816, loss: 0.028603\n",
      "step 7817, loss: 0.028595\n",
      "step 7818, loss: 0.028587\n",
      "step 7819, loss: 0.028580\n",
      "step 7820, loss: 0.028572\n",
      "step 7821, loss: 0.028565\n",
      "step 7822, loss: 0.028557\n",
      "step 7823, loss: 0.028550\n",
      "step 7824, loss: 0.028542\n",
      "step 7825, loss: 0.028534\n",
      "step 7826, loss: 0.028527\n",
      "step 7827, loss: 0.028520\n",
      "step 7828, loss: 0.028512\n",
      "step 7829, loss: 0.028505\n",
      "step 7830, loss: 0.028497\n",
      "step 7831, loss: 0.028490\n",
      "step 7832, loss: 0.028482\n",
      "step 7833, loss: 0.028475\n",
      "step 7834, loss: 0.028468\n",
      "step 7835, loss: 0.028460\n",
      "step 7836, loss: 0.028453\n",
      "step 7837, loss: 0.028446\n",
      "step 7838, loss: 0.028439\n",
      "step 7839, loss: 0.028431\n",
      "step 7840, loss: 0.028424\n",
      "step 7841, loss: 0.028417\n",
      "step 7842, loss: 0.028410\n",
      "step 7843, loss: 0.028402\n",
      "step 7844, loss: 0.028395\n",
      "step 7845, loss: 0.028388\n",
      "step 7846, loss: 0.028381\n",
      "step 7847, loss: 0.028374\n",
      "step 7848, loss: 0.028367\n",
      "step 7849, loss: 0.028360\n",
      "step 7850, loss: 0.028353\n",
      "step 7851, loss: 0.028346\n",
      "step 7852, loss: 0.028339\n",
      "step 7853, loss: 0.028332\n",
      "step 7854, loss: 0.028325\n",
      "step 7855, loss: 0.028318\n",
      "step 7856, loss: 0.028311\n",
      "step 7857, loss: 0.028304\n",
      "step 7858, loss: 0.028297\n",
      "step 7859, loss: 0.028290\n",
      "step 7860, loss: 0.028283\n",
      "step 7861, loss: 0.028276\n",
      "step 7862, loss: 0.028270\n",
      "step 7863, loss: 0.028263\n",
      "step 7864, loss: 0.028256\n",
      "step 7865, loss: 0.028249\n",
      "step 7866, loss: 0.028242\n",
      "step 7867, loss: 0.028236\n",
      "step 7868, loss: 0.028229\n",
      "step 7869, loss: 0.028222\n",
      "step 7870, loss: 0.028216\n",
      "step 7871, loss: 0.028209\n",
      "step 7872, loss: 0.028202\n",
      "step 7873, loss: 0.028196\n",
      "step 7874, loss: 0.028189\n",
      "step 7875, loss: 0.028182\n",
      "step 7876, loss: 0.028176\n",
      "step 7877, loss: 0.028169\n",
      "step 7878, loss: 0.028163\n",
      "step 7879, loss: 0.028156\n",
      "step 7880, loss: 0.028150\n",
      "step 7881, loss: 0.028143\n",
      "step 7882, loss: 0.028137\n",
      "step 7883, loss: 0.028130\n",
      "step 7884, loss: 0.028124\n",
      "step 7885, loss: 0.028117\n",
      "step 7886, loss: 0.028111\n",
      "step 7887, loss: 0.028104\n",
      "step 7888, loss: 0.028098\n",
      "step 7889, loss: 0.028092\n",
      "step 7890, loss: 0.028085\n",
      "step 7891, loss: 0.028079\n",
      "step 7892, loss: 0.028073\n",
      "step 7893, loss: 0.028066\n",
      "step 7894, loss: 0.028060\n",
      "step 7895, loss: 0.028054\n",
      "step 7896, loss: 0.028048\n",
      "step 7897, loss: 0.028041\n",
      "step 7898, loss: 0.028035\n",
      "step 7899, loss: 0.028029\n",
      "step 7900, loss: 0.028023\n",
      "step 7901, loss: 0.028017\n",
      "step 7902, loss: 0.028010\n",
      "step 7903, loss: 0.028004\n",
      "step 7904, loss: 0.027998\n",
      "step 7905, loss: 0.027992\n",
      "step 7906, loss: 0.027986\n",
      "step 7907, loss: 0.027980\n",
      "step 7908, loss: 0.027974\n",
      "step 7909, loss: 0.027968\n",
      "step 7910, loss: 0.027962\n",
      "step 7911, loss: 0.027956\n",
      "step 7912, loss: 0.027950\n",
      "step 7913, loss: 0.027944\n",
      "step 7914, loss: 0.027938\n",
      "step 7915, loss: 0.027932\n",
      "step 7916, loss: 0.027926\n",
      "step 7917, loss: 0.027920\n",
      "step 7918, loss: 0.027914\n",
      "step 7919, loss: 0.027908\n",
      "step 7920, loss: 0.027903\n",
      "step 7921, loss: 0.027897\n",
      "step 7922, loss: 0.027891\n",
      "step 7923, loss: 0.027885\n",
      "step 7924, loss: 0.027879\n",
      "step 7925, loss: 0.027874\n",
      "step 7926, loss: 0.027868\n",
      "step 7927, loss: 0.027862\n",
      "step 7928, loss: 0.027856\n",
      "step 7929, loss: 0.027851\n",
      "step 7930, loss: 0.027845\n",
      "step 7931, loss: 0.027839\n",
      "step 7932, loss: 0.027834\n",
      "step 7933, loss: 0.027828\n",
      "step 7934, loss: 0.027822\n",
      "step 7935, loss: 0.027817\n",
      "step 7936, loss: 0.027811\n",
      "step 7937, loss: 0.027805\n",
      "step 7938, loss: 0.027800\n",
      "step 7939, loss: 0.027794\n",
      "step 7940, loss: 0.027789\n",
      "step 7941, loss: 0.027783\n",
      "step 7942, loss: 0.027778\n",
      "step 7943, loss: 0.027772\n",
      "step 7944, loss: 0.027767\n",
      "step 7945, loss: 0.027761\n",
      "step 7946, loss: 0.027756\n",
      "step 7947, loss: 0.027750\n",
      "step 7948, loss: 0.027745\n",
      "step 7949, loss: 0.027739\n",
      "step 7950, loss: 0.027734\n",
      "step 7951, loss: 0.027729\n",
      "step 7952, loss: 0.027723\n",
      "step 7953, loss: 0.027718\n",
      "step 7954, loss: 0.027713\n",
      "step 7955, loss: 0.027707\n",
      "step 7956, loss: 0.027702\n",
      "step 7957, loss: 0.027697\n",
      "step 7958, loss: 0.027691\n",
      "step 7959, loss: 0.027686\n",
      "step 7960, loss: 0.027681\n",
      "step 7961, loss: 0.027676\n",
      "step 7962, loss: 0.027670\n",
      "step 7963, loss: 0.027665\n",
      "step 7964, loss: 0.027660\n",
      "step 7965, loss: 0.027655\n",
      "step 7966, loss: 0.027650\n",
      "step 7967, loss: 0.027645\n",
      "step 7968, loss: 0.027639\n",
      "step 7969, loss: 0.027634\n",
      "step 7970, loss: 0.027629\n",
      "step 7971, loss: 0.027624\n",
      "step 7972, loss: 0.027619\n",
      "step 7973, loss: 0.027614\n",
      "step 7974, loss: 0.027609\n",
      "step 7975, loss: 0.027604\n",
      "step 7976, loss: 0.027599\n",
      "step 7977, loss: 0.027594\n",
      "step 7978, loss: 0.027589\n",
      "step 7979, loss: 0.027584\n",
      "step 7980, loss: 0.027579\n",
      "step 7981, loss: 0.027574\n",
      "step 7982, loss: 0.027569\n",
      "step 7983, loss: 0.027564\n",
      "step 7984, loss: 0.027559\n",
      "step 7985, loss: 0.027554\n",
      "step 7986, loss: 0.027549\n",
      "step 7987, loss: 0.027544\n",
      "step 7988, loss: 0.027540\n",
      "step 7989, loss: 0.027535\n",
      "step 7990, loss: 0.027530\n",
      "step 7991, loss: 0.027525\n",
      "step 7992, loss: 0.027520\n",
      "step 7993, loss: 0.027516\n",
      "step 7994, loss: 0.027511\n",
      "step 7995, loss: 0.027506\n",
      "step 7996, loss: 0.027501\n",
      "step 7997, loss: 0.027496\n",
      "step 7998, loss: 0.027492\n",
      "step 7999, loss: 0.027487\n",
      "step 8000, loss: 0.027482\n",
      "step 8001, loss: 0.027478\n",
      "step 8002, loss: 0.027473\n",
      "step 8003, loss: 0.027468\n",
      "step 8004, loss: 0.027464\n",
      "step 8005, loss: 0.027459\n",
      "step 8006, loss: 0.027454\n",
      "step 8007, loss: 0.027450\n",
      "step 8008, loss: 0.027445\n",
      "step 8009, loss: 0.027441\n",
      "step 8010, loss: 0.027436\n",
      "step 8011, loss: 0.027432\n",
      "step 8012, loss: 0.027427\n",
      "step 8013, loss: 0.027423\n",
      "step 8014, loss: 0.027418\n",
      "step 8015, loss: 0.027414\n",
      "step 8016, loss: 0.027409\n",
      "step 8017, loss: 0.027405\n",
      "step 8018, loss: 0.027400\n",
      "step 8019, loss: 0.027396\n",
      "step 8020, loss: 0.027391\n",
      "step 8021, loss: 0.027387\n",
      "step 8022, loss: 0.027382\n",
      "step 8023, loss: 0.027378\n",
      "step 8024, loss: 0.027374\n",
      "step 8025, loss: 0.027369\n",
      "step 8026, loss: 0.027365\n",
      "step 8027, loss: 0.027361\n",
      "step 8028, loss: 0.027356\n",
      "step 8029, loss: 0.027352\n",
      "step 8030, loss: 0.027348\n",
      "step 8031, loss: 0.027343\n",
      "step 8032, loss: 0.027339\n",
      "step 8033, loss: 0.027335\n",
      "step 8034, loss: 0.027330\n",
      "step 8035, loss: 0.027326\n",
      "step 8036, loss: 0.027322\n",
      "step 8037, loss: 0.027318\n",
      "step 8038, loss: 0.027314\n",
      "step 8039, loss: 0.027309\n",
      "step 8040, loss: 0.027305\n",
      "step 8041, loss: 0.027301\n",
      "step 8042, loss: 0.027297\n",
      "step 8043, loss: 0.027293\n",
      "step 8044, loss: 0.027288\n",
      "step 8045, loss: 0.027284\n",
      "step 8046, loss: 0.027280\n",
      "step 8047, loss: 0.027276\n",
      "step 8048, loss: 0.027272\n",
      "step 8049, loss: 0.027268\n",
      "step 8050, loss: 0.027264\n",
      "step 8051, loss: 0.027260\n",
      "step 8052, loss: 0.027256\n",
      "step 8053, loss: 0.027252\n",
      "step 8054, loss: 0.027248\n",
      "step 8055, loss: 0.027244\n",
      "step 8056, loss: 0.027240\n",
      "step 8057, loss: 0.027236\n",
      "step 8058, loss: 0.027232\n",
      "step 8059, loss: 0.027228\n",
      "step 8060, loss: 0.027224\n",
      "step 8061, loss: 0.027220\n",
      "step 8062, loss: 0.027216\n",
      "step 8063, loss: 0.027212\n",
      "step 8064, loss: 0.027208\n",
      "step 8065, loss: 0.027204\n",
      "step 8066, loss: 0.027200\n",
      "step 8067, loss: 0.027197\n",
      "step 8068, loss: 0.027193\n",
      "step 8069, loss: 0.027189\n",
      "step 8070, loss: 0.027185\n",
      "step 8071, loss: 0.027181\n",
      "step 8072, loss: 0.027177\n",
      "step 8073, loss: 0.027174\n",
      "step 8074, loss: 0.027170\n",
      "step 8075, loss: 0.027166\n",
      "step 8076, loss: 0.027162\n",
      "step 8077, loss: 0.027158\n",
      "step 8078, loss: 0.027155\n",
      "step 8079, loss: 0.027151\n",
      "step 8080, loss: 0.027147\n",
      "step 8081, loss: 0.027144\n",
      "step 8082, loss: 0.027140\n",
      "step 8083, loss: 0.027136\n",
      "step 8084, loss: 0.027132\n",
      "step 8085, loss: 0.027129\n",
      "step 8086, loss: 0.027125\n",
      "step 8087, loss: 0.027121\n",
      "step 8088, loss: 0.027118\n",
      "step 8089, loss: 0.027114\n",
      "step 8090, loss: 0.027111\n",
      "step 8091, loss: 0.027107\n",
      "step 8092, loss: 0.027103\n",
      "step 8093, loss: 0.027100\n",
      "step 8094, loss: 0.027096\n",
      "step 8095, loss: 0.027093\n",
      "step 8096, loss: 0.027089\n",
      "step 8097, loss: 0.027086\n",
      "step 8098, loss: 0.027082\n",
      "step 8099, loss: 0.027079\n",
      "step 8100, loss: 0.027075\n",
      "step 8101, loss: 0.027072\n",
      "step 8102, loss: 0.027068\n",
      "step 8103, loss: 0.027065\n",
      "step 8104, loss: 0.027061\n",
      "step 8105, loss: 0.027058\n",
      "step 8106, loss: 0.027054\n",
      "step 8107, loss: 0.027051\n",
      "step 8108, loss: 0.027047\n",
      "step 8109, loss: 0.027044\n",
      "step 8110, loss: 0.027041\n",
      "step 8111, loss: 0.027037\n",
      "step 8112, loss: 0.027034\n",
      "step 8113, loss: 0.027030\n",
      "step 8114, loss: 0.027027\n",
      "step 8115, loss: 0.027024\n",
      "step 8116, loss: 0.027020\n",
      "step 8117, loss: 0.027017\n",
      "step 8118, loss: 0.027014\n",
      "step 8119, loss: 0.027010\n",
      "step 8120, loss: 0.027007\n",
      "step 8121, loss: 0.027004\n",
      "step 8122, loss: 0.027000\n",
      "step 8123, loss: 0.026997\n",
      "step 8124, loss: 0.026994\n",
      "step 8125, loss: 0.026991\n",
      "step 8126, loss: 0.026987\n",
      "step 8127, loss: 0.026984\n",
      "step 8128, loss: 0.026981\n",
      "step 8129, loss: 0.026978\n",
      "step 8130, loss: 0.026975\n",
      "step 8131, loss: 0.026971\n",
      "step 8132, loss: 0.026968\n",
      "step 8133, loss: 0.026965\n",
      "step 8134, loss: 0.026962\n",
      "step 8135, loss: 0.026959\n",
      "step 8136, loss: 0.026956\n",
      "step 8137, loss: 0.026952\n",
      "step 8138, loss: 0.026949\n",
      "step 8139, loss: 0.026946\n",
      "step 8140, loss: 0.026943\n",
      "step 8141, loss: 0.026940\n",
      "step 8142, loss: 0.026937\n",
      "step 8143, loss: 0.026934\n",
      "step 8144, loss: 0.026931\n",
      "step 8145, loss: 0.026928\n",
      "step 8146, loss: 0.026925\n",
      "step 8147, loss: 0.026922\n",
      "step 8148, loss: 0.026919\n",
      "step 8149, loss: 0.026916\n",
      "step 8150, loss: 0.026913\n",
      "step 8151, loss: 0.026910\n",
      "step 8152, loss: 0.026907\n",
      "step 8153, loss: 0.026904\n",
      "step 8154, loss: 0.026901\n",
      "step 8155, loss: 0.026898\n",
      "step 8156, loss: 0.026895\n",
      "step 8157, loss: 0.026892\n",
      "step 8158, loss: 0.026889\n",
      "step 8159, loss: 0.026886\n",
      "step 8160, loss: 0.026883\n",
      "step 8161, loss: 0.026880\n",
      "step 8162, loss: 0.026877\n",
      "step 8163, loss: 0.026874\n",
      "step 8164, loss: 0.026871\n",
      "step 8165, loss: 0.026868\n",
      "step 8166, loss: 0.026866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8167, loss: 0.026863\n",
      "step 8168, loss: 0.026860\n",
      "step 8169, loss: 0.026857\n",
      "step 8170, loss: 0.026854\n",
      "step 8171, loss: 0.026851\n",
      "step 8172, loss: 0.026849\n",
      "step 8173, loss: 0.026846\n",
      "step 8174, loss: 0.026843\n",
      "step 8175, loss: 0.026840\n",
      "step 8176, loss: 0.026837\n",
      "step 8177, loss: 0.026835\n",
      "step 8178, loss: 0.026832\n",
      "step 8179, loss: 0.026829\n",
      "step 8180, loss: 0.026826\n",
      "step 8181, loss: 0.026824\n",
      "step 8182, loss: 0.026821\n",
      "step 8183, loss: 0.026818\n",
      "step 8184, loss: 0.026816\n",
      "step 8185, loss: 0.026813\n",
      "step 8186, loss: 0.026810\n",
      "step 8187, loss: 0.026807\n",
      "step 8188, loss: 0.026805\n",
      "step 8189, loss: 0.026802\n",
      "step 8190, loss: 0.026799\n",
      "step 8191, loss: 0.026797\n",
      "step 8192, loss: 0.026794\n",
      "step 8193, loss: 0.026792\n",
      "step 8194, loss: 0.026789\n",
      "step 8195, loss: 0.026786\n",
      "step 8196, loss: 0.026784\n",
      "step 8197, loss: 0.026781\n",
      "step 8198, loss: 0.026778\n",
      "step 8199, loss: 0.026776\n",
      "step 8200, loss: 0.026773\n",
      "step 8201, loss: 0.026771\n",
      "step 8202, loss: 0.026768\n",
      "step 8203, loss: 0.026766\n",
      "step 8204, loss: 0.026763\n",
      "step 8205, loss: 0.026761\n",
      "step 8206, loss: 0.026758\n",
      "step 8207, loss: 0.026756\n",
      "step 8208, loss: 0.026753\n",
      "step 8209, loss: 0.026751\n",
      "step 8210, loss: 0.026748\n",
      "step 8211, loss: 0.026746\n",
      "step 8212, loss: 0.026743\n",
      "step 8213, loss: 0.026741\n",
      "step 8214, loss: 0.026738\n",
      "step 8215, loss: 0.026736\n",
      "step 8216, loss: 0.026733\n",
      "step 8217, loss: 0.026731\n",
      "step 8218, loss: 0.026728\n",
      "step 8219, loss: 0.026726\n",
      "step 8220, loss: 0.026724\n",
      "step 8221, loss: 0.026721\n",
      "step 8222, loss: 0.026719\n",
      "step 8223, loss: 0.026716\n",
      "step 8224, loss: 0.026714\n",
      "step 8225, loss: 0.026712\n",
      "step 8226, loss: 0.026709\n",
      "step 8227, loss: 0.026707\n",
      "step 8228, loss: 0.026705\n",
      "step 8229, loss: 0.026702\n",
      "step 8230, loss: 0.026700\n",
      "step 8231, loss: 0.026697\n",
      "step 8232, loss: 0.026695\n",
      "step 8233, loss: 0.026693\n",
      "step 8234, loss: 0.026691\n",
      "step 8235, loss: 0.026688\n",
      "step 8236, loss: 0.026686\n",
      "step 8237, loss: 0.026684\n",
      "step 8238, loss: 0.026681\n",
      "step 8239, loss: 0.026679\n",
      "step 8240, loss: 0.026677\n",
      "step 8241, loss: 0.026675\n",
      "step 8242, loss: 0.026672\n",
      "step 8243, loss: 0.026670\n",
      "step 8244, loss: 0.026668\n",
      "step 8245, loss: 0.026666\n",
      "step 8246, loss: 0.026663\n",
      "step 8247, loss: 0.026661\n",
      "step 8248, loss: 0.026659\n",
      "step 8249, loss: 0.026657\n",
      "step 8250, loss: 0.026655\n",
      "step 8251, loss: 0.026652\n",
      "step 8252, loss: 0.026650\n",
      "step 8253, loss: 0.026648\n",
      "step 8254, loss: 0.026646\n",
      "step 8255, loss: 0.026644\n",
      "step 8256, loss: 0.026642\n",
      "step 8257, loss: 0.026639\n",
      "step 8258, loss: 0.026637\n",
      "step 8259, loss: 0.026635\n",
      "step 8260, loss: 0.026633\n",
      "step 8261, loss: 0.026631\n",
      "step 8262, loss: 0.026629\n",
      "step 8263, loss: 0.026627\n",
      "step 8264, loss: 0.026625\n",
      "step 8265, loss: 0.026623\n",
      "step 8266, loss: 0.026621\n",
      "step 8267, loss: 0.026618\n",
      "step 8268, loss: 0.026616\n",
      "step 8269, loss: 0.026614\n",
      "step 8270, loss: 0.026612\n",
      "step 8271, loss: 0.026610\n",
      "step 8272, loss: 0.026608\n",
      "step 8273, loss: 0.026606\n",
      "step 8274, loss: 0.026604\n",
      "step 8275, loss: 0.026602\n",
      "step 8276, loss: 0.026600\n",
      "step 8277, loss: 0.026598\n",
      "step 8278, loss: 0.026596\n",
      "step 8279, loss: 0.026594\n",
      "step 8280, loss: 0.026592\n",
      "step 8281, loss: 0.026590\n",
      "step 8282, loss: 0.026588\n",
      "step 8283, loss: 0.026586\n",
      "step 8284, loss: 0.026584\n",
      "step 8285, loss: 0.026582\n",
      "step 8286, loss: 0.026580\n",
      "step 8287, loss: 0.026578\n",
      "step 8288, loss: 0.026577\n",
      "step 8289, loss: 0.026575\n",
      "step 8290, loss: 0.026573\n",
      "step 8291, loss: 0.026571\n",
      "step 8292, loss: 0.026569\n",
      "step 8293, loss: 0.026567\n",
      "step 8294, loss: 0.026565\n",
      "step 8295, loss: 0.026563\n",
      "step 8296, loss: 0.026561\n",
      "step 8297, loss: 0.026559\n",
      "step 8298, loss: 0.026558\n",
      "step 8299, loss: 0.026556\n",
      "step 8300, loss: 0.026554\n",
      "step 8301, loss: 0.026552\n",
      "step 8302, loss: 0.026550\n",
      "step 8303, loss: 0.026548\n",
      "step 8304, loss: 0.026546\n",
      "step 8305, loss: 0.026545\n",
      "step 8306, loss: 0.026543\n",
      "step 8307, loss: 0.026541\n",
      "step 8308, loss: 0.026539\n",
      "step 8309, loss: 0.026537\n",
      "step 8310, loss: 0.026536\n",
      "step 8311, loss: 0.026534\n",
      "step 8312, loss: 0.026532\n",
      "step 8313, loss: 0.026530\n",
      "step 8314, loss: 0.026528\n",
      "step 8315, loss: 0.026527\n",
      "step 8316, loss: 0.026525\n",
      "step 8317, loss: 0.026523\n",
      "step 8318, loss: 0.026521\n",
      "step 8319, loss: 0.026520\n",
      "step 8320, loss: 0.026518\n",
      "step 8321, loss: 0.026516\n",
      "step 8322, loss: 0.026515\n",
      "step 8323, loss: 0.026513\n",
      "step 8324, loss: 0.026511\n",
      "step 8325, loss: 0.026509\n",
      "step 8326, loss: 0.026508\n",
      "step 8327, loss: 0.026506\n",
      "step 8328, loss: 0.026504\n",
      "step 8329, loss: 0.026503\n",
      "step 8330, loss: 0.026501\n",
      "step 8331, loss: 0.026499\n",
      "step 8332, loss: 0.026498\n",
      "step 8333, loss: 0.026496\n",
      "step 8334, loss: 0.026494\n",
      "step 8335, loss: 0.026493\n",
      "step 8336, loss: 0.026491\n",
      "step 8337, loss: 0.026489\n",
      "step 8338, loss: 0.026488\n",
      "step 8339, loss: 0.026486\n",
      "step 8340, loss: 0.026485\n",
      "step 8341, loss: 0.026483\n",
      "step 8342, loss: 0.026481\n",
      "step 8343, loss: 0.026480\n",
      "step 8344, loss: 0.026478\n",
      "step 8345, loss: 0.026477\n",
      "step 8346, loss: 0.026475\n",
      "step 8347, loss: 0.026473\n",
      "step 8348, loss: 0.026472\n",
      "step 8349, loss: 0.026470\n",
      "step 8350, loss: 0.026469\n",
      "step 8351, loss: 0.026467\n",
      "step 8352, loss: 0.026466\n",
      "step 8353, loss: 0.026464\n",
      "step 8354, loss: 0.026462\n",
      "step 8355, loss: 0.026461\n",
      "step 8356, loss: 0.026459\n",
      "step 8357, loss: 0.026458\n",
      "step 8358, loss: 0.026456\n",
      "step 8359, loss: 0.026455\n",
      "step 8360, loss: 0.026453\n",
      "step 8361, loss: 0.026452\n",
      "step 8362, loss: 0.026450\n",
      "step 8363, loss: 0.026449\n",
      "step 8364, loss: 0.026447\n",
      "step 8365, loss: 0.026446\n",
      "step 8366, loss: 0.026444\n",
      "step 8367, loss: 0.026443\n",
      "step 8368, loss: 0.026441\n",
      "step 8369, loss: 0.026440\n",
      "step 8370, loss: 0.026439\n",
      "step 8371, loss: 0.026437\n",
      "step 8372, loss: 0.026436\n",
      "step 8373, loss: 0.026434\n",
      "step 8374, loss: 0.026433\n",
      "step 8375, loss: 0.026431\n",
      "step 8376, loss: 0.026430\n",
      "step 8377, loss: 0.026428\n",
      "step 8378, loss: 0.026427\n",
      "step 8379, loss: 0.026426\n",
      "step 8380, loss: 0.026424\n",
      "step 8381, loss: 0.026423\n",
      "step 8382, loss: 0.026421\n",
      "step 8383, loss: 0.026420\n",
      "step 8384, loss: 0.026419\n",
      "step 8385, loss: 0.026417\n",
      "step 8386, loss: 0.026416\n",
      "step 8387, loss: 0.026415\n",
      "step 8388, loss: 0.026413\n",
      "step 8389, loss: 0.026412\n",
      "step 8390, loss: 0.026410\n",
      "step 8391, loss: 0.026409\n",
      "step 8392, loss: 0.026408\n",
      "step 8393, loss: 0.026406\n",
      "step 8394, loss: 0.026405\n",
      "step 8395, loss: 0.026404\n",
      "step 8396, loss: 0.026402\n",
      "step 8397, loss: 0.026401\n",
      "step 8398, loss: 0.026400\n",
      "step 8399, loss: 0.026398\n",
      "step 8400, loss: 0.026397\n",
      "step 8401, loss: 0.026396\n",
      "step 8402, loss: 0.026395\n",
      "step 8403, loss: 0.026393\n",
      "step 8404, loss: 0.026392\n",
      "step 8405, loss: 0.026391\n",
      "step 8406, loss: 0.026389\n",
      "step 8407, loss: 0.026388\n",
      "step 8408, loss: 0.026387\n",
      "step 8409, loss: 0.026386\n",
      "step 8410, loss: 0.026384\n",
      "step 8411, loss: 0.026383\n",
      "step 8412, loss: 0.026382\n",
      "step 8413, loss: 0.026381\n",
      "step 8414, loss: 0.026379\n",
      "step 8415, loss: 0.026378\n",
      "step 8416, loss: 0.026377\n",
      "step 8417, loss: 0.026376\n",
      "step 8418, loss: 0.026374\n",
      "step 8419, loss: 0.026373\n",
      "step 8420, loss: 0.026372\n",
      "step 8421, loss: 0.026371\n",
      "step 8422, loss: 0.026369\n",
      "step 8423, loss: 0.026368\n",
      "step 8424, loss: 0.026367\n",
      "step 8425, loss: 0.026366\n",
      "step 8426, loss: 0.026365\n",
      "step 8427, loss: 0.026363\n",
      "step 8428, loss: 0.026362\n",
      "step 8429, loss: 0.026361\n",
      "step 8430, loss: 0.026360\n",
      "step 8431, loss: 0.026359\n",
      "step 8432, loss: 0.026358\n",
      "step 8433, loss: 0.026356\n",
      "step 8434, loss: 0.026355\n",
      "step 8435, loss: 0.026354\n",
      "step 8436, loss: 0.026353\n",
      "step 8437, loss: 0.026352\n",
      "step 8438, loss: 0.026351\n",
      "step 8439, loss: 0.026350\n",
      "step 8440, loss: 0.026348\n",
      "step 8441, loss: 0.026347\n",
      "step 8442, loss: 0.026346\n",
      "step 8443, loss: 0.026345\n",
      "step 8444, loss: 0.026344\n",
      "step 8445, loss: 0.026343\n",
      "step 8446, loss: 0.026342\n",
      "step 8447, loss: 0.026341\n",
      "step 8448, loss: 0.026339\n",
      "step 8449, loss: 0.026338\n",
      "step 8450, loss: 0.026337\n",
      "step 8451, loss: 0.026336\n",
      "step 8452, loss: 0.026335\n",
      "step 8453, loss: 0.026334\n",
      "step 8454, loss: 0.026333\n",
      "step 8455, loss: 0.026332\n",
      "step 8456, loss: 0.026331\n",
      "step 8457, loss: 0.026330\n",
      "step 8458, loss: 0.026329\n",
      "step 8459, loss: 0.026328\n",
      "step 8460, loss: 0.026327\n",
      "step 8461, loss: 0.026326\n",
      "step 8462, loss: 0.026324\n",
      "step 8463, loss: 0.026323\n",
      "step 8464, loss: 0.026322\n",
      "step 8465, loss: 0.026321\n",
      "step 8466, loss: 0.026320\n",
      "step 8467, loss: 0.026319\n",
      "step 8468, loss: 0.026318\n",
      "step 8469, loss: 0.026317\n",
      "step 8470, loss: 0.026316\n",
      "step 8471, loss: 0.026315\n",
      "step 8472, loss: 0.026314\n",
      "step 8473, loss: 0.026313\n",
      "step 8474, loss: 0.026312\n",
      "step 8475, loss: 0.026311\n",
      "step 8476, loss: 0.026310\n",
      "step 8477, loss: 0.026309\n",
      "step 8478, loss: 0.026308\n",
      "step 8479, loss: 0.026307\n",
      "step 8480, loss: 0.026306\n",
      "step 8481, loss: 0.026305\n",
      "step 8482, loss: 0.026304\n",
      "step 8483, loss: 0.026303\n",
      "step 8484, loss: 0.026302\n",
      "step 8485, loss: 0.026301\n",
      "step 8486, loss: 0.026301\n",
      "step 8487, loss: 0.026300\n",
      "step 8488, loss: 0.026299\n",
      "step 8489, loss: 0.026298\n",
      "step 8490, loss: 0.026297\n",
      "step 8491, loss: 0.026296\n",
      "step 8492, loss: 0.026295\n",
      "step 8493, loss: 0.026294\n",
      "step 8494, loss: 0.026293\n",
      "step 8495, loss: 0.026292\n",
      "step 8496, loss: 0.026291\n",
      "step 8497, loss: 0.026290\n",
      "step 8498, loss: 0.026289\n",
      "step 8499, loss: 0.026288\n",
      "step 8500, loss: 0.026288\n",
      "step 8501, loss: 0.026287\n",
      "step 8502, loss: 0.026286\n",
      "step 8503, loss: 0.026285\n",
      "step 8504, loss: 0.026284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8505, loss: 0.026283\n",
      "step 8506, loss: 0.026282\n",
      "step 8507, loss: 0.026281\n",
      "step 8508, loss: 0.026280\n",
      "step 8509, loss: 0.026279\n",
      "step 8510, loss: 0.026279\n",
      "step 8511, loss: 0.026278\n",
      "step 8512, loss: 0.026277\n",
      "step 8513, loss: 0.026276\n",
      "step 8514, loss: 0.026275\n",
      "step 8515, loss: 0.026274\n",
      "step 8516, loss: 0.026273\n",
      "step 8517, loss: 0.026273\n",
      "step 8518, loss: 0.026272\n",
      "step 8519, loss: 0.026271\n",
      "step 8520, loss: 0.026270\n",
      "step 8521, loss: 0.026269\n",
      "step 8522, loss: 0.026268\n",
      "step 8523, loss: 0.026268\n",
      "step 8524, loss: 0.026267\n",
      "step 8525, loss: 0.026266\n",
      "step 8526, loss: 0.026265\n",
      "step 8527, loss: 0.026264\n",
      "step 8528, loss: 0.026263\n",
      "step 8529, loss: 0.026263\n",
      "step 8530, loss: 0.026262\n",
      "step 8531, loss: 0.026261\n",
      "step 8532, loss: 0.026260\n",
      "step 8533, loss: 0.026259\n",
      "step 8534, loss: 0.026259\n",
      "step 8535, loss: 0.026258\n",
      "step 8536, loss: 0.026257\n",
      "step 8537, loss: 0.026256\n",
      "step 8538, loss: 0.026255\n",
      "step 8539, loss: 0.026255\n",
      "step 8540, loss: 0.026254\n",
      "step 8541, loss: 0.026253\n",
      "step 8542, loss: 0.026252\n",
      "step 8543, loss: 0.026252\n",
      "step 8544, loss: 0.026251\n",
      "step 8545, loss: 0.026250\n",
      "step 8546, loss: 0.026249\n",
      "step 8547, loss: 0.026249\n",
      "step 8548, loss: 0.026248\n",
      "step 8549, loss: 0.026247\n",
      "step 8550, loss: 0.026246\n",
      "step 8551, loss: 0.026246\n",
      "step 8552, loss: 0.026245\n",
      "step 8553, loss: 0.026244\n",
      "step 8554, loss: 0.026243\n",
      "step 8555, loss: 0.026243\n",
      "step 8556, loss: 0.026242\n",
      "step 8557, loss: 0.026241\n",
      "step 8558, loss: 0.026240\n",
      "step 8559, loss: 0.026240\n",
      "step 8560, loss: 0.026239\n",
      "step 8561, loss: 0.026238\n",
      "step 8562, loss: 0.026238\n",
      "step 8563, loss: 0.026237\n",
      "step 8564, loss: 0.026236\n",
      "step 8565, loss: 0.026235\n",
      "step 8566, loss: 0.026235\n",
      "step 8567, loss: 0.026234\n",
      "step 8568, loss: 0.026233\n",
      "step 8569, loss: 0.026233\n",
      "step 8570, loss: 0.026232\n",
      "step 8571, loss: 0.026231\n",
      "step 8572, loss: 0.026231\n",
      "step 8573, loss: 0.026230\n",
      "step 8574, loss: 0.026229\n",
      "step 8575, loss: 0.026229\n",
      "step 8576, loss: 0.026228\n",
      "step 8577, loss: 0.026227\n",
      "step 8578, loss: 0.026227\n",
      "step 8579, loss: 0.026226\n",
      "step 8580, loss: 0.026225\n",
      "step 8581, loss: 0.026225\n",
      "step 8582, loss: 0.026224\n",
      "step 8583, loss: 0.026223\n",
      "step 8584, loss: 0.026223\n",
      "step 8585, loss: 0.026222\n",
      "step 8586, loss: 0.026221\n",
      "step 8587, loss: 0.026221\n",
      "step 8588, loss: 0.026220\n",
      "step 8589, loss: 0.026219\n",
      "step 8590, loss: 0.026219\n",
      "step 8591, loss: 0.026218\n",
      "step 8592, loss: 0.026217\n",
      "step 8593, loss: 0.026217\n",
      "step 8594, loss: 0.026216\n",
      "step 8595, loss: 0.026216\n",
      "step 8596, loss: 0.026215\n",
      "step 8597, loss: 0.026214\n",
      "step 8598, loss: 0.026214\n",
      "step 8599, loss: 0.026213\n",
      "step 8600, loss: 0.026212\n",
      "step 8601, loss: 0.026212\n",
      "step 8602, loss: 0.026211\n",
      "step 8603, loss: 0.026211\n",
      "step 8604, loss: 0.026210\n",
      "step 8605, loss: 0.026209\n",
      "step 8606, loss: 0.026209\n",
      "step 8607, loss: 0.026208\n",
      "step 8608, loss: 0.026208\n",
      "step 8609, loss: 0.026207\n",
      "step 8610, loss: 0.026206\n",
      "step 8611, loss: 0.026206\n",
      "step 8612, loss: 0.026205\n",
      "step 8613, loss: 0.026205\n",
      "step 8614, loss: 0.026204\n",
      "step 8615, loss: 0.026204\n",
      "step 8616, loss: 0.026203\n",
      "step 8617, loss: 0.026202\n",
      "step 8618, loss: 0.026202\n",
      "step 8619, loss: 0.026201\n",
      "step 8620, loss: 0.026201\n",
      "step 8621, loss: 0.026200\n",
      "step 8622, loss: 0.026199\n",
      "step 8623, loss: 0.026199\n",
      "step 8624, loss: 0.026198\n",
      "step 8625, loss: 0.026198\n",
      "step 8626, loss: 0.026197\n",
      "step 8627, loss: 0.026197\n",
      "step 8628, loss: 0.026196\n",
      "step 8629, loss: 0.026196\n",
      "step 8630, loss: 0.026195\n",
      "step 8631, loss: 0.026195\n",
      "step 8632, loss: 0.026194\n",
      "step 8633, loss: 0.026194\n",
      "step 8634, loss: 0.026193\n",
      "step 8635, loss: 0.026192\n",
      "step 8636, loss: 0.026192\n",
      "step 8637, loss: 0.026191\n",
      "step 8638, loss: 0.026191\n",
      "step 8639, loss: 0.026190\n",
      "step 8640, loss: 0.026190\n",
      "step 8641, loss: 0.026189\n",
      "step 8642, loss: 0.026189\n",
      "step 8643, loss: 0.026188\n",
      "step 8644, loss: 0.026188\n",
      "step 8645, loss: 0.026187\n",
      "step 8646, loss: 0.026187\n",
      "step 8647, loss: 0.026186\n",
      "step 8648, loss: 0.026186\n",
      "step 8649, loss: 0.026185\n",
      "step 8650, loss: 0.026185\n",
      "step 8651, loss: 0.026184\n",
      "step 8652, loss: 0.026184\n",
      "step 8653, loss: 0.026183\n",
      "step 8654, loss: 0.026183\n",
      "step 8655, loss: 0.026182\n",
      "step 8656, loss: 0.026182\n",
      "step 8657, loss: 0.026181\n",
      "step 8658, loss: 0.026181\n",
      "step 8659, loss: 0.026180\n",
      "step 8660, loss: 0.026180\n",
      "step 8661, loss: 0.026179\n",
      "step 8662, loss: 0.026179\n",
      "step 8663, loss: 0.026178\n",
      "step 8664, loss: 0.026178\n",
      "step 8665, loss: 0.026177\n",
      "step 8666, loss: 0.026177\n",
      "step 8667, loss: 0.026177\n",
      "step 8668, loss: 0.026176\n",
      "step 8669, loss: 0.026176\n",
      "step 8670, loss: 0.026175\n",
      "step 8671, loss: 0.026175\n",
      "step 8672, loss: 0.026174\n",
      "step 8673, loss: 0.026174\n",
      "step 8674, loss: 0.026173\n",
      "step 8675, loss: 0.026173\n",
      "step 8676, loss: 0.026172\n",
      "step 8677, loss: 0.026172\n",
      "step 8678, loss: 0.026172\n",
      "step 8679, loss: 0.026171\n",
      "step 8680, loss: 0.026171\n",
      "step 8681, loss: 0.026170\n",
      "step 8682, loss: 0.026170\n",
      "step 8683, loss: 0.026169\n",
      "step 8684, loss: 0.026169\n",
      "step 8685, loss: 0.026168\n",
      "step 8686, loss: 0.026168\n",
      "step 8687, loss: 0.026168\n",
      "step 8688, loss: 0.026167\n",
      "step 8689, loss: 0.026167\n",
      "step 8690, loss: 0.026166\n",
      "step 8691, loss: 0.026166\n",
      "step 8692, loss: 0.026165\n",
      "step 8693, loss: 0.026165\n",
      "step 8694, loss: 0.026165\n",
      "step 8695, loss: 0.026164\n",
      "step 8696, loss: 0.026164\n",
      "step 8697, loss: 0.026163\n",
      "step 8698, loss: 0.026163\n",
      "step 8699, loss: 0.026163\n",
      "step 8700, loss: 0.026162\n",
      "step 8701, loss: 0.026162\n",
      "step 8702, loss: 0.026161\n",
      "step 8703, loss: 0.026161\n",
      "step 8704, loss: 0.026161\n",
      "step 8705, loss: 0.026160\n",
      "step 8706, loss: 0.026160\n",
      "step 8707, loss: 0.026159\n",
      "step 8708, loss: 0.026159\n",
      "step 8709, loss: 0.026159\n",
      "step 8710, loss: 0.026158\n",
      "step 8711, loss: 0.026158\n",
      "step 8712, loss: 0.026157\n",
      "step 8713, loss: 0.026157\n",
      "step 8714, loss: 0.026157\n",
      "step 8715, loss: 0.026156\n",
      "step 8716, loss: 0.026156\n",
      "step 8717, loss: 0.026156\n",
      "step 8718, loss: 0.026155\n",
      "step 8719, loss: 0.026155\n",
      "step 8720, loss: 0.026154\n",
      "step 8721, loss: 0.026154\n",
      "step 8722, loss: 0.026154\n",
      "step 8723, loss: 0.026153\n",
      "step 8724, loss: 0.026153\n",
      "step 8725, loss: 0.026153\n",
      "step 8726, loss: 0.026152\n",
      "step 8727, loss: 0.026152\n",
      "step 8728, loss: 0.026151\n",
      "step 8729, loss: 0.026151\n",
      "step 8730, loss: 0.026151\n",
      "step 8731, loss: 0.026150\n",
      "step 8732, loss: 0.026150\n",
      "step 8733, loss: 0.026150\n",
      "step 8734, loss: 0.026149\n",
      "step 8735, loss: 0.026149\n",
      "step 8736, loss: 0.026149\n",
      "step 8737, loss: 0.026148\n",
      "step 8738, loss: 0.026148\n",
      "step 8739, loss: 0.026148\n",
      "step 8740, loss: 0.026147\n",
      "step 8741, loss: 0.026147\n",
      "step 8742, loss: 0.026147\n",
      "step 8743, loss: 0.026146\n",
      "step 8744, loss: 0.026146\n",
      "step 8745, loss: 0.026146\n",
      "step 8746, loss: 0.026145\n",
      "step 8747, loss: 0.026145\n",
      "step 8748, loss: 0.026145\n",
      "step 8749, loss: 0.026144\n",
      "step 8750, loss: 0.026144\n",
      "step 8751, loss: 0.026144\n",
      "step 8752, loss: 0.026143\n",
      "step 8753, loss: 0.026143\n",
      "step 8754, loss: 0.026143\n",
      "step 8755, loss: 0.026142\n",
      "step 8756, loss: 0.026142\n",
      "step 8757, loss: 0.026142\n",
      "step 8758, loss: 0.026141\n",
      "step 8759, loss: 0.026141\n",
      "step 8760, loss: 0.026141\n",
      "step 8761, loss: 0.026140\n",
      "step 8762, loss: 0.026140\n",
      "step 8763, loss: 0.026140\n",
      "step 8764, loss: 0.026140\n",
      "step 8765, loss: 0.026139\n",
      "step 8766, loss: 0.026139\n",
      "step 8767, loss: 0.026139\n",
      "step 8768, loss: 0.026138\n",
      "step 8769, loss: 0.026138\n",
      "step 8770, loss: 0.026138\n",
      "step 8771, loss: 0.026137\n",
      "step 8772, loss: 0.026137\n",
      "step 8773, loss: 0.026137\n",
      "step 8774, loss: 0.026137\n",
      "step 8775, loss: 0.026136\n",
      "step 8776, loss: 0.026136\n",
      "step 8777, loss: 0.026136\n",
      "step 8778, loss: 0.026135\n",
      "step 8779, loss: 0.026135\n",
      "step 8780, loss: 0.026135\n",
      "step 8781, loss: 0.026134\n",
      "step 8782, loss: 0.026134\n",
      "step 8783, loss: 0.026134\n",
      "step 8784, loss: 0.026134\n",
      "step 8785, loss: 0.026133\n",
      "step 8786, loss: 0.026133\n",
      "step 8787, loss: 0.026133\n",
      "step 8788, loss: 0.026133\n",
      "step 8789, loss: 0.026132\n",
      "step 8790, loss: 0.026132\n",
      "step 8791, loss: 0.026132\n",
      "step 8792, loss: 0.026131\n",
      "step 8793, loss: 0.026131\n",
      "step 8794, loss: 0.026131\n",
      "step 8795, loss: 0.026131\n",
      "step 8796, loss: 0.026130\n",
      "step 8797, loss: 0.026130\n",
      "step 8798, loss: 0.026130\n",
      "step 8799, loss: 0.026130\n",
      "step 8800, loss: 0.026129\n",
      "step 8801, loss: 0.026129\n",
      "step 8802, loss: 0.026129\n",
      "step 8803, loss: 0.026129\n",
      "step 8804, loss: 0.026128\n",
      "step 8805, loss: 0.026128\n",
      "step 8806, loss: 0.026128\n",
      "step 8807, loss: 0.026127\n",
      "step 8808, loss: 0.026127\n",
      "step 8809, loss: 0.026127\n",
      "step 8810, loss: 0.026127\n",
      "step 8811, loss: 0.026126\n",
      "step 8812, loss: 0.026126\n",
      "step 8813, loss: 0.026126\n",
      "step 8814, loss: 0.026126\n",
      "step 8815, loss: 0.026126\n",
      "step 8816, loss: 0.026125\n",
      "step 8817, loss: 0.026125\n",
      "step 8818, loss: 0.026125\n",
      "step 8819, loss: 0.026125\n",
      "step 8820, loss: 0.026124\n",
      "step 8821, loss: 0.026124\n",
      "step 8822, loss: 0.026124\n",
      "step 8823, loss: 0.026124\n",
      "step 8824, loss: 0.026123\n",
      "step 8825, loss: 0.026123\n",
      "step 8826, loss: 0.026123\n",
      "step 8827, loss: 0.026123\n",
      "step 8828, loss: 0.026122\n",
      "step 8829, loss: 0.026122\n",
      "step 8830, loss: 0.026122\n",
      "step 8831, loss: 0.026122\n",
      "step 8832, loss: 0.026122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 8833, loss: 0.026121\n",
      "step 8834, loss: 0.026121\n",
      "step 8835, loss: 0.026121\n",
      "step 8836, loss: 0.026121\n",
      "step 8837, loss: 0.026120\n",
      "step 8838, loss: 0.026120\n",
      "step 8839, loss: 0.026120\n",
      "step 8840, loss: 0.026120\n",
      "step 8841, loss: 0.026119\n",
      "step 8842, loss: 0.026119\n",
      "step 8843, loss: 0.026119\n",
      "step 8844, loss: 0.026119\n",
      "step 8845, loss: 0.026119\n",
      "step 8846, loss: 0.026118\n",
      "step 8847, loss: 0.026118\n",
      "step 8848, loss: 0.026118\n",
      "step 8849, loss: 0.026118\n",
      "step 8850, loss: 0.026118\n",
      "step 8851, loss: 0.026117\n",
      "step 8852, loss: 0.026117\n",
      "step 8853, loss: 0.026117\n",
      "step 8854, loss: 0.026117\n",
      "step 8855, loss: 0.026117\n",
      "step 8856, loss: 0.026116\n",
      "step 8857, loss: 0.026116\n",
      "step 8858, loss: 0.026116\n",
      "step 8859, loss: 0.026116\n",
      "step 8860, loss: 0.026116\n",
      "step 8861, loss: 0.026115\n",
      "step 8862, loss: 0.026115\n",
      "step 8863, loss: 0.026115\n",
      "step 8864, loss: 0.026115\n",
      "step 8865, loss: 0.026115\n",
      "step 8866, loss: 0.026114\n",
      "step 8867, loss: 0.026114\n",
      "step 8868, loss: 0.026114\n",
      "step 8869, loss: 0.026114\n",
      "step 8870, loss: 0.026114\n",
      "step 8871, loss: 0.026113\n",
      "step 8872, loss: 0.026113\n",
      "step 8873, loss: 0.026113\n",
      "step 8874, loss: 0.026113\n",
      "step 8875, loss: 0.026113\n",
      "step 8876, loss: 0.026113\n",
      "step 8877, loss: 0.026112\n",
      "step 8878, loss: 0.026112\n",
      "step 8879, loss: 0.026112\n",
      "step 8880, loss: 0.026112\n",
      "step 8881, loss: 0.026112\n",
      "step 8882, loss: 0.026111\n",
      "step 8883, loss: 0.026111\n",
      "step 8884, loss: 0.026111\n",
      "step 8885, loss: 0.026111\n",
      "step 8886, loss: 0.026111\n",
      "step 8887, loss: 0.026111\n",
      "step 8888, loss: 0.026110\n",
      "step 8889, loss: 0.026110\n",
      "step 8890, loss: 0.026110\n",
      "step 8891, loss: 0.026110\n",
      "step 8892, loss: 0.026110\n",
      "step 8893, loss: 0.026110\n",
      "step 8894, loss: 0.026109\n",
      "step 8895, loss: 0.026109\n",
      "step 8896, loss: 0.026109\n",
      "step 8897, loss: 0.026109\n",
      "step 8898, loss: 0.026109\n",
      "step 8899, loss: 0.026108\n",
      "step 8900, loss: 0.026108\n",
      "step 8901, loss: 0.026108\n",
      "step 8902, loss: 0.026108\n",
      "step 8903, loss: 0.026108\n",
      "step 8904, loss: 0.026108\n",
      "step 8905, loss: 0.026107\n",
      "step 8906, loss: 0.026107\n",
      "step 8907, loss: 0.026107\n",
      "step 8908, loss: 0.026107\n",
      "step 8909, loss: 0.026107\n",
      "step 8910, loss: 0.026107\n",
      "step 8911, loss: 0.026107\n",
      "step 8912, loss: 0.026106\n",
      "step 8913, loss: 0.026106\n",
      "step 8914, loss: 0.026106\n",
      "step 8915, loss: 0.026106\n",
      "step 8916, loss: 0.026106\n",
      "step 8917, loss: 0.026106\n",
      "step 8918, loss: 0.026105\n",
      "step 8919, loss: 0.026105\n",
      "step 8920, loss: 0.026105\n",
      "step 8921, loss: 0.026105\n",
      "step 8922, loss: 0.026105\n",
      "step 8923, loss: 0.026105\n",
      "step 8924, loss: 0.026105\n",
      "step 8925, loss: 0.026104\n",
      "step 8926, loss: 0.026104\n",
      "step 8927, loss: 0.026104\n",
      "step 8928, loss: 0.026104\n",
      "step 8929, loss: 0.026104\n",
      "step 8930, loss: 0.026104\n",
      "step 8931, loss: 0.026104\n",
      "step 8932, loss: 0.026103\n",
      "step 8933, loss: 0.026103\n",
      "step 8934, loss: 0.026103\n",
      "step 8935, loss: 0.026103\n",
      "step 8936, loss: 0.026103\n",
      "step 8937, loss: 0.026103\n",
      "step 8938, loss: 0.026103\n",
      "step 8939, loss: 0.026102\n",
      "step 8940, loss: 0.026102\n",
      "step 8941, loss: 0.026102\n",
      "step 8942, loss: 0.026102\n",
      "step 8943, loss: 0.026102\n",
      "step 8944, loss: 0.026102\n",
      "step 8945, loss: 0.026102\n",
      "step 8946, loss: 0.026101\n",
      "step 8947, loss: 0.026101\n",
      "step 8948, loss: 0.026101\n",
      "step 8949, loss: 0.026101\n",
      "step 8950, loss: 0.026101\n",
      "step 8951, loss: 0.026101\n",
      "step 8952, loss: 0.026101\n",
      "step 8953, loss: 0.026101\n",
      "step 8954, loss: 0.026100\n",
      "step 8955, loss: 0.026100\n",
      "step 8956, loss: 0.026100\n",
      "step 8957, loss: 0.026100\n",
      "step 8958, loss: 0.026100\n",
      "step 8959, loss: 0.026100\n",
      "step 8960, loss: 0.026100\n",
      "step 8961, loss: 0.026100\n",
      "step 8962, loss: 0.026100\n",
      "step 8963, loss: 0.026099\n",
      "step 8964, loss: 0.026099\n",
      "step 8965, loss: 0.026099\n",
      "step 8966, loss: 0.026099\n",
      "step 8967, loss: 0.026099\n",
      "step 8968, loss: 0.026099\n",
      "step 8969, loss: 0.026099\n",
      "step 8970, loss: 0.026099\n",
      "step 8971, loss: 0.026098\n",
      "step 8972, loss: 0.026098\n",
      "step 8973, loss: 0.026098\n",
      "step 8974, loss: 0.026098\n",
      "step 8975, loss: 0.026098\n",
      "step 8976, loss: 0.026098\n",
      "step 8977, loss: 0.026098\n",
      "step 8978, loss: 0.026098\n",
      "step 8979, loss: 0.026098\n",
      "step 8980, loss: 0.026097\n",
      "step 8981, loss: 0.026097\n",
      "step 8982, loss: 0.026097\n",
      "step 8983, loss: 0.026097\n",
      "step 8984, loss: 0.026097\n",
      "step 8985, loss: 0.026097\n",
      "step 8986, loss: 0.026097\n",
      "step 8987, loss: 0.026097\n",
      "step 8988, loss: 0.026097\n",
      "step 8989, loss: 0.026096\n",
      "step 8990, loss: 0.026096\n",
      "step 8991, loss: 0.026096\n",
      "step 8992, loss: 0.026096\n",
      "step 8993, loss: 0.026096\n",
      "step 8994, loss: 0.026096\n",
      "step 8995, loss: 0.026096\n",
      "step 8996, loss: 0.026096\n",
      "step 8997, loss: 0.026096\n",
      "step 8998, loss: 0.026095\n",
      "step 8999, loss: 0.026095\n",
      "step 9000, loss: 0.026095\n",
      "step 9001, loss: 0.026095\n",
      "step 9002, loss: 0.026095\n",
      "step 9003, loss: 0.026095\n",
      "step 9004, loss: 0.026095\n",
      "step 9005, loss: 0.026095\n",
      "step 9006, loss: 0.026095\n",
      "step 9007, loss: 0.026095\n",
      "step 9008, loss: 0.026094\n",
      "step 9009, loss: 0.026094\n",
      "step 9010, loss: 0.026094\n",
      "step 9011, loss: 0.026094\n",
      "step 9012, loss: 0.026094\n",
      "step 9013, loss: 0.026094\n",
      "step 9014, loss: 0.026094\n",
      "step 9015, loss: 0.026094\n",
      "step 9016, loss: 0.026094\n",
      "step 9017, loss: 0.026094\n",
      "step 9018, loss: 0.026094\n",
      "step 9019, loss: 0.026093\n",
      "step 9020, loss: 0.026093\n",
      "step 9021, loss: 0.026093\n",
      "step 9022, loss: 0.026093\n",
      "step 9023, loss: 0.026093\n",
      "step 9024, loss: 0.026093\n",
      "step 9025, loss: 0.026093\n",
      "step 9026, loss: 0.026093\n",
      "step 9027, loss: 0.026093\n",
      "step 9028, loss: 0.026093\n",
      "step 9029, loss: 0.026093\n",
      "step 9030, loss: 0.026092\n",
      "step 9031, loss: 0.026092\n",
      "step 9032, loss: 0.026092\n",
      "step 9033, loss: 0.026092\n",
      "step 9034, loss: 0.026092\n",
      "step 9035, loss: 0.026092\n",
      "step 9036, loss: 0.026092\n",
      "step 9037, loss: 0.026092\n",
      "step 9038, loss: 0.026092\n",
      "step 9039, loss: 0.026092\n",
      "step 9040, loss: 0.026092\n",
      "step 9041, loss: 0.026092\n",
      "step 9042, loss: 0.026091\n",
      "step 9043, loss: 0.026091\n",
      "step 9044, loss: 0.026091\n",
      "step 9045, loss: 0.026091\n",
      "step 9046, loss: 0.026091\n",
      "step 9047, loss: 0.026091\n",
      "step 9048, loss: 0.026091\n",
      "step 9049, loss: 0.026091\n",
      "step 9050, loss: 0.026091\n",
      "step 9051, loss: 0.026091\n",
      "step 9052, loss: 0.026091\n",
      "step 9053, loss: 0.026091\n",
      "step 9054, loss: 0.026091\n",
      "step 9055, loss: 0.026090\n",
      "step 9056, loss: 0.026090\n",
      "step 9057, loss: 0.026090\n",
      "step 9058, loss: 0.026090\n",
      "step 9059, loss: 0.026090\n",
      "step 9060, loss: 0.026090\n",
      "step 9061, loss: 0.026090\n",
      "step 9062, loss: 0.026090\n",
      "step 9063, loss: 0.026090\n",
      "step 9064, loss: 0.026090\n",
      "step 9065, loss: 0.026090\n",
      "step 9066, loss: 0.026090\n",
      "step 9067, loss: 0.026090\n",
      "step 9068, loss: 0.026090\n",
      "step 9069, loss: 0.026089\n",
      "step 9070, loss: 0.026089\n",
      "step 9071, loss: 0.026089\n",
      "step 9072, loss: 0.026089\n",
      "step 9073, loss: 0.026089\n",
      "step 9074, loss: 0.026089\n",
      "step 9075, loss: 0.026089\n",
      "step 9076, loss: 0.026089\n",
      "step 9077, loss: 0.026089\n",
      "step 9078, loss: 0.026089\n",
      "step 9079, loss: 0.026089\n",
      "step 9080, loss: 0.026089\n",
      "step 9081, loss: 0.026089\n",
      "step 9082, loss: 0.026089\n",
      "step 9083, loss: 0.026088\n",
      "step 9084, loss: 0.026088\n",
      "step 9085, loss: 0.026088\n",
      "step 9086, loss: 0.026088\n",
      "step 9087, loss: 0.026088\n",
      "step 9088, loss: 0.026088\n",
      "step 9089, loss: 0.026088\n",
      "step 9090, loss: 0.026088\n",
      "step 9091, loss: 0.026088\n",
      "step 9092, loss: 0.026088\n",
      "step 9093, loss: 0.026088\n",
      "step 9094, loss: 0.026088\n",
      "step 9095, loss: 0.026088\n",
      "step 9096, loss: 0.026088\n",
      "step 9097, loss: 0.026088\n",
      "step 9098, loss: 0.026088\n",
      "step 9099, loss: 0.026087\n",
      "step 9100, loss: 0.026087\n",
      "step 9101, loss: 0.026087\n",
      "step 9102, loss: 0.026087\n",
      "step 9103, loss: 0.026087\n",
      "step 9104, loss: 0.026087\n",
      "step 9105, loss: 0.026087\n",
      "step 9106, loss: 0.026087\n",
      "step 9107, loss: 0.026087\n",
      "step 9108, loss: 0.026087\n",
      "step 9109, loss: 0.026087\n",
      "step 9110, loss: 0.026087\n",
      "step 9111, loss: 0.026087\n",
      "step 9112, loss: 0.026087\n",
      "step 9113, loss: 0.026087\n",
      "step 9114, loss: 0.026087\n",
      "step 9115, loss: 0.026087\n",
      "step 9116, loss: 0.026086\n",
      "step 9117, loss: 0.026086\n",
      "step 9118, loss: 0.026086\n",
      "step 9119, loss: 0.026086\n",
      "step 9120, loss: 0.026086\n",
      "step 9121, loss: 0.026086\n",
      "step 9122, loss: 0.026086\n",
      "step 9123, loss: 0.026086\n",
      "step 9124, loss: 0.026086\n",
      "step 9125, loss: 0.026086\n",
      "step 9126, loss: 0.026086\n",
      "step 9127, loss: 0.026086\n",
      "step 9128, loss: 0.026086\n",
      "step 9129, loss: 0.026086\n",
      "step 9130, loss: 0.026086\n",
      "step 9131, loss: 0.026086\n",
      "step 9132, loss: 0.026086\n",
      "step 9133, loss: 0.026086\n",
      "step 9134, loss: 0.026086\n",
      "step 9135, loss: 0.026085\n",
      "step 9136, loss: 0.026085\n",
      "step 9137, loss: 0.026085\n",
      "step 9138, loss: 0.026085\n",
      "step 9139, loss: 0.026085\n",
      "step 9140, loss: 0.026085\n",
      "step 9141, loss: 0.026085\n",
      "step 9142, loss: 0.026085\n",
      "step 9143, loss: 0.026085\n",
      "step 9144, loss: 0.026085\n",
      "step 9145, loss: 0.026085\n",
      "step 9146, loss: 0.026085\n",
      "step 9147, loss: 0.026085\n",
      "step 9148, loss: 0.026085\n",
      "step 9149, loss: 0.026085\n",
      "step 9150, loss: 0.026085\n",
      "step 9151, loss: 0.026085\n",
      "step 9152, loss: 0.026085\n",
      "step 9153, loss: 0.026085\n",
      "step 9154, loss: 0.026085\n",
      "step 9155, loss: 0.026085\n",
      "step 9156, loss: 0.026084\n",
      "step 9157, loss: 0.026084\n",
      "step 9158, loss: 0.026084\n",
      "step 9159, loss: 0.026084\n",
      "step 9160, loss: 0.026084\n",
      "step 9161, loss: 0.026084\n",
      "step 9162, loss: 0.026084\n",
      "step 9163, loss: 0.026084\n",
      "step 9164, loss: 0.026084\n",
      "step 9165, loss: 0.026084\n",
      "step 9166, loss: 0.026084\n",
      "step 9167, loss: 0.026084\n",
      "step 9168, loss: 0.026084\n",
      "step 9169, loss: 0.026084\n",
      "step 9170, loss: 0.026084\n",
      "step 9171, loss: 0.026084\n",
      "step 9172, loss: 0.026084\n",
      "step 9173, loss: 0.026084\n",
      "step 9174, loss: 0.026084\n",
      "step 9175, loss: 0.026084\n",
      "step 9176, loss: 0.026084\n",
      "step 9177, loss: 0.026084\n",
      "step 9178, loss: 0.026084\n",
      "step 9179, loss: 0.026084\n",
      "step 9180, loss: 0.026083\n",
      "step 9181, loss: 0.026083\n",
      "step 9182, loss: 0.026083\n",
      "step 9183, loss: 0.026083\n",
      "step 9184, loss: 0.026083\n",
      "step 9185, loss: 0.026083\n",
      "step 9186, loss: 0.026083\n",
      "step 9187, loss: 0.026083\n",
      "step 9188, loss: 0.026083\n",
      "step 9189, loss: 0.026083\n",
      "step 9190, loss: 0.026083\n",
      "step 9191, loss: 0.026083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 9192, loss: 0.026083\n",
      "step 9193, loss: 0.026083\n",
      "step 9194, loss: 0.026083\n",
      "step 9195, loss: 0.026083\n",
      "step 9196, loss: 0.026083\n",
      "step 9197, loss: 0.026083\n",
      "step 9198, loss: 0.026083\n",
      "step 9199, loss: 0.026083\n",
      "step 9200, loss: 0.026083\n",
      "step 9201, loss: 0.026083\n",
      "step 9202, loss: 0.026083\n",
      "step 9203, loss: 0.026083\n",
      "step 9204, loss: 0.026083\n",
      "step 9205, loss: 0.026083\n",
      "step 9206, loss: 0.026083\n",
      "step 9207, loss: 0.026083\n",
      "step 9208, loss: 0.026082\n",
      "step 9209, loss: 0.026082\n",
      "step 9210, loss: 0.026082\n",
      "step 9211, loss: 0.026082\n",
      "step 9212, loss: 0.026082\n",
      "step 9213, loss: 0.026082\n",
      "step 9214, loss: 0.026082\n",
      "step 9215, loss: 0.026082\n",
      "step 9216, loss: 0.026082\n",
      "step 9217, loss: 0.026082\n",
      "step 9218, loss: 0.026082\n",
      "step 9219, loss: 0.026082\n",
      "step 9220, loss: 0.026082\n",
      "step 9221, loss: 0.026082\n",
      "step 9222, loss: 0.026082\n",
      "step 9223, loss: 0.026082\n",
      "step 9224, loss: 0.026082\n",
      "step 9225, loss: 0.026082\n",
      "step 9226, loss: 0.026082\n",
      "step 9227, loss: 0.026082\n",
      "step 9228, loss: 0.026082\n",
      "step 9229, loss: 0.026082\n",
      "step 9230, loss: 0.026082\n",
      "step 9231, loss: 0.026082\n",
      "step 9232, loss: 0.026082\n",
      "step 9233, loss: 0.026082\n",
      "step 9234, loss: 0.026082\n",
      "step 9235, loss: 0.026082\n",
      "step 9236, loss: 0.026082\n",
      "step 9237, loss: 0.026082\n",
      "step 9238, loss: 0.026082\n",
      "step 9239, loss: 0.026082\n",
      "step 9240, loss: 0.026082\n",
      "step 9241, loss: 0.026082\n",
      "step 9242, loss: 0.026082\n",
      "step 9243, loss: 0.026081\n",
      "step 9244, loss: 0.026081\n",
      "step 9245, loss: 0.026081\n",
      "step 9246, loss: 0.026081\n",
      "step 9247, loss: 0.026081\n",
      "step 9248, loss: 0.026081\n",
      "step 9249, loss: 0.026081\n",
      "step 9250, loss: 0.026081\n",
      "step 9251, loss: 0.026081\n",
      "step 9252, loss: 0.026081\n",
      "step 9253, loss: 0.026081\n",
      "step 9254, loss: 0.026081\n",
      "step 9255, loss: 0.026081\n",
      "step 9256, loss: 0.026081\n",
      "step 9257, loss: 0.026081\n",
      "step 9258, loss: 0.026081\n",
      "step 9259, loss: 0.026081\n",
      "step 9260, loss: 0.026081\n",
      "step 9261, loss: 0.026081\n",
      "step 9262, loss: 0.026081\n",
      "step 9263, loss: 0.026081\n",
      "step 9264, loss: 0.026081\n",
      "step 9265, loss: 0.026081\n",
      "step 9266, loss: 0.026081\n",
      "step 9267, loss: 0.026081\n",
      "step 9268, loss: 0.026081\n",
      "step 9269, loss: 0.026081\n",
      "step 9270, loss: 0.026081\n",
      "step 9271, loss: 0.026081\n",
      "step 9272, loss: 0.026081\n",
      "step 9273, loss: 0.026081\n",
      "step 9274, loss: 0.026081\n",
      "step 9275, loss: 0.026081\n",
      "step 9276, loss: 0.026081\n",
      "step 9277, loss: 0.026081\n",
      "step 9278, loss: 0.026081\n",
      "step 9279, loss: 0.026081\n",
      "step 9280, loss: 0.026081\n",
      "step 9281, loss: 0.026081\n",
      "step 9282, loss: 0.026081\n",
      "step 9283, loss: 0.026081\n",
      "step 9284, loss: 0.026080\n",
      "step 9285, loss: 0.026080\n",
      "step 9286, loss: 0.026080\n",
      "step 9287, loss: 0.026080\n",
      "step 9288, loss: 0.026080\n",
      "step 9289, loss: 0.026080\n",
      "step 9290, loss: 0.026080\n",
      "step 9291, loss: 0.026080\n",
      "step 9292, loss: 0.026080\n",
      "step 9293, loss: 0.026080\n",
      "step 9294, loss: 0.026080\n",
      "step 9295, loss: 0.026080\n",
      "step 9296, loss: 0.026080\n",
      "step 9297, loss: 0.026080\n",
      "step 9298, loss: 0.026080\n",
      "step 9299, loss: 0.026080\n",
      "step 9300, loss: 0.026080\n",
      "step 9301, loss: 0.026080\n",
      "step 9302, loss: 0.026080\n",
      "step 9303, loss: 0.026080\n",
      "step 9304, loss: 0.026080\n",
      "step 9305, loss: 0.026080\n",
      "step 9306, loss: 0.026080\n",
      "step 9307, loss: 0.026080\n",
      "step 9308, loss: 0.026080\n",
      "step 9309, loss: 0.026080\n",
      "step 9310, loss: 0.026080\n",
      "step 9311, loss: 0.026080\n",
      "step 9312, loss: 0.026080\n",
      "step 9313, loss: 0.026080\n",
      "step 9314, loss: 0.026080\n",
      "step 9315, loss: 0.026080\n",
      "step 9316, loss: 0.026080\n",
      "step 9317, loss: 0.026080\n",
      "step 9318, loss: 0.026080\n",
      "step 9319, loss: 0.026080\n",
      "step 9320, loss: 0.026080\n",
      "step 9321, loss: 0.026080\n",
      "step 9322, loss: 0.026080\n",
      "step 9323, loss: 0.026080\n",
      "step 9324, loss: 0.026080\n",
      "step 9325, loss: 0.026080\n",
      "step 9326, loss: 0.026080\n",
      "step 9327, loss: 0.026080\n",
      "step 9328, loss: 0.026080\n",
      "step 9329, loss: 0.026080\n",
      "step 9330, loss: 0.026080\n",
      "step 9331, loss: 0.026080\n",
      "step 9332, loss: 0.026080\n",
      "step 9333, loss: 0.026080\n",
      "step 9334, loss: 0.026080\n",
      "step 9335, loss: 0.026080\n",
      "step 9336, loss: 0.026080\n",
      "step 9337, loss: 0.026079\n",
      "step 9338, loss: 0.026079\n",
      "step 9339, loss: 0.026079\n",
      "step 9340, loss: 0.026079\n",
      "step 9341, loss: 0.026079\n",
      "step 9342, loss: 0.026079\n",
      "step 9343, loss: 0.026079\n",
      "step 9344, loss: 0.026079\n",
      "step 9345, loss: 0.026079\n",
      "step 9346, loss: 0.026079\n",
      "step 9347, loss: 0.026079\n",
      "step 9348, loss: 0.026079\n",
      "step 9349, loss: 0.026079\n",
      "step 9350, loss: 0.026079\n",
      "step 9351, loss: 0.026079\n",
      "step 9352, loss: 0.026079\n",
      "step 9353, loss: 0.026079\n",
      "step 9354, loss: 0.026079\n",
      "step 9355, loss: 0.026079\n",
      "step 9356, loss: 0.026079\n",
      "step 9357, loss: 0.026079\n",
      "step 9358, loss: 0.026079\n",
      "step 9359, loss: 0.026079\n",
      "step 9360, loss: 0.026079\n",
      "step 9361, loss: 0.026079\n",
      "step 9362, loss: 0.026079\n",
      "step 9363, loss: 0.026079\n",
      "step 9364, loss: 0.026079\n",
      "step 9365, loss: 0.026079\n",
      "step 9366, loss: 0.026079\n",
      "step 9367, loss: 0.026079\n",
      "step 9368, loss: 0.026079\n",
      "step 9369, loss: 0.026079\n",
      "step 9370, loss: 0.026079\n",
      "step 9371, loss: 0.026079\n",
      "step 9372, loss: 0.026079\n",
      "step 9373, loss: 0.026079\n",
      "step 9374, loss: 0.026079\n",
      "step 9375, loss: 0.026079\n",
      "step 9376, loss: 0.026079\n",
      "step 9377, loss: 0.026079\n",
      "step 9378, loss: 0.026079\n",
      "step 9379, loss: 0.026079\n",
      "step 9380, loss: 0.026079\n",
      "step 9381, loss: 0.026079\n",
      "step 9382, loss: 0.026079\n",
      "step 9383, loss: 0.026079\n",
      "step 9384, loss: 0.026079\n",
      "step 9385, loss: 0.026079\n",
      "step 9386, loss: 0.026079\n",
      "step 9387, loss: 0.026079\n",
      "step 9388, loss: 0.026079\n",
      "step 9389, loss: 0.026079\n",
      "step 9390, loss: 0.026079\n",
      "step 9391, loss: 0.026079\n",
      "step 9392, loss: 0.026079\n",
      "step 9393, loss: 0.026079\n",
      "step 9394, loss: 0.026079\n",
      "step 9395, loss: 0.026079\n",
      "step 9396, loss: 0.026079\n",
      "step 9397, loss: 0.026079\n",
      "step 9398, loss: 0.026079\n",
      "step 9399, loss: 0.026079\n",
      "step 9400, loss: 0.026079\n",
      "step 9401, loss: 0.026079\n",
      "step 9402, loss: 0.026079\n",
      "step 9403, loss: 0.026079\n",
      "step 9404, loss: 0.026079\n",
      "step 9405, loss: 0.026079\n",
      "step 9406, loss: 0.026079\n",
      "step 9407, loss: 0.026079\n",
      "step 9408, loss: 0.026079\n",
      "step 9409, loss: 0.026079\n",
      "step 9410, loss: 0.026079\n",
      "step 9411, loss: 0.026079\n",
      "step 9412, loss: 0.026079\n",
      "step 9413, loss: 0.026079\n",
      "step 9414, loss: 0.026079\n",
      "step 9415, loss: 0.026079\n",
      "step 9416, loss: 0.026079\n",
      "step 9417, loss: 0.026078\n",
      "step 9418, loss: 0.026079\n",
      "step 9419, loss: 0.026079\n",
      "step 9420, loss: 0.026078\n",
      "step 9421, loss: 0.026078\n",
      "step 9422, loss: 0.026078\n",
      "step 9423, loss: 0.026078\n",
      "step 9424, loss: 0.026078\n",
      "step 9425, loss: 0.026078\n",
      "step 9426, loss: 0.026078\n",
      "step 9427, loss: 0.026078\n",
      "step 9428, loss: 0.026078\n",
      "step 9429, loss: 0.026078\n",
      "step 9430, loss: 0.026078\n",
      "step 9431, loss: 0.026078\n",
      "step 9432, loss: 0.026078\n",
      "step 9433, loss: 0.026078\n",
      "step 9434, loss: 0.026078\n",
      "step 9435, loss: 0.026078\n",
      "step 9436, loss: 0.026078\n",
      "step 9437, loss: 0.026078\n",
      "step 9438, loss: 0.026078\n",
      "step 9439, loss: 0.026078\n",
      "step 9440, loss: 0.026078\n",
      "step 9441, loss: 0.026078\n",
      "step 9442, loss: 0.026078\n",
      "step 9443, loss: 0.026078\n",
      "step 9444, loss: 0.026078\n",
      "step 9445, loss: 0.026078\n",
      "step 9446, loss: 0.026078\n",
      "step 9447, loss: 0.026078\n",
      "step 9448, loss: 0.026078\n",
      "step 9449, loss: 0.026078\n",
      "step 9450, loss: 0.026078\n",
      "step 9451, loss: 0.026078\n",
      "step 9452, loss: 0.026078\n",
      "step 9453, loss: 0.026078\n",
      "step 9454, loss: 0.026078\n",
      "step 9455, loss: 0.026078\n",
      "step 9456, loss: 0.026078\n",
      "step 9457, loss: 0.026078\n",
      "step 9458, loss: 0.026078\n",
      "step 9459, loss: 0.026078\n",
      "step 9460, loss: 0.026078\n",
      "step 9461, loss: 0.026078\n",
      "step 9462, loss: 0.026078\n",
      "step 9463, loss: 0.026078\n",
      "step 9464, loss: 0.026078\n",
      "step 9465, loss: 0.026078\n",
      "step 9466, loss: 0.026078\n",
      "step 9467, loss: 0.026078\n",
      "step 9468, loss: 0.026078\n",
      "step 9469, loss: 0.026078\n",
      "step 9470, loss: 0.026078\n",
      "step 9471, loss: 0.026078\n",
      "step 9472, loss: 0.026078\n",
      "step 9473, loss: 0.026078\n",
      "step 9474, loss: 0.026078\n",
      "step 9475, loss: 0.026078\n",
      "step 9476, loss: 0.026078\n",
      "step 9477, loss: 0.026078\n",
      "step 9478, loss: 0.026078\n",
      "step 9479, loss: 0.026078\n",
      "step 9480, loss: 0.026078\n",
      "step 9481, loss: 0.026078\n",
      "step 9482, loss: 0.026078\n",
      "step 9483, loss: 0.026078\n",
      "step 9484, loss: 0.026078\n",
      "step 9485, loss: 0.026078\n",
      "step 9486, loss: 0.026078\n",
      "step 9487, loss: 0.026078\n",
      "step 9488, loss: 0.026078\n",
      "step 9489, loss: 0.026078\n",
      "step 9490, loss: 0.026078\n",
      "step 9491, loss: 0.026078\n",
      "step 9492, loss: 0.026078\n",
      "step 9493, loss: 0.026078\n",
      "step 9494, loss: 0.026078\n",
      "step 9495, loss: 0.026078\n",
      "step 9496, loss: 0.026078\n",
      "step 9497, loss: 0.026078\n",
      "step 9498, loss: 0.026078\n",
      "step 9499, loss: 0.026078\n",
      "step 9500, loss: 0.026078\n",
      "step 9501, loss: 0.026078\n",
      "step 9502, loss: 0.026078\n",
      "step 9503, loss: 0.026078\n",
      "step 9504, loss: 0.026078\n",
      "step 9505, loss: 0.026078\n",
      "step 9506, loss: 0.026078\n",
      "step 9507, loss: 0.026078\n",
      "step 9508, loss: 0.026078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 9509, loss: 0.026078\n",
      "step 9510, loss: 0.026078\n",
      "step 9511, loss: 0.026078\n",
      "step 9512, loss: 0.026078\n",
      "step 9513, loss: 0.026078\n",
      "step 9514, loss: 0.026078\n",
      "step 9515, loss: 0.026078\n",
      "step 9516, loss: 0.026078\n",
      "step 9517, loss: 0.026078\n",
      "step 9518, loss: 0.026078\n",
      "step 9519, loss: 0.026078\n",
      "step 9520, loss: 0.026078\n",
      "step 9521, loss: 0.026078\n",
      "step 9522, loss: 0.026078\n",
      "step 9523, loss: 0.026078\n",
      "step 9524, loss: 0.026078\n",
      "step 9525, loss: 0.026078\n",
      "step 9526, loss: 0.026078\n",
      "step 9527, loss: 0.026078\n",
      "step 9528, loss: 0.026078\n",
      "step 9529, loss: 0.026078\n",
      "step 9530, loss: 0.026078\n",
      "step 9531, loss: 0.026078\n",
      "step 9532, loss: 0.026078\n",
      "step 9533, loss: 0.026078\n",
      "step 9534, loss: 0.026078\n",
      "step 9535, loss: 0.026078\n",
      "step 9536, loss: 0.026078\n",
      "step 9537, loss: 0.026078\n",
      "step 9538, loss: 0.026078\n",
      "step 9539, loss: 0.026078\n",
      "step 9540, loss: 0.026078\n",
      "step 9541, loss: 0.026078\n",
      "step 9542, loss: 0.026078\n",
      "step 9543, loss: 0.026078\n",
      "step 9544, loss: 0.026078\n",
      "step 9545, loss: 0.026078\n",
      "step 9546, loss: 0.026078\n",
      "step 9547, loss: 0.026078\n",
      "step 9548, loss: 0.026078\n",
      "step 9549, loss: 0.026078\n",
      "step 9550, loss: 0.026078\n",
      "step 9551, loss: 0.026078\n",
      "step 9552, loss: 0.026078\n",
      "step 9553, loss: 0.026078\n",
      "step 9554, loss: 0.026078\n",
      "step 9555, loss: 0.026078\n",
      "step 9556, loss: 0.026078\n",
      "step 9557, loss: 0.026078\n",
      "step 9558, loss: 0.026078\n",
      "step 9559, loss: 0.026078\n",
      "step 9560, loss: 0.026078\n",
      "step 9561, loss: 0.026078\n",
      "step 9562, loss: 0.026078\n",
      "step 9563, loss: 0.026078\n",
      "step 9564, loss: 0.026078\n",
      "step 9565, loss: 0.026078\n",
      "step 9566, loss: 0.026078\n",
      "step 9567, loss: 0.026078\n",
      "step 9568, loss: 0.026078\n",
      "step 9569, loss: 0.026078\n",
      "step 9570, loss: 0.026078\n",
      "step 9571, loss: 0.026078\n",
      "step 9572, loss: 0.026078\n",
      "step 9573, loss: 0.026078\n",
      "step 9574, loss: 0.026078\n",
      "step 9575, loss: 0.026078\n",
      "step 9576, loss: 0.026078\n",
      "step 9577, loss: 0.026078\n",
      "step 9578, loss: 0.026078\n",
      "step 9579, loss: 0.026078\n",
      "step 9580, loss: 0.026078\n",
      "step 9581, loss: 0.026078\n",
      "step 9582, loss: 0.026078\n",
      "step 9583, loss: 0.026078\n",
      "step 9584, loss: 0.026078\n",
      "step 9585, loss: 0.026078\n",
      "step 9586, loss: 0.026078\n",
      "step 9587, loss: 0.026078\n",
      "step 9588, loss: 0.026078\n",
      "step 9589, loss: 0.026078\n",
      "step 9590, loss: 0.026078\n",
      "step 9591, loss: 0.026078\n",
      "step 9592, loss: 0.026077\n",
      "step 9593, loss: 0.026077\n",
      "step 9594, loss: 0.026078\n",
      "step 9595, loss: 0.026078\n",
      "step 9596, loss: 0.026078\n",
      "step 9597, loss: 0.026077\n",
      "step 9598, loss: 0.026078\n",
      "step 9599, loss: 0.026078\n",
      "step 9600, loss: 0.026078\n",
      "step 9601, loss: 0.026077\n",
      "step 9602, loss: 0.026078\n",
      "step 9603, loss: 0.026077\n",
      "step 9604, loss: 0.026077\n",
      "step 9605, loss: 0.026077\n",
      "step 9606, loss: 0.026077\n",
      "step 9607, loss: 0.026077\n",
      "step 9608, loss: 0.026077\n",
      "step 9609, loss: 0.026078\n",
      "step 9610, loss: 0.026077\n",
      "step 9611, loss: 0.026077\n",
      "step 9612, loss: 0.026077\n",
      "step 9613, loss: 0.026077\n",
      "step 9614, loss: 0.026077\n",
      "step 9615, loss: 0.026077\n",
      "step 9616, loss: 0.026077\n",
      "step 9617, loss: 0.026077\n",
      "step 9618, loss: 0.026077\n",
      "step 9619, loss: 0.026077\n",
      "step 9620, loss: 0.026077\n",
      "step 9621, loss: 0.026077\n",
      "step 9622, loss: 0.026077\n",
      "step 9623, loss: 0.026077\n",
      "step 9624, loss: 0.026077\n",
      "step 9625, loss: 0.026077\n",
      "step 9626, loss: 0.026077\n",
      "step 9627, loss: 0.026077\n",
      "step 9628, loss: 0.026077\n",
      "step 9629, loss: 0.026077\n",
      "step 9630, loss: 0.026077\n",
      "step 9631, loss: 0.026077\n",
      "step 9632, loss: 0.026077\n",
      "step 9633, loss: 0.026077\n",
      "step 9634, loss: 0.026077\n",
      "step 9635, loss: 0.026077\n",
      "step 9636, loss: 0.026077\n",
      "step 9637, loss: 0.026077\n",
      "step 9638, loss: 0.026077\n",
      "step 9639, loss: 0.026077\n",
      "step 9640, loss: 0.026077\n",
      "step 9641, loss: 0.026077\n",
      "step 9642, loss: 0.026077\n",
      "step 9643, loss: 0.026077\n",
      "step 9644, loss: 0.026077\n",
      "step 9645, loss: 0.026077\n",
      "step 9646, loss: 0.026077\n",
      "step 9647, loss: 0.026077\n",
      "step 9648, loss: 0.026077\n",
      "step 9649, loss: 0.026077\n",
      "step 9650, loss: 0.026077\n",
      "step 9651, loss: 0.026077\n",
      "step 9652, loss: 0.026077\n",
      "step 9653, loss: 0.026077\n",
      "step 9654, loss: 0.026077\n",
      "step 9655, loss: 0.026077\n",
      "step 9656, loss: 0.026077\n",
      "step 9657, loss: 0.026077\n",
      "step 9658, loss: 0.026077\n",
      "step 9659, loss: 0.026077\n",
      "step 9660, loss: 0.026077\n",
      "step 9661, loss: 0.026077\n",
      "step 9662, loss: 0.026077\n",
      "step 9663, loss: 0.026077\n",
      "step 9664, loss: 0.026077\n",
      "step 9665, loss: 0.026077\n",
      "step 9666, loss: 0.026077\n",
      "step 9667, loss: 0.026077\n",
      "step 9668, loss: 0.026077\n",
      "step 9669, loss: 0.026077\n",
      "step 9670, loss: 0.026077\n",
      "step 9671, loss: 0.026077\n",
      "step 9672, loss: 0.026077\n",
      "step 9673, loss: 0.026077\n",
      "step 9674, loss: 0.026077\n",
      "step 9675, loss: 0.026077\n",
      "step 9676, loss: 0.026077\n",
      "step 9677, loss: 0.026077\n",
      "step 9678, loss: 0.026077\n",
      "step 9679, loss: 0.026077\n",
      "step 9680, loss: 0.026077\n",
      "step 9681, loss: 0.026077\n",
      "step 9682, loss: 0.026077\n",
      "step 9683, loss: 0.026077\n",
      "step 9684, loss: 0.026077\n",
      "step 9685, loss: 0.026077\n",
      "step 9686, loss: 0.026077\n",
      "step 9687, loss: 0.026077\n",
      "step 9688, loss: 0.026077\n",
      "step 9689, loss: 0.026077\n",
      "step 9690, loss: 0.026077\n",
      "step 9691, loss: 0.026077\n",
      "step 9692, loss: 0.026077\n",
      "step 9693, loss: 0.026077\n",
      "step 9694, loss: 0.026077\n",
      "step 9695, loss: 0.026077\n",
      "step 9696, loss: 0.026077\n",
      "step 9697, loss: 0.026077\n",
      "step 9698, loss: 0.026077\n",
      "step 9699, loss: 0.026077\n",
      "step 9700, loss: 0.026077\n",
      "step 9701, loss: 0.026077\n",
      "step 9702, loss: 0.026077\n",
      "step 9703, loss: 0.026077\n",
      "step 9704, loss: 0.026077\n",
      "step 9705, loss: 0.026077\n",
      "step 9706, loss: 0.026077\n",
      "step 9707, loss: 0.026077\n",
      "step 9708, loss: 0.026077\n",
      "step 9709, loss: 0.026077\n",
      "step 9710, loss: 0.026077\n",
      "step 9711, loss: 0.026077\n",
      "step 9712, loss: 0.026077\n",
      "step 9713, loss: 0.026077\n",
      "step 9714, loss: 0.026077\n",
      "step 9715, loss: 0.026077\n",
      "step 9716, loss: 0.026077\n",
      "step 9717, loss: 0.026077\n",
      "step 9718, loss: 0.026077\n",
      "step 9719, loss: 0.026077\n",
      "step 9720, loss: 0.026077\n",
      "step 9721, loss: 0.026077\n",
      "step 9722, loss: 0.026077\n",
      "step 9723, loss: 0.026077\n",
      "step 9724, loss: 0.026077\n",
      "step 9725, loss: 0.026077\n",
      "step 9726, loss: 0.026077\n",
      "step 9727, loss: 0.026077\n",
      "step 9728, loss: 0.026077\n",
      "step 9729, loss: 0.026077\n",
      "step 9730, loss: 0.026077\n",
      "step 9731, loss: 0.026077\n",
      "step 9732, loss: 0.026077\n",
      "step 9733, loss: 0.026077\n",
      "step 9734, loss: 0.026077\n",
      "step 9735, loss: 0.026077\n",
      "step 9736, loss: 0.026077\n",
      "step 9737, loss: 0.026077\n",
      "step 9738, loss: 0.026077\n",
      "step 9739, loss: 0.026077\n",
      "step 9740, loss: 0.026077\n",
      "step 9741, loss: 0.026077\n",
      "step 9742, loss: 0.026077\n",
      "step 9743, loss: 0.026077\n",
      "step 9744, loss: 0.026077\n",
      "step 9745, loss: 0.026077\n",
      "step 9746, loss: 0.026077\n",
      "step 9747, loss: 0.026077\n",
      "step 9748, loss: 0.026077\n",
      "step 9749, loss: 0.026077\n",
      "step 9750, loss: 0.026077\n",
      "step 9751, loss: 0.026077\n",
      "step 9752, loss: 0.026077\n",
      "step 9753, loss: 0.026077\n",
      "step 9754, loss: 0.026077\n",
      "step 9755, loss: 0.026077\n",
      "step 9756, loss: 0.026077\n",
      "step 9757, loss: 0.026077\n",
      "step 9758, loss: 0.026077\n",
      "step 9759, loss: 0.026077\n",
      "step 9760, loss: 0.026077\n",
      "step 9761, loss: 0.026077\n",
      "step 9762, loss: 0.026077\n",
      "step 9763, loss: 0.026077\n",
      "step 9764, loss: 0.026077\n",
      "step 9765, loss: 0.026077\n",
      "step 9766, loss: 0.026077\n",
      "step 9767, loss: 0.026077\n",
      "step 9768, loss: 0.026077\n",
      "step 9769, loss: 0.026077\n",
      "step 9770, loss: 0.026077\n",
      "step 9771, loss: 0.026077\n",
      "step 9772, loss: 0.026077\n",
      "step 9773, loss: 0.026077\n",
      "step 9774, loss: 0.026077\n",
      "step 9775, loss: 0.026077\n",
      "step 9776, loss: 0.026077\n",
      "step 9777, loss: 0.026077\n",
      "step 9778, loss: 0.026077\n",
      "step 9779, loss: 0.026077\n",
      "step 9780, loss: 0.026077\n",
      "step 9781, loss: 0.026077\n",
      "step 9782, loss: 0.026077\n",
      "step 9783, loss: 0.026077\n",
      "step 9784, loss: 0.026077\n",
      "step 9785, loss: 0.026077\n",
      "step 9786, loss: 0.026077\n",
      "step 9787, loss: 0.026077\n",
      "step 9788, loss: 0.026077\n",
      "step 9789, loss: 0.026077\n",
      "step 9790, loss: 0.026077\n",
      "step 9791, loss: 0.026077\n",
      "step 9792, loss: 0.026077\n",
      "step 9793, loss: 0.026077\n",
      "step 9794, loss: 0.026077\n",
      "step 9795, loss: 0.026077\n",
      "step 9796, loss: 0.026077\n",
      "step 9797, loss: 0.026077\n",
      "step 9798, loss: 0.026077\n",
      "step 9799, loss: 0.026077\n",
      "step 9800, loss: 0.026077\n",
      "step 9801, loss: 0.026077\n",
      "step 9802, loss: 0.026077\n",
      "step 9803, loss: 0.026077\n",
      "step 9804, loss: 0.026077\n",
      "step 9805, loss: 0.026077\n",
      "step 9806, loss: 0.026077\n",
      "step 9807, loss: 0.026077\n",
      "step 9808, loss: 0.026077\n",
      "step 9809, loss: 0.026077\n",
      "step 9810, loss: 0.026077\n",
      "step 9811, loss: 0.026077\n",
      "step 9812, loss: 0.026077\n",
      "step 9813, loss: 0.026077\n",
      "step 9814, loss: 0.026077\n",
      "step 9815, loss: 0.026077\n",
      "step 9816, loss: 0.026077\n",
      "step 9817, loss: 0.026077\n",
      "step 9818, loss: 0.026077\n",
      "step 9819, loss: 0.026077\n",
      "step 9820, loss: 0.026077\n",
      "step 9821, loss: 0.026077\n",
      "step 9822, loss: 0.026077\n",
      "step 9823, loss: 0.026077\n",
      "step 9824, loss: 0.026077\n",
      "step 9825, loss: 0.026077\n",
      "step 9826, loss: 0.026077\n",
      "step 9827, loss: 0.026077\n",
      "step 9828, loss: 0.026077\n",
      "step 9829, loss: 0.026077\n",
      "step 9830, loss: 0.026077\n",
      "step 9831, loss: 0.026077\n",
      "step 9832, loss: 0.026077\n",
      "step 9833, loss: 0.026077\n",
      "step 9834, loss: 0.026077\n",
      "step 9835, loss: 0.026077\n",
      "step 9836, loss: 0.026077\n",
      "step 9837, loss: 0.026077\n",
      "step 9838, loss: 0.026077\n",
      "step 9839, loss: 0.026077\n",
      "step 9840, loss: 0.026077\n",
      "step 9841, loss: 0.026077\n",
      "step 9842, loss: 0.026077\n",
      "step 9843, loss: 0.026077\n",
      "step 9844, loss: 0.026077\n",
      "step 9845, loss: 0.026077\n",
      "step 9846, loss: 0.026077\n",
      "step 9847, loss: 0.026077\n",
      "step 9848, loss: 0.026077\n",
      "step 9849, loss: 0.026077\n",
      "step 9850, loss: 0.026077\n",
      "step 9851, loss: 0.026077\n",
      "step 9852, loss: 0.026077\n",
      "step 9853, loss: 0.026077\n",
      "step 9854, loss: 0.026077\n",
      "step 9855, loss: 0.026077\n",
      "step 9856, loss: 0.026077\n",
      "step 9857, loss: 0.026077\n",
      "step 9858, loss: 0.026077\n",
      "step 9859, loss: 0.026077\n",
      "step 9860, loss: 0.026077\n",
      "step 9861, loss: 0.026077\n",
      "step 9862, loss: 0.026077\n",
      "step 9863, loss: 0.026077\n",
      "step 9864, loss: 0.026077\n",
      "step 9865, loss: 0.026077\n",
      "step 9866, loss: 0.026077\n",
      "step 9867, loss: 0.026077\n",
      "step 9868, loss: 0.026077\n",
      "step 9869, loss: 0.026077\n",
      "step 9870, loss: 0.026077\n",
      "step 9871, loss: 0.026077\n",
      "step 9872, loss: 0.026077\n",
      "step 9873, loss: 0.026077\n",
      "step 9874, loss: 0.026077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 9875, loss: 0.026077\n",
      "step 9876, loss: 0.026077\n",
      "step 9877, loss: 0.026077\n",
      "step 9878, loss: 0.026077\n",
      "step 9879, loss: 0.026077\n",
      "step 9880, loss: 0.026077\n",
      "step 9881, loss: 0.026077\n",
      "step 9882, loss: 0.026077\n",
      "step 9883, loss: 0.026077\n",
      "step 9884, loss: 0.026077\n",
      "step 9885, loss: 0.026077\n",
      "step 9886, loss: 0.026077\n",
      "step 9887, loss: 0.026077\n",
      "step 9888, loss: 0.026077\n",
      "step 9889, loss: 0.026077\n",
      "step 9890, loss: 0.026077\n",
      "step 9891, loss: 0.026077\n",
      "step 9892, loss: 0.026077\n",
      "step 9893, loss: 0.026077\n",
      "step 9894, loss: 0.026077\n",
      "step 9895, loss: 0.026077\n",
      "step 9896, loss: 0.026077\n",
      "step 9897, loss: 0.026077\n",
      "step 9898, loss: 0.026077\n",
      "step 9899, loss: 0.026077\n",
      "step 9900, loss: 0.026077\n",
      "step 9901, loss: 0.026077\n",
      "step 9902, loss: 0.026077\n",
      "step 9903, loss: 0.026077\n",
      "step 9904, loss: 0.026077\n",
      "step 9905, loss: 0.026077\n",
      "step 9906, loss: 0.026077\n",
      "step 9907, loss: 0.026077\n",
      "step 9908, loss: 0.026077\n",
      "step 9909, loss: 0.026077\n",
      "step 9910, loss: 0.026077\n",
      "step 9911, loss: 0.026077\n",
      "step 9912, loss: 0.026077\n",
      "step 9913, loss: 0.026077\n",
      "step 9914, loss: 0.026077\n",
      "step 9915, loss: 0.026077\n",
      "step 9916, loss: 0.026077\n",
      "step 9917, loss: 0.026077\n",
      "step 9918, loss: 0.026077\n",
      "step 9919, loss: 0.026077\n",
      "step 9920, loss: 0.026077\n",
      "step 9921, loss: 0.026077\n",
      "step 9922, loss: 0.026077\n",
      "step 9923, loss: 0.026077\n",
      "step 9924, loss: 0.026077\n",
      "step 9925, loss: 0.026077\n",
      "step 9926, loss: 0.026077\n",
      "step 9927, loss: 0.026077\n",
      "step 9928, loss: 0.026077\n",
      "step 9929, loss: 0.026077\n",
      "step 9930, loss: 0.026077\n",
      "step 9931, loss: 0.026077\n",
      "step 9932, loss: 0.026077\n",
      "step 9933, loss: 0.026077\n",
      "step 9934, loss: 0.026077\n",
      "step 9935, loss: 0.026077\n",
      "step 9936, loss: 0.026077\n",
      "step 9937, loss: 0.026077\n",
      "step 9938, loss: 0.026077\n",
      "step 9939, loss: 0.026077\n",
      "step 9940, loss: 0.026077\n",
      "step 9941, loss: 0.026077\n",
      "step 9942, loss: 0.026077\n",
      "step 9943, loss: 0.026077\n",
      "step 9944, loss: 0.026077\n",
      "step 9945, loss: 0.026077\n",
      "step 9946, loss: 0.026077\n",
      "step 9947, loss: 0.026077\n",
      "step 9948, loss: 0.026077\n",
      "step 9949, loss: 0.026077\n",
      "step 9950, loss: 0.026077\n",
      "step 9951, loss: 0.026077\n",
      "step 9952, loss: 0.026077\n",
      "step 9953, loss: 0.026077\n",
      "step 9954, loss: 0.026077\n",
      "step 9955, loss: 0.026077\n",
      "step 9956, loss: 0.026077\n",
      "step 9957, loss: 0.026077\n",
      "step 9958, loss: 0.026077\n",
      "step 9959, loss: 0.026077\n",
      "step 9960, loss: 0.026077\n",
      "step 9961, loss: 0.026077\n",
      "step 9962, loss: 0.026077\n",
      "step 9963, loss: 0.026077\n",
      "step 9964, loss: 0.026077\n",
      "step 9965, loss: 0.026077\n",
      "step 9966, loss: 0.026077\n",
      "step 9967, loss: 0.026077\n",
      "step 9968, loss: 0.026077\n",
      "step 9969, loss: 0.026077\n",
      "step 9970, loss: 0.026077\n",
      "step 9971, loss: 0.026077\n",
      "step 9972, loss: 0.026077\n",
      "step 9973, loss: 0.026077\n",
      "step 9974, loss: 0.026077\n",
      "step 9975, loss: 0.026077\n",
      "step 9976, loss: 0.026077\n",
      "step 9977, loss: 0.026077\n",
      "step 9978, loss: 0.026077\n",
      "step 9979, loss: 0.026077\n",
      "step 9980, loss: 0.026077\n",
      "step 9981, loss: 0.026077\n",
      "step 9982, loss: 0.026077\n",
      "step 9983, loss: 0.026077\n",
      "step 9984, loss: 0.026077\n",
      "step 9985, loss: 0.026077\n",
      "step 9986, loss: 0.026077\n",
      "step 9987, loss: 0.026077\n",
      "step 9988, loss: 0.026077\n",
      "step 9989, loss: 0.026077\n",
      "step 9990, loss: 0.026077\n",
      "step 9991, loss: 0.026077\n",
      "step 9992, loss: 0.026077\n",
      "step 9993, loss: 0.026077\n",
      "step 9994, loss: 0.026077\n",
      "step 9995, loss: 0.026077\n",
      "step 9996, loss: 0.026077\n",
      "step 9997, loss: 0.026077\n",
      "step 9998, loss: 0.026077\n",
      "step 9999, loss: 0.026077\n"
     ]
    }
   ],
   "source": [
    "iteration = 10000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Training the model\n",
    "    for i in range(iteration):\n",
    "        feed_dict = {x:x_normal,y:y_normal}\n",
    "        _, summary, loss = sess.run([train_op, merged, l], feed_dict=feed_dict)\n",
    "        print(\"step %d, loss: %f\" % (i, loss))\n",
    "        train_writer.add_summary(summary, i)\n",
    "\n",
    "    # Get weights\n",
    "    a,b,c,d,e = sess.run([a,b,c,d,e])\n",
    "    \n",
    "    # Make Predictions\n",
    "    y_pred_final = sess.run(y_pred, feed_dict={x: x_normal})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualising the Polynomial Regression results\n",
    "X_grid = np.arange(min(x_normal), max(x_normal), 0.1)#for better graph\n",
    "X_grid = X_grid.reshape((len(X_grid), 1))\n",
    "plt.scatter(x_normal,y_normal,color = 'red')\n",
    "plt.plot(x_normal,y_pred_final,color = 'blue')\n",
    "plt.title('Polynomial Regression')\n",
    "plt.xlabel('Position level')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

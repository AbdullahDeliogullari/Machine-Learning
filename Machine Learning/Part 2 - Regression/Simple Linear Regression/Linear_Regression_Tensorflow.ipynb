{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30, 1), (30,), 30)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "dataset=pd.read_csv('Salary_Data.csv')\n",
    "\n",
    "# Import dataset\n",
    "# Placed at x-y axis\n",
    "x_true = dataset.iloc[:,:-1].values\n",
    "y_true = dataset.iloc[:,1].values\n",
    "N = np.size(x_true)\n",
    "\n",
    "x_true.shape,y_true.shape,N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the training data\n",
    "plt.scatter(x_true,y_true)\n",
    "plt.title(\"Experience - Salary Dataset\")\n",
    "plt.xlabel(\"x values\")\n",
    "plt.ylabel(\"y values\")\n",
    "plt.savefig(\"train_data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30, 1), (30,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize the training data\n",
    "x_mean = x_true.mean()\n",
    "x_std = x_true.std()\n",
    "\n",
    "y_mean = y_true.mean()\n",
    "y_std = y_true.std()\n",
    "\n",
    "x_normal = (x_true - x_mean) / x_std\n",
    "y_normal = (y_true - y_mean) / y_std\n",
    "\n",
    "x_normal.shape,y_normal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'placeholders/Placeholder:0' shape=(30, 1) dtype=float32>,\n",
       " <tf.Tensor 'placeholders/Placeholder_1:0' shape=(30,) dtype=float32>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Placeholders\n",
    "with tf.name_scope(\"placeholders\"):\n",
    "    x = tf.placeholder(tf.float32,(N,1))\n",
    "    y = tf.placeholder(tf.float32,(N,))\n",
    "x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Variable 'weights/Variable:0' shape=(1, 1) dtype=float32_ref>,\n",
       " <tf.Variable 'weights/Variable_1:0' shape=(1,) dtype=float32_ref>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weights --> m and n\n",
    "with tf.name_scope(\"weights\"):\n",
    "    m = tf.Variable(tf.random_normal((1,1)))\n",
    "    n = tf.Variable(tf.random_normal((1,)))\n",
    "m,n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'prediction/add:0' shape=(30, 1) dtype=float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y = m*x + n\n",
    "with tf.name_scope(\"prediction\"):\n",
    "    y_pred = tf.matmul(x,m) + n\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'loss/Sum:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Squared error --> (y - y_pred) ** 2\n",
    "with tf.name_scope(\"loss\"):\n",
    "    l = tf.reduce_sum((y - tf.squeeze(y_pred))**2)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Operation 'optim/Adam' type=NoOp>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adam Optimizer minimizing the squared error\n",
    "with tf.name_scope(\"optim\"):\n",
    "    train_op = tf.train.AdamOptimizer(.001).minimize(l)\n",
    "train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"summaries\"):\n",
    "    tf.summary.scalar(\"loss\",l)\n",
    "    merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter('/tensor_flow/linear_regression', tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 160.060425\n",
      "step 1, loss: 159.898315\n",
      "step 2, loss: 159.736328\n",
      "step 3, loss: 159.574493\n",
      "step 4, loss: 159.412750\n",
      "step 5, loss: 159.251144\n",
      "step 6, loss: 159.089661\n",
      "step 7, loss: 158.928329\n",
      "step 8, loss: 158.767120\n",
      "step 9, loss: 158.606064\n",
      "step 10, loss: 158.445129\n",
      "step 11, loss: 158.284348\n",
      "step 12, loss: 158.123688\n",
      "step 13, loss: 157.963196\n",
      "step 14, loss: 157.802856\n",
      "step 15, loss: 157.642654\n",
      "step 16, loss: 157.482605\n",
      "step 17, loss: 157.322678\n",
      "step 18, loss: 157.162949\n",
      "step 19, loss: 157.003342\n",
      "step 20, loss: 156.843887\n",
      "step 21, loss: 156.684601\n",
      "step 22, loss: 156.525467\n",
      "step 23, loss: 156.366501\n",
      "step 24, loss: 156.207672\n",
      "step 25, loss: 156.049042\n",
      "step 26, loss: 155.890503\n",
      "step 27, loss: 155.732178\n",
      "step 28, loss: 155.574020\n",
      "step 29, loss: 155.416031\n",
      "step 30, loss: 155.258179\n",
      "step 31, loss: 155.100494\n",
      "step 32, loss: 154.942978\n",
      "step 33, loss: 154.785645\n",
      "step 34, loss: 154.628464\n",
      "step 35, loss: 154.471436\n",
      "step 36, loss: 154.314590\n",
      "step 37, loss: 154.157928\n",
      "step 38, loss: 154.001434\n",
      "step 39, loss: 153.845093\n",
      "step 40, loss: 153.688904\n",
      "step 41, loss: 153.532883\n",
      "step 42, loss: 153.377060\n",
      "step 43, loss: 153.221405\n",
      "step 44, loss: 153.065903\n",
      "step 45, loss: 152.910614\n",
      "step 46, loss: 152.755432\n",
      "step 47, loss: 152.600449\n",
      "step 48, loss: 152.445633\n",
      "step 49, loss: 152.290970\n",
      "step 50, loss: 152.136505\n",
      "step 51, loss: 151.982208\n",
      "step 52, loss: 151.828064\n",
      "step 53, loss: 151.674118\n",
      "step 54, loss: 151.520325\n",
      "step 55, loss: 151.366684\n",
      "step 56, loss: 151.213211\n",
      "step 57, loss: 151.059921\n",
      "step 58, loss: 150.906799\n",
      "step 59, loss: 150.753860\n",
      "step 60, loss: 150.601074\n",
      "step 61, loss: 150.448471\n",
      "step 62, loss: 150.296021\n",
      "step 63, loss: 150.143784\n",
      "step 64, loss: 149.991699\n",
      "step 65, loss: 149.839737\n",
      "step 66, loss: 149.687973\n",
      "step 67, loss: 149.536377\n",
      "step 68, loss: 149.384933\n",
      "step 69, loss: 149.233658\n",
      "step 70, loss: 149.082565\n",
      "step 71, loss: 148.931641\n",
      "step 72, loss: 148.780899\n",
      "step 73, loss: 148.630295\n",
      "step 74, loss: 148.479889\n",
      "step 75, loss: 148.329636\n",
      "step 76, loss: 148.179581\n",
      "step 77, loss: 148.029617\n",
      "step 78, loss: 147.879852\n",
      "step 79, loss: 147.730240\n",
      "step 80, loss: 147.580811\n",
      "step 81, loss: 147.431549\n",
      "step 82, loss: 147.282440\n",
      "step 83, loss: 147.133514\n",
      "step 84, loss: 146.984756\n",
      "step 85, loss: 146.836166\n",
      "step 86, loss: 146.687729\n",
      "step 87, loss: 146.539459\n",
      "step 88, loss: 146.391373\n",
      "step 89, loss: 146.243439\n",
      "step 90, loss: 146.095642\n",
      "step 91, loss: 145.948013\n",
      "step 92, loss: 145.800552\n",
      "step 93, loss: 145.653244\n",
      "step 94, loss: 145.506104\n",
      "step 95, loss: 145.359161\n",
      "step 96, loss: 145.212326\n",
      "step 97, loss: 145.065704\n",
      "step 98, loss: 144.919220\n",
      "step 99, loss: 144.772919\n",
      "step 100, loss: 144.626770\n",
      "step 101, loss: 144.480789\n",
      "step 102, loss: 144.334961\n",
      "step 103, loss: 144.189270\n",
      "step 104, loss: 144.043747\n",
      "step 105, loss: 143.898392\n",
      "step 106, loss: 143.753189\n",
      "step 107, loss: 143.608139\n",
      "step 108, loss: 143.463272\n",
      "step 109, loss: 143.318558\n",
      "step 110, loss: 143.173996\n",
      "step 111, loss: 143.029602\n",
      "step 112, loss: 142.885376\n",
      "step 113, loss: 142.741318\n",
      "step 114, loss: 142.597412\n",
      "step 115, loss: 142.453659\n",
      "step 116, loss: 142.310043\n",
      "step 117, loss: 142.166580\n",
      "step 118, loss: 142.023270\n",
      "step 119, loss: 141.880142\n",
      "step 120, loss: 141.737152\n",
      "step 121, loss: 141.594330\n",
      "step 122, loss: 141.451675\n",
      "step 123, loss: 141.309174\n",
      "step 124, loss: 141.166824\n",
      "step 125, loss: 141.024643\n",
      "step 126, loss: 140.882629\n",
      "step 127, loss: 140.740753\n",
      "step 128, loss: 140.599014\n",
      "step 129, loss: 140.457443\n",
      "step 130, loss: 140.316025\n",
      "step 131, loss: 140.174728\n",
      "step 132, loss: 140.033615\n",
      "step 133, loss: 139.892670\n",
      "step 134, loss: 139.751877\n",
      "step 135, loss: 139.611221\n",
      "step 136, loss: 139.470718\n",
      "step 137, loss: 139.330383\n",
      "step 138, loss: 139.190216\n",
      "step 139, loss: 139.050186\n",
      "step 140, loss: 138.910309\n",
      "step 141, loss: 138.770584\n",
      "step 142, loss: 138.630981\n",
      "step 143, loss: 138.491547\n",
      "step 144, loss: 138.352264\n",
      "step 145, loss: 138.213135\n",
      "step 146, loss: 138.074158\n",
      "step 147, loss: 137.935333\n",
      "step 148, loss: 137.796661\n",
      "step 149, loss: 137.658157\n",
      "step 150, loss: 137.519775\n",
      "step 151, loss: 137.381577\n",
      "step 152, loss: 137.243515\n",
      "step 153, loss: 137.105621\n",
      "step 154, loss: 136.967834\n",
      "step 155, loss: 136.830200\n",
      "step 156, loss: 136.692719\n",
      "step 157, loss: 136.555389\n",
      "step 158, loss: 136.418198\n",
      "step 159, loss: 136.281189\n",
      "step 160, loss: 136.144302\n",
      "step 161, loss: 136.007568\n",
      "step 162, loss: 135.871002\n",
      "step 163, loss: 135.734573\n",
      "step 164, loss: 135.598282\n",
      "step 165, loss: 135.462158\n",
      "step 166, loss: 135.326157\n",
      "step 167, loss: 135.190292\n",
      "step 168, loss: 135.054565\n",
      "step 169, loss: 134.919022\n",
      "step 170, loss: 134.783600\n",
      "step 171, loss: 134.648331\n",
      "step 172, loss: 134.513214\n",
      "step 173, loss: 134.378250\n",
      "step 174, loss: 134.243423\n",
      "step 175, loss: 134.108749\n",
      "step 176, loss: 133.974213\n",
      "step 177, loss: 133.839844\n",
      "step 178, loss: 133.705566\n",
      "step 179, loss: 133.571442\n",
      "step 180, loss: 133.437485\n",
      "step 181, loss: 133.303650\n",
      "step 182, loss: 133.169968\n",
      "step 183, loss: 133.036423\n",
      "step 184, loss: 132.903030\n",
      "step 185, loss: 132.769791\n",
      "step 186, loss: 132.636703\n",
      "step 187, loss: 132.503738\n",
      "step 188, loss: 132.370941\n",
      "step 189, loss: 132.238266\n",
      "step 190, loss: 132.105713\n",
      "step 191, loss: 131.973297\n",
      "step 192, loss: 131.841034\n",
      "step 193, loss: 131.708908\n",
      "step 194, loss: 131.576935\n",
      "step 195, loss: 131.445099\n",
      "step 196, loss: 131.313400\n",
      "step 197, loss: 131.181839\n",
      "step 198, loss: 131.050446\n",
      "step 199, loss: 130.919174\n",
      "step 200, loss: 130.788055\n",
      "step 201, loss: 130.657074\n",
      "step 202, loss: 130.526199\n",
      "step 203, loss: 130.395477\n",
      "step 204, loss: 130.264893\n",
      "step 205, loss: 130.134430\n",
      "step 206, loss: 130.004120\n",
      "step 207, loss: 129.873962\n",
      "step 208, loss: 129.743927\n",
      "step 209, loss: 129.614044\n",
      "step 210, loss: 129.484299\n",
      "step 211, loss: 129.354706\n",
      "step 212, loss: 129.225220\n",
      "step 213, loss: 129.095901\n",
      "step 214, loss: 128.966675\n",
      "step 215, loss: 128.837601\n",
      "step 216, loss: 128.708664\n",
      "step 217, loss: 128.579865\n",
      "step 218, loss: 128.451187\n",
      "step 219, loss: 128.322662\n",
      "step 220, loss: 128.194275\n",
      "step 221, loss: 128.066025\n",
      "step 222, loss: 127.937912\n",
      "step 223, loss: 127.809929\n",
      "step 224, loss: 127.682091\n",
      "step 225, loss: 127.554344\n",
      "step 226, loss: 127.426758\n",
      "step 227, loss: 127.299286\n",
      "step 228, loss: 127.171967\n",
      "step 229, loss: 127.044777\n",
      "step 230, loss: 126.917717\n",
      "step 231, loss: 126.790802\n",
      "step 232, loss: 126.664032\n",
      "step 233, loss: 126.537384\n",
      "step 234, loss: 126.410858\n",
      "step 235, loss: 126.284477\n",
      "step 236, loss: 126.158234\n",
      "step 237, loss: 126.032097\n",
      "step 238, loss: 125.906090\n",
      "step 239, loss: 125.780212\n",
      "step 240, loss: 125.654465\n",
      "step 241, loss: 125.528870\n",
      "step 242, loss: 125.403397\n",
      "step 243, loss: 125.278061\n",
      "step 244, loss: 125.152855\n",
      "step 245, loss: 125.027771\n",
      "step 246, loss: 124.902832\n",
      "step 247, loss: 124.778023\n",
      "step 248, loss: 124.653320\n",
      "step 249, loss: 124.528732\n",
      "step 250, loss: 124.404289\n",
      "step 251, loss: 124.279976\n",
      "step 252, loss: 124.155807\n",
      "step 253, loss: 124.031754\n",
      "step 254, loss: 123.907829\n",
      "step 255, loss: 123.784042\n",
      "step 256, loss: 123.660385\n",
      "step 257, loss: 123.536850\n",
      "step 258, loss: 123.413452\n",
      "step 259, loss: 123.290154\n",
      "step 260, loss: 123.166977\n",
      "step 261, loss: 123.043930\n",
      "step 262, loss: 122.921028\n",
      "step 263, loss: 122.798233\n",
      "step 264, loss: 122.675583\n",
      "step 265, loss: 122.553062\n",
      "step 266, loss: 122.430664\n",
      "step 267, loss: 122.308395\n",
      "step 268, loss: 122.186241\n",
      "step 269, loss: 122.064209\n",
      "step 270, loss: 121.942314\n",
      "step 271, loss: 121.820541\n",
      "step 272, loss: 121.698883\n",
      "step 273, loss: 121.577347\n",
      "step 274, loss: 121.455948\n",
      "step 275, loss: 121.334663\n",
      "step 276, loss: 121.213501\n",
      "step 277, loss: 121.092484\n",
      "step 278, loss: 120.971565\n",
      "step 279, loss: 120.850784\n",
      "step 280, loss: 120.730110\n",
      "step 281, loss: 120.609573\n",
      "step 282, loss: 120.489151\n",
      "step 283, loss: 120.368866\n",
      "step 284, loss: 120.248688\n",
      "step 285, loss: 120.128632\n",
      "step 286, loss: 120.008682\n",
      "step 287, loss: 119.888878\n",
      "step 288, loss: 119.769203\n",
      "step 289, loss: 119.649635\n",
      "step 290, loss: 119.530182\n",
      "step 291, loss: 119.410851\n",
      "step 292, loss: 119.291649\n",
      "step 293, loss: 119.172562\n",
      "step 294, loss: 119.053612\n",
      "step 295, loss: 118.934753\n",
      "step 296, loss: 118.816025\n",
      "step 297, loss: 118.697418\n",
      "step 298, loss: 118.578926\n",
      "step 299, loss: 118.460564\n",
      "step 300, loss: 118.342323\n",
      "step 301, loss: 118.224182\n",
      "step 302, loss: 118.106171\n",
      "step 303, loss: 117.988281\n",
      "step 304, loss: 117.870506\n",
      "step 305, loss: 117.752853\n",
      "step 306, loss: 117.635315\n",
      "step 307, loss: 117.517883\n",
      "step 308, loss: 117.400581\n",
      "step 309, loss: 117.283386\n",
      "step 310, loss: 117.166321\n",
      "step 311, loss: 117.049377\n",
      "step 312, loss: 116.932526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 313, loss: 116.815811\n",
      "step 314, loss: 116.699203\n",
      "step 315, loss: 116.582718\n",
      "step 316, loss: 116.466354\n",
      "step 317, loss: 116.350098\n",
      "step 318, loss: 116.233940\n",
      "step 319, loss: 116.117920\n",
      "step 320, loss: 116.002014\n",
      "step 321, loss: 115.886215\n",
      "step 322, loss: 115.770531\n",
      "step 323, loss: 115.654968\n",
      "step 324, loss: 115.539513\n",
      "step 325, loss: 115.424171\n",
      "step 326, loss: 115.308960\n",
      "step 327, loss: 115.193863\n",
      "step 328, loss: 115.078857\n",
      "step 329, loss: 114.963959\n",
      "step 330, loss: 114.849190\n",
      "step 331, loss: 114.734535\n",
      "step 332, loss: 114.619995\n",
      "step 333, loss: 114.505554\n",
      "step 334, loss: 114.391235\n",
      "step 335, loss: 114.277031\n",
      "step 336, loss: 114.162933\n",
      "step 337, loss: 114.048950\n",
      "step 338, loss: 113.935097\n",
      "step 339, loss: 113.821327\n",
      "step 340, loss: 113.707680\n",
      "step 341, loss: 113.594139\n",
      "step 342, loss: 113.480728\n",
      "step 343, loss: 113.367401\n",
      "step 344, loss: 113.254196\n",
      "step 345, loss: 113.141098\n",
      "step 346, loss: 113.028122\n",
      "step 347, loss: 112.915253\n",
      "step 348, loss: 112.802490\n",
      "step 349, loss: 112.689827\n",
      "step 350, loss: 112.577286\n",
      "step 351, loss: 112.464844\n",
      "step 352, loss: 112.352516\n",
      "step 353, loss: 112.240311\n",
      "step 354, loss: 112.128197\n",
      "step 355, loss: 112.016190\n",
      "step 356, loss: 111.904297\n",
      "step 357, loss: 111.792526\n",
      "step 358, loss: 111.680855\n",
      "step 359, loss: 111.569290\n",
      "step 360, loss: 111.457825\n",
      "step 361, loss: 111.346481\n",
      "step 362, loss: 111.235229\n",
      "step 363, loss: 111.124100\n",
      "step 364, loss: 111.013077\n",
      "step 365, loss: 110.902153\n",
      "step 366, loss: 110.791328\n",
      "step 367, loss: 110.680618\n",
      "step 368, loss: 110.570023\n",
      "step 369, loss: 110.459541\n",
      "step 370, loss: 110.349136\n",
      "step 371, loss: 110.238846\n",
      "step 372, loss: 110.128677\n",
      "step 373, loss: 110.018600\n",
      "step 374, loss: 109.908646\n",
      "step 375, loss: 109.798767\n",
      "step 376, loss: 109.689026\n",
      "step 377, loss: 109.579353\n",
      "step 378, loss: 109.469818\n",
      "step 379, loss: 109.360374\n",
      "step 380, loss: 109.251038\n",
      "step 381, loss: 109.141808\n",
      "step 382, loss: 109.032677\n",
      "step 383, loss: 108.923645\n",
      "step 384, loss: 108.814728\n",
      "step 385, loss: 108.705917\n",
      "step 386, loss: 108.597198\n",
      "step 387, loss: 108.488586\n",
      "step 388, loss: 108.380081\n",
      "step 389, loss: 108.271667\n",
      "step 390, loss: 108.163376\n",
      "step 391, loss: 108.055168\n",
      "step 392, loss: 107.947067\n",
      "step 393, loss: 107.839066\n",
      "step 394, loss: 107.731178\n",
      "step 395, loss: 107.623383\n",
      "step 396, loss: 107.515686\n",
      "step 397, loss: 107.408104\n",
      "step 398, loss: 107.300613\n",
      "step 399, loss: 107.193230\n",
      "step 400, loss: 107.085945\n",
      "step 401, loss: 106.978760\n",
      "step 402, loss: 106.871666\n",
      "step 403, loss: 106.764694\n",
      "step 404, loss: 106.657806\n",
      "step 405, loss: 106.551025\n",
      "step 406, loss: 106.444336\n",
      "step 407, loss: 106.337753\n",
      "step 408, loss: 106.231270\n",
      "step 409, loss: 106.124886\n",
      "step 410, loss: 106.018616\n",
      "step 411, loss: 105.912430\n",
      "step 412, loss: 105.806335\n",
      "step 413, loss: 105.700348\n",
      "step 414, loss: 105.594460\n",
      "step 415, loss: 105.488678\n",
      "step 416, loss: 105.382996\n",
      "step 417, loss: 105.277405\n",
      "step 418, loss: 105.171898\n",
      "step 419, loss: 105.066513\n",
      "step 420, loss: 104.961212\n",
      "step 421, loss: 104.856018\n",
      "step 422, loss: 104.750916\n",
      "step 423, loss: 104.645912\n",
      "step 424, loss: 104.541000\n",
      "step 425, loss: 104.436188\n",
      "step 426, loss: 104.331482\n",
      "step 427, loss: 104.226860\n",
      "step 428, loss: 104.122345\n",
      "step 429, loss: 104.017914\n",
      "step 430, loss: 103.913589\n",
      "step 431, loss: 103.809364\n",
      "step 432, loss: 103.705231\n",
      "step 433, loss: 103.601196\n",
      "step 434, loss: 103.497238\n",
      "step 435, loss: 103.393394\n",
      "step 436, loss: 103.289642\n",
      "step 437, loss: 103.185982\n",
      "step 438, loss: 103.082420\n",
      "step 439, loss: 102.978958\n",
      "step 440, loss: 102.875587\n",
      "step 441, loss: 102.772308\n",
      "step 442, loss: 102.669128\n",
      "step 443, loss: 102.566032\n",
      "step 444, loss: 102.463051\n",
      "step 445, loss: 102.360153\n",
      "step 446, loss: 102.257347\n",
      "step 447, loss: 102.154633\n",
      "step 448, loss: 102.052010\n",
      "step 449, loss: 101.949478\n",
      "step 450, loss: 101.847054\n",
      "step 451, loss: 101.744720\n",
      "step 452, loss: 101.642471\n",
      "step 453, loss: 101.540314\n",
      "step 454, loss: 101.438255\n",
      "step 455, loss: 101.336288\n",
      "step 456, loss: 101.234421\n",
      "step 457, loss: 101.132637\n",
      "step 458, loss: 101.030945\n",
      "step 459, loss: 100.929352\n",
      "step 460, loss: 100.827835\n",
      "step 461, loss: 100.726440\n",
      "step 462, loss: 100.625107\n",
      "step 463, loss: 100.523872\n",
      "step 464, loss: 100.422737\n",
      "step 465, loss: 100.321686\n",
      "step 466, loss: 100.220741\n",
      "step 467, loss: 100.119865\n",
      "step 468, loss: 100.019089\n",
      "step 469, loss: 99.918404\n",
      "step 470, loss: 99.817818\n",
      "step 471, loss: 99.717323\n",
      "step 472, loss: 99.616898\n",
      "step 473, loss: 99.516571\n",
      "step 474, loss: 99.416344\n",
      "step 475, loss: 99.316208\n",
      "step 476, loss: 99.216156\n",
      "step 477, loss: 99.116188\n",
      "step 478, loss: 99.016312\n",
      "step 479, loss: 98.916527\n",
      "step 480, loss: 98.816833\n",
      "step 481, loss: 98.717239\n",
      "step 482, loss: 98.617722\n",
      "step 483, loss: 98.518280\n",
      "step 484, loss: 98.418945\n",
      "step 485, loss: 98.319695\n",
      "step 486, loss: 98.220551\n",
      "step 487, loss: 98.121460\n",
      "step 488, loss: 98.022476\n",
      "step 489, loss: 97.923576\n",
      "step 490, loss: 97.824776\n",
      "step 491, loss: 97.726051\n",
      "step 492, loss: 97.627411\n",
      "step 493, loss: 97.528870\n",
      "step 494, loss: 97.430405\n",
      "step 495, loss: 97.332039\n",
      "step 496, loss: 97.233765\n",
      "step 497, loss: 97.135559\n",
      "step 498, loss: 97.037445\n",
      "step 499, loss: 96.939423\n",
      "step 500, loss: 96.841492\n",
      "step 501, loss: 96.743637\n",
      "step 502, loss: 96.645874\n",
      "step 503, loss: 96.548187\n",
      "step 504, loss: 96.450600\n",
      "step 505, loss: 96.353104\n",
      "step 506, loss: 96.255684\n",
      "step 507, loss: 96.158348\n",
      "step 508, loss: 96.061104\n",
      "step 509, loss: 95.963928\n",
      "step 510, loss: 95.866859\n",
      "step 511, loss: 95.769875\n",
      "step 512, loss: 95.672966\n",
      "step 513, loss: 95.576134\n",
      "step 514, loss: 95.479401\n",
      "step 515, loss: 95.382751\n",
      "step 516, loss: 95.286194\n",
      "step 517, loss: 95.189697\n",
      "step 518, loss: 95.093300\n",
      "step 519, loss: 94.996994\n",
      "step 520, loss: 94.900772\n",
      "step 521, loss: 94.804626\n",
      "step 522, loss: 94.708557\n",
      "step 523, loss: 94.612595\n",
      "step 524, loss: 94.516693\n",
      "step 525, loss: 94.420883\n",
      "step 526, loss: 94.325157\n",
      "step 527, loss: 94.229523\n",
      "step 528, loss: 94.133957\n",
      "step 529, loss: 94.038475\n",
      "step 530, loss: 93.943085\n",
      "step 531, loss: 93.847778\n",
      "step 532, loss: 93.752556\n",
      "step 533, loss: 93.657410\n",
      "step 534, loss: 93.562347\n",
      "step 535, loss: 93.467377\n",
      "step 536, loss: 93.372475\n",
      "step 537, loss: 93.277657\n",
      "step 538, loss: 93.182922\n",
      "step 539, loss: 93.088280\n",
      "step 540, loss: 92.993713\n",
      "step 541, loss: 92.899231\n",
      "step 542, loss: 92.804810\n",
      "step 543, loss: 92.710495\n",
      "step 544, loss: 92.616249\n",
      "step 545, loss: 92.522095\n",
      "step 546, loss: 92.428017\n",
      "step 547, loss: 92.334007\n",
      "step 548, loss: 92.240097\n",
      "step 549, loss: 92.146255\n",
      "step 550, loss: 92.052505\n",
      "step 551, loss: 91.958817\n",
      "step 552, loss: 91.865227\n",
      "step 553, loss: 91.771706\n",
      "step 554, loss: 91.678276\n",
      "step 555, loss: 91.584930\n",
      "step 556, loss: 91.491646\n",
      "step 557, loss: 91.398453\n",
      "step 558, loss: 91.305344\n",
      "step 559, loss: 91.212311\n",
      "step 560, loss: 91.119362\n",
      "step 561, loss: 91.026482\n",
      "step 562, loss: 90.933685\n",
      "step 563, loss: 90.840973\n",
      "step 564, loss: 90.748337\n",
      "step 565, loss: 90.655785\n",
      "step 566, loss: 90.563301\n",
      "step 567, loss: 90.470909\n",
      "step 568, loss: 90.378586\n",
      "step 569, loss: 90.286346\n",
      "step 570, loss: 90.194199\n",
      "step 571, loss: 90.102112\n",
      "step 572, loss: 90.010101\n",
      "step 573, loss: 89.918167\n",
      "step 574, loss: 89.826332\n",
      "step 575, loss: 89.734566\n",
      "step 576, loss: 89.642868\n",
      "step 577, loss: 89.551254\n",
      "step 578, loss: 89.459717\n",
      "step 579, loss: 89.368271\n",
      "step 580, loss: 89.276886\n",
      "step 581, loss: 89.185577\n",
      "step 582, loss: 89.094353\n",
      "step 583, loss: 89.003204\n",
      "step 584, loss: 88.912140\n",
      "step 585, loss: 88.821144\n",
      "step 586, loss: 88.730225\n",
      "step 587, loss: 88.639389\n",
      "step 588, loss: 88.548622\n",
      "step 589, loss: 88.457947\n",
      "step 590, loss: 88.367325\n",
      "step 591, loss: 88.276794\n",
      "step 592, loss: 88.186348\n",
      "step 593, loss: 88.095970\n",
      "step 594, loss: 88.005669\n",
      "step 595, loss: 87.915428\n",
      "step 596, loss: 87.825287\n",
      "step 597, loss: 87.735214\n",
      "step 598, loss: 87.645226\n",
      "step 599, loss: 87.555298\n",
      "step 600, loss: 87.465454\n",
      "step 601, loss: 87.375687\n",
      "step 602, loss: 87.285995\n",
      "step 603, loss: 87.196373\n",
      "step 604, loss: 87.106850\n",
      "step 605, loss: 87.017365\n",
      "step 606, loss: 86.927979\n",
      "step 607, loss: 86.838669\n",
      "step 608, loss: 86.749428\n",
      "step 609, loss: 86.660263\n",
      "step 610, loss: 86.571167\n",
      "step 611, loss: 86.482147\n",
      "step 612, loss: 86.393204\n",
      "step 613, loss: 86.304344\n",
      "step 614, loss: 86.215546\n",
      "step 615, loss: 86.126831\n",
      "step 616, loss: 86.038177\n",
      "step 617, loss: 85.949615\n",
      "step 618, loss: 85.861130\n",
      "step 619, loss: 85.772697\n",
      "step 620, loss: 85.684349\n",
      "step 621, loss: 85.596077\n",
      "step 622, loss: 85.507881\n",
      "step 623, loss: 85.419762\n",
      "step 624, loss: 85.331711\n",
      "step 625, loss: 85.243721\n",
      "step 626, loss: 85.155815\n",
      "step 627, loss: 85.067993\n",
      "step 628, loss: 84.980240\n",
      "step 629, loss: 84.892555\n",
      "step 630, loss: 84.804939\n",
      "step 631, loss: 84.717400\n",
      "step 632, loss: 84.629944\n",
      "step 633, loss: 84.542557\n",
      "step 634, loss: 84.455231\n",
      "step 635, loss: 84.367981\n",
      "step 636, loss: 84.280815\n",
      "step 637, loss: 84.193718\n",
      "step 638, loss: 84.106689\n",
      "step 639, loss: 84.019730\n",
      "step 640, loss: 83.932838\n",
      "step 641, loss: 83.846039\n",
      "step 642, loss: 83.759293\n",
      "step 643, loss: 83.672623\n",
      "step 644, loss: 83.586037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 645, loss: 83.499504\n",
      "step 646, loss: 83.413071\n",
      "step 647, loss: 83.326691\n",
      "step 648, loss: 83.240387\n",
      "step 649, loss: 83.154144\n",
      "step 650, loss: 83.067978\n",
      "step 651, loss: 82.981895\n",
      "step 652, loss: 82.895874\n",
      "step 653, loss: 82.809929\n",
      "step 654, loss: 82.724052\n",
      "step 655, loss: 82.638237\n",
      "step 656, loss: 82.552513\n",
      "step 657, loss: 82.466850\n",
      "step 658, loss: 82.381264\n",
      "step 659, loss: 82.295738\n",
      "step 660, loss: 82.210289\n",
      "step 661, loss: 82.124908\n",
      "step 662, loss: 82.039597\n",
      "step 663, loss: 81.954361\n",
      "step 664, loss: 81.869179\n",
      "step 665, loss: 81.784096\n",
      "step 666, loss: 81.699066\n",
      "step 667, loss: 81.614113\n",
      "step 668, loss: 81.529221\n",
      "step 669, loss: 81.444405\n",
      "step 670, loss: 81.359650\n",
      "step 671, loss: 81.274994\n",
      "step 672, loss: 81.190376\n",
      "step 673, loss: 81.105843\n",
      "step 674, loss: 81.021370\n",
      "step 675, loss: 80.936989\n",
      "step 676, loss: 80.852661\n",
      "step 677, loss: 80.768402\n",
      "step 678, loss: 80.684204\n",
      "step 679, loss: 80.600098\n",
      "step 680, loss: 80.516052\n",
      "step 681, loss: 80.432083\n",
      "step 682, loss: 80.348167\n",
      "step 683, loss: 80.264328\n",
      "step 684, loss: 80.180557\n",
      "step 685, loss: 80.096863\n",
      "step 686, loss: 80.013229\n",
      "step 687, loss: 79.929657\n",
      "step 688, loss: 79.846161\n",
      "step 689, loss: 79.762741\n",
      "step 690, loss: 79.679390\n",
      "step 691, loss: 79.596092\n",
      "step 692, loss: 79.512871\n",
      "step 693, loss: 79.429718\n",
      "step 694, loss: 79.346642\n",
      "step 695, loss: 79.263634\n",
      "step 696, loss: 79.180679\n",
      "step 697, loss: 79.097809\n",
      "step 698, loss: 79.014992\n",
      "step 699, loss: 78.932251\n",
      "step 700, loss: 78.849586\n",
      "step 701, loss: 78.766975\n",
      "step 702, loss: 78.684441\n",
      "step 703, loss: 78.601974\n",
      "step 704, loss: 78.519577\n",
      "step 705, loss: 78.437248\n",
      "step 706, loss: 78.354980\n",
      "step 707, loss: 78.272781\n",
      "step 708, loss: 78.190659\n",
      "step 709, loss: 78.108597\n",
      "step 710, loss: 78.026596\n",
      "step 711, loss: 77.944664\n",
      "step 712, loss: 77.862808\n",
      "step 713, loss: 77.781013\n",
      "step 714, loss: 77.699295\n",
      "step 715, loss: 77.617630\n",
      "step 716, loss: 77.536041\n",
      "step 717, loss: 77.454514\n",
      "step 718, loss: 77.373055\n",
      "step 719, loss: 77.291672\n",
      "step 720, loss: 77.210350\n",
      "step 721, loss: 77.129089\n",
      "step 722, loss: 77.047905\n",
      "step 723, loss: 76.966782\n",
      "step 724, loss: 76.885727\n",
      "step 725, loss: 76.804733\n",
      "step 726, loss: 76.723816\n",
      "step 727, loss: 76.642952\n",
      "step 728, loss: 76.562164\n",
      "step 729, loss: 76.481445\n",
      "step 730, loss: 76.400787\n",
      "step 731, loss: 76.320198\n",
      "step 732, loss: 76.239670\n",
      "step 733, loss: 76.159218\n",
      "step 734, loss: 76.078819\n",
      "step 735, loss: 75.998489\n",
      "step 736, loss: 75.918228\n",
      "step 737, loss: 75.838036\n",
      "step 738, loss: 75.757904\n",
      "step 739, loss: 75.677841\n",
      "step 740, loss: 75.597832\n",
      "step 741, loss: 75.517906\n",
      "step 742, loss: 75.438042\n",
      "step 743, loss: 75.358246\n",
      "step 744, loss: 75.278503\n",
      "step 745, loss: 75.198837\n",
      "step 746, loss: 75.119232\n",
      "step 747, loss: 75.039696\n",
      "step 748, loss: 74.960228\n",
      "step 749, loss: 74.880814\n",
      "step 750, loss: 74.801468\n",
      "step 751, loss: 74.722191\n",
      "step 752, loss: 74.642982\n",
      "step 753, loss: 74.563828\n",
      "step 754, loss: 74.484741\n",
      "step 755, loss: 74.405724\n",
      "step 756, loss: 74.326767\n",
      "step 757, loss: 74.247887\n",
      "step 758, loss: 74.169060\n",
      "step 759, loss: 74.090294\n",
      "step 760, loss: 74.011597\n",
      "step 761, loss: 73.932968\n",
      "step 762, loss: 73.854408\n",
      "step 763, loss: 73.775902\n",
      "step 764, loss: 73.697456\n",
      "step 765, loss: 73.619087\n",
      "step 766, loss: 73.540787\n",
      "step 767, loss: 73.462540\n",
      "step 768, loss: 73.384361\n",
      "step 769, loss: 73.306244\n",
      "step 770, loss: 73.228188\n",
      "step 771, loss: 73.150208\n",
      "step 772, loss: 73.072289\n",
      "step 773, loss: 72.994423\n",
      "step 774, loss: 72.916626\n",
      "step 775, loss: 72.838898\n",
      "step 776, loss: 72.761230\n",
      "step 777, loss: 72.683624\n",
      "step 778, loss: 72.606079\n",
      "step 779, loss: 72.528603\n",
      "step 780, loss: 72.451195\n",
      "step 781, loss: 72.373840\n",
      "step 782, loss: 72.296555\n",
      "step 783, loss: 72.219330\n",
      "step 784, loss: 72.142166\n",
      "step 785, loss: 72.065071\n",
      "step 786, loss: 71.988037\n",
      "step 787, loss: 71.911072\n",
      "step 788, loss: 71.834160\n",
      "step 789, loss: 71.757317\n",
      "step 790, loss: 71.680542\n",
      "step 791, loss: 71.603828\n",
      "step 792, loss: 71.527176\n",
      "step 793, loss: 71.450577\n",
      "step 794, loss: 71.374054\n",
      "step 795, loss: 71.297592\n",
      "step 796, loss: 71.221184\n",
      "step 797, loss: 71.144852\n",
      "step 798, loss: 71.068565\n",
      "step 799, loss: 70.992355\n",
      "step 800, loss: 70.916206\n",
      "step 801, loss: 70.840118\n",
      "step 802, loss: 70.764091\n",
      "step 803, loss: 70.688126\n",
      "step 804, loss: 70.612228\n",
      "step 805, loss: 70.536392\n",
      "step 806, loss: 70.460609\n",
      "step 807, loss: 70.384888\n",
      "step 808, loss: 70.309242\n",
      "step 809, loss: 70.233658\n",
      "step 810, loss: 70.158127\n",
      "step 811, loss: 70.082657\n",
      "step 812, loss: 70.007248\n",
      "step 813, loss: 69.931908\n",
      "step 814, loss: 69.856628\n",
      "step 815, loss: 69.781418\n",
      "step 816, loss: 69.706253\n",
      "step 817, loss: 69.631165\n",
      "step 818, loss: 69.556129\n",
      "step 819, loss: 69.481163\n",
      "step 820, loss: 69.406265\n",
      "step 821, loss: 69.331406\n",
      "step 822, loss: 69.256622\n",
      "step 823, loss: 69.181900\n",
      "step 824, loss: 69.107239\n",
      "step 825, loss: 69.032639\n",
      "step 826, loss: 68.958099\n",
      "step 827, loss: 68.883621\n",
      "step 828, loss: 68.809204\n",
      "step 829, loss: 68.734863\n",
      "step 830, loss: 68.660553\n",
      "step 831, loss: 68.586319\n",
      "step 832, loss: 68.512146\n",
      "step 833, loss: 68.438042\n",
      "step 834, loss: 68.363998\n",
      "step 835, loss: 68.290001\n",
      "step 836, loss: 68.216072\n",
      "step 837, loss: 68.142204\n",
      "step 838, loss: 68.068405\n",
      "step 839, loss: 67.994659\n",
      "step 840, loss: 67.920975\n",
      "step 841, loss: 67.847351\n",
      "step 842, loss: 67.773788\n",
      "step 843, loss: 67.700287\n",
      "step 844, loss: 67.626846\n",
      "step 845, loss: 67.553467\n",
      "step 846, loss: 67.480148\n",
      "step 847, loss: 67.406883\n",
      "step 848, loss: 67.333694\n",
      "step 849, loss: 67.260559\n",
      "step 850, loss: 67.187485\n",
      "step 851, loss: 67.114456\n",
      "step 852, loss: 67.041504\n",
      "step 853, loss: 66.968613\n",
      "step 854, loss: 66.895782\n",
      "step 855, loss: 66.822998\n",
      "step 856, loss: 66.750290\n",
      "step 857, loss: 66.677635\n",
      "step 858, loss: 66.605042\n",
      "step 859, loss: 66.532509\n",
      "step 860, loss: 66.460030\n",
      "step 861, loss: 66.387619\n",
      "step 862, loss: 66.315262\n",
      "step 863, loss: 66.242973\n",
      "step 864, loss: 66.170731\n",
      "step 865, loss: 66.098549\n",
      "step 866, loss: 66.026436\n",
      "step 867, loss: 65.954384\n",
      "step 868, loss: 65.882385\n",
      "step 869, loss: 65.810440\n",
      "step 870, loss: 65.738571\n",
      "step 871, loss: 65.666748\n",
      "step 872, loss: 65.594994\n",
      "step 873, loss: 65.523300\n",
      "step 874, loss: 65.451660\n",
      "step 875, loss: 65.380074\n",
      "step 876, loss: 65.308563\n",
      "step 877, loss: 65.237099\n",
      "step 878, loss: 65.165703\n",
      "step 879, loss: 65.094360\n",
      "step 880, loss: 65.023071\n",
      "step 881, loss: 64.951851\n",
      "step 882, loss: 64.880692\n",
      "step 883, loss: 64.809586\n",
      "step 884, loss: 64.738533\n",
      "step 885, loss: 64.667542\n",
      "step 886, loss: 64.596619\n",
      "step 887, loss: 64.525749\n",
      "step 888, loss: 64.454941\n",
      "step 889, loss: 64.384193\n",
      "step 890, loss: 64.313492\n",
      "step 891, loss: 64.242867\n",
      "step 892, loss: 64.172295\n",
      "step 893, loss: 64.101776\n",
      "step 894, loss: 64.031326\n",
      "step 895, loss: 63.960922\n",
      "step 896, loss: 63.890579\n",
      "step 897, loss: 63.820301\n",
      "step 898, loss: 63.750088\n",
      "step 899, loss: 63.679920\n",
      "step 900, loss: 63.609810\n",
      "step 901, loss: 63.539768\n",
      "step 902, loss: 63.469780\n",
      "step 903, loss: 63.399841\n",
      "step 904, loss: 63.329971\n",
      "step 905, loss: 63.260155\n",
      "step 906, loss: 63.190407\n",
      "step 907, loss: 63.120705\n",
      "step 908, loss: 63.051064\n",
      "step 909, loss: 62.981483\n",
      "step 910, loss: 62.911957\n",
      "step 911, loss: 62.842491\n",
      "step 912, loss: 62.773090\n",
      "step 913, loss: 62.703735\n",
      "step 914, loss: 62.634438\n",
      "step 915, loss: 62.565212\n",
      "step 916, loss: 62.496033\n",
      "step 917, loss: 62.426926\n",
      "step 918, loss: 62.357861\n",
      "step 919, loss: 62.288857\n",
      "step 920, loss: 62.219910\n",
      "step 921, loss: 62.151028\n",
      "step 922, loss: 62.082203\n",
      "step 923, loss: 62.013428\n",
      "step 924, loss: 61.944717\n",
      "step 925, loss: 61.876057\n",
      "step 926, loss: 61.807457\n",
      "step 927, loss: 61.738922\n",
      "step 928, loss: 61.670437\n",
      "step 929, loss: 61.602009\n",
      "step 930, loss: 61.533638\n",
      "step 931, loss: 61.465332\n",
      "step 932, loss: 61.397087\n",
      "step 933, loss: 61.328884\n",
      "step 934, loss: 61.260746\n",
      "step 935, loss: 61.192665\n",
      "step 936, loss: 61.124641\n",
      "step 937, loss: 61.056679\n",
      "step 938, loss: 60.988766\n",
      "step 939, loss: 60.920910\n",
      "step 940, loss: 60.853111\n",
      "step 941, loss: 60.785378\n",
      "step 942, loss: 60.717697\n",
      "step 943, loss: 60.650074\n",
      "step 944, loss: 60.582500\n",
      "step 945, loss: 60.514988\n",
      "step 946, loss: 60.447540\n",
      "step 947, loss: 60.380142\n",
      "step 948, loss: 60.312798\n",
      "step 949, loss: 60.245514\n",
      "step 950, loss: 60.178291\n",
      "step 951, loss: 60.111122\n",
      "step 952, loss: 60.044006\n",
      "step 953, loss: 59.976952\n",
      "step 954, loss: 59.909946\n",
      "step 955, loss: 59.843002\n",
      "step 956, loss: 59.776115\n",
      "step 957, loss: 59.709290\n",
      "step 958, loss: 59.642509\n",
      "step 959, loss: 59.575790\n",
      "step 960, loss: 59.509132\n",
      "step 961, loss: 59.442528\n",
      "step 962, loss: 59.375980\n",
      "step 963, loss: 59.309486\n",
      "step 964, loss: 59.243050\n",
      "step 965, loss: 59.176670\n",
      "step 966, loss: 59.110344\n",
      "step 967, loss: 59.044083\n",
      "step 968, loss: 58.977867\n",
      "step 969, loss: 58.911705\n",
      "step 970, loss: 58.845612\n",
      "step 971, loss: 58.779568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 972, loss: 58.713585\n",
      "step 973, loss: 58.647648\n",
      "step 974, loss: 58.581772\n",
      "step 975, loss: 58.515953\n",
      "step 976, loss: 58.450191\n",
      "step 977, loss: 58.384483\n",
      "step 978, loss: 58.318825\n",
      "step 979, loss: 58.253227\n",
      "step 980, loss: 58.187695\n",
      "step 981, loss: 58.122204\n",
      "step 982, loss: 58.056778\n",
      "step 983, loss: 57.991402\n",
      "step 984, loss: 57.926083\n",
      "step 985, loss: 57.860817\n",
      "step 986, loss: 57.795616\n",
      "step 987, loss: 57.730465\n",
      "step 988, loss: 57.665363\n",
      "step 989, loss: 57.600323\n",
      "step 990, loss: 57.535332\n",
      "step 991, loss: 57.470406\n",
      "step 992, loss: 57.405533\n",
      "step 993, loss: 57.340710\n",
      "step 994, loss: 57.275944\n",
      "step 995, loss: 57.211235\n",
      "step 996, loss: 57.146584\n",
      "step 997, loss: 57.081989\n",
      "step 998, loss: 57.017441\n",
      "step 999, loss: 56.952950\n",
      "step 1000, loss: 56.888515\n",
      "step 1001, loss: 56.824135\n",
      "step 1002, loss: 56.759815\n",
      "step 1003, loss: 56.695541\n",
      "step 1004, loss: 56.631329\n",
      "step 1005, loss: 56.567165\n",
      "step 1006, loss: 56.503067\n",
      "step 1007, loss: 56.439022\n",
      "step 1008, loss: 56.375023\n",
      "step 1009, loss: 56.311077\n",
      "step 1010, loss: 56.247196\n",
      "step 1011, loss: 56.183365\n",
      "step 1012, loss: 56.119595\n",
      "step 1013, loss: 56.055870\n",
      "step 1014, loss: 55.992199\n",
      "step 1015, loss: 55.928585\n",
      "step 1016, loss: 55.865032\n",
      "step 1017, loss: 55.801537\n",
      "step 1018, loss: 55.738091\n",
      "step 1019, loss: 55.674698\n",
      "step 1020, loss: 55.611355\n",
      "step 1021, loss: 55.548073\n",
      "step 1022, loss: 55.484844\n",
      "step 1023, loss: 55.421669\n",
      "step 1024, loss: 55.358551\n",
      "step 1025, loss: 55.295479\n",
      "step 1026, loss: 55.232471\n",
      "step 1027, loss: 55.169510\n",
      "step 1028, loss: 55.106609\n",
      "step 1029, loss: 55.043755\n",
      "step 1030, loss: 54.980957\n",
      "step 1031, loss: 54.918221\n",
      "step 1032, loss: 54.855530\n",
      "step 1033, loss: 54.792900\n",
      "step 1034, loss: 54.730316\n",
      "step 1035, loss: 54.667786\n",
      "step 1036, loss: 54.605316\n",
      "step 1037, loss: 54.542904\n",
      "step 1038, loss: 54.480541\n",
      "step 1039, loss: 54.418224\n",
      "step 1040, loss: 54.355965\n",
      "step 1041, loss: 54.293762\n",
      "step 1042, loss: 54.231617\n",
      "step 1043, loss: 54.169521\n",
      "step 1044, loss: 54.107479\n",
      "step 1045, loss: 54.045483\n",
      "step 1046, loss: 53.983551\n",
      "step 1047, loss: 53.921669\n",
      "step 1048, loss: 53.859848\n",
      "step 1049, loss: 53.798073\n",
      "step 1050, loss: 53.736343\n",
      "step 1051, loss: 53.674675\n",
      "step 1052, loss: 53.613064\n",
      "step 1053, loss: 53.551506\n",
      "step 1054, loss: 53.490005\n",
      "step 1055, loss: 53.428551\n",
      "step 1056, loss: 53.367146\n",
      "step 1057, loss: 53.305801\n",
      "step 1058, loss: 53.244511\n",
      "step 1059, loss: 53.183273\n",
      "step 1060, loss: 53.122086\n",
      "step 1061, loss: 53.060947\n",
      "step 1062, loss: 52.999870\n",
      "step 1063, loss: 52.938847\n",
      "step 1064, loss: 52.877869\n",
      "step 1065, loss: 52.816948\n",
      "step 1066, loss: 52.756077\n",
      "step 1067, loss: 52.695259\n",
      "step 1068, loss: 52.634499\n",
      "step 1069, loss: 52.573792\n",
      "step 1070, loss: 52.513134\n",
      "step 1071, loss: 52.452530\n",
      "step 1072, loss: 52.391975\n",
      "step 1073, loss: 52.331474\n",
      "step 1074, loss: 52.271034\n",
      "step 1075, loss: 52.210648\n",
      "step 1076, loss: 52.150307\n",
      "step 1077, loss: 52.090019\n",
      "step 1078, loss: 52.029785\n",
      "step 1079, loss: 51.969604\n",
      "step 1080, loss: 51.909473\n",
      "step 1081, loss: 51.849396\n",
      "step 1082, loss: 51.789371\n",
      "step 1083, loss: 51.729401\n",
      "step 1084, loss: 51.669483\n",
      "step 1085, loss: 51.609619\n",
      "step 1086, loss: 51.549797\n",
      "step 1087, loss: 51.490040\n",
      "step 1088, loss: 51.430328\n",
      "step 1089, loss: 51.370674\n",
      "step 1090, loss: 51.311069\n",
      "step 1091, loss: 51.251514\n",
      "step 1092, loss: 51.192013\n",
      "step 1093, loss: 51.132561\n",
      "step 1094, loss: 51.073166\n",
      "step 1095, loss: 51.013824\n",
      "step 1096, loss: 50.954536\n",
      "step 1097, loss: 50.895294\n",
      "step 1098, loss: 50.836105\n",
      "step 1099, loss: 50.776970\n",
      "step 1100, loss: 50.717888\n",
      "step 1101, loss: 50.658859\n",
      "step 1102, loss: 50.599876\n",
      "step 1103, loss: 50.540947\n",
      "step 1104, loss: 50.482067\n",
      "step 1105, loss: 50.423244\n",
      "step 1106, loss: 50.364479\n",
      "step 1107, loss: 50.305763\n",
      "step 1108, loss: 50.247093\n",
      "step 1109, loss: 50.188477\n",
      "step 1110, loss: 50.129913\n",
      "step 1111, loss: 50.071400\n",
      "step 1112, loss: 50.012943\n",
      "step 1113, loss: 49.954521\n",
      "step 1114, loss: 49.896172\n",
      "step 1115, loss: 49.837860\n",
      "step 1116, loss: 49.779610\n",
      "step 1117, loss: 49.721405\n",
      "step 1118, loss: 49.663254\n",
      "step 1119, loss: 49.605148\n",
      "step 1120, loss: 49.547096\n",
      "step 1121, loss: 49.489101\n",
      "step 1122, loss: 49.431152\n",
      "step 1123, loss: 49.373264\n",
      "step 1124, loss: 49.315414\n",
      "step 1125, loss: 49.257622\n",
      "step 1126, loss: 49.199883\n",
      "step 1127, loss: 49.142193\n",
      "step 1128, loss: 49.084557\n",
      "step 1129, loss: 49.026970\n",
      "step 1130, loss: 48.969425\n",
      "step 1131, loss: 48.911942\n",
      "step 1132, loss: 48.854507\n",
      "step 1133, loss: 48.797119\n",
      "step 1134, loss: 48.739796\n",
      "step 1135, loss: 48.682514\n",
      "step 1136, loss: 48.625278\n",
      "step 1137, loss: 48.568104\n",
      "step 1138, loss: 48.510979\n",
      "step 1139, loss: 48.453896\n",
      "step 1140, loss: 48.396870\n",
      "step 1141, loss: 48.339890\n",
      "step 1142, loss: 48.282967\n",
      "step 1143, loss: 48.226093\n",
      "step 1144, loss: 48.169270\n",
      "step 1145, loss: 48.112492\n",
      "step 1146, loss: 48.055771\n",
      "step 1147, loss: 47.999092\n",
      "step 1148, loss: 47.942471\n",
      "step 1149, loss: 47.885902\n",
      "step 1150, loss: 47.829384\n",
      "step 1151, loss: 47.772911\n",
      "step 1152, loss: 47.716492\n",
      "step 1153, loss: 47.660122\n",
      "step 1154, loss: 47.603809\n",
      "step 1155, loss: 47.547539\n",
      "step 1156, loss: 47.491325\n",
      "step 1157, loss: 47.435154\n",
      "step 1158, loss: 47.379036\n",
      "step 1159, loss: 47.322975\n",
      "step 1160, loss: 47.266953\n",
      "step 1161, loss: 47.210995\n",
      "step 1162, loss: 47.155075\n",
      "step 1163, loss: 47.099209\n",
      "step 1164, loss: 47.043388\n",
      "step 1165, loss: 46.987625\n",
      "step 1166, loss: 46.931911\n",
      "step 1167, loss: 46.876251\n",
      "step 1168, loss: 46.820633\n",
      "step 1169, loss: 46.765068\n",
      "step 1170, loss: 46.709557\n",
      "step 1171, loss: 46.654091\n",
      "step 1172, loss: 46.598675\n",
      "step 1173, loss: 46.543304\n",
      "step 1174, loss: 46.487984\n",
      "step 1175, loss: 46.432716\n",
      "step 1176, loss: 46.377506\n",
      "step 1177, loss: 46.322338\n",
      "step 1178, loss: 46.267223\n",
      "step 1179, loss: 46.212158\n",
      "step 1180, loss: 46.157139\n",
      "step 1181, loss: 46.102169\n",
      "step 1182, loss: 46.047253\n",
      "step 1183, loss: 45.992386\n",
      "step 1184, loss: 45.937565\n",
      "step 1185, loss: 45.882790\n",
      "step 1186, loss: 45.828072\n",
      "step 1187, loss: 45.773403\n",
      "step 1188, loss: 45.718784\n",
      "step 1189, loss: 45.664211\n",
      "step 1190, loss: 45.609688\n",
      "step 1191, loss: 45.555214\n",
      "step 1192, loss: 45.500786\n",
      "step 1193, loss: 45.446419\n",
      "step 1194, loss: 45.392094\n",
      "step 1195, loss: 45.337822\n",
      "step 1196, loss: 45.283596\n",
      "step 1197, loss: 45.229416\n",
      "step 1198, loss: 45.175282\n",
      "step 1199, loss: 45.121208\n",
      "step 1200, loss: 45.067184\n",
      "step 1201, loss: 45.013203\n",
      "step 1202, loss: 44.959274\n",
      "step 1203, loss: 44.905388\n",
      "step 1204, loss: 44.851555\n",
      "step 1205, loss: 44.797775\n",
      "step 1206, loss: 44.744041\n",
      "step 1207, loss: 44.690350\n",
      "step 1208, loss: 44.636711\n",
      "step 1209, loss: 44.583118\n",
      "step 1210, loss: 44.529579\n",
      "step 1211, loss: 44.476089\n",
      "step 1212, loss: 44.422657\n",
      "step 1213, loss: 44.369255\n",
      "step 1214, loss: 44.315907\n",
      "step 1215, loss: 44.262611\n",
      "step 1216, loss: 44.209366\n",
      "step 1217, loss: 44.156174\n",
      "step 1218, loss: 44.103024\n",
      "step 1219, loss: 44.049915\n",
      "step 1220, loss: 43.996868\n",
      "step 1221, loss: 43.943859\n",
      "step 1222, loss: 43.890903\n",
      "step 1223, loss: 43.838001\n",
      "step 1224, loss: 43.785145\n",
      "step 1225, loss: 43.732330\n",
      "step 1226, loss: 43.679565\n",
      "step 1227, loss: 43.626850\n",
      "step 1228, loss: 43.574188\n",
      "step 1229, loss: 43.521576\n",
      "step 1230, loss: 43.469002\n",
      "step 1231, loss: 43.416481\n",
      "step 1232, loss: 43.364006\n",
      "step 1233, loss: 43.311577\n",
      "step 1234, loss: 43.259205\n",
      "step 1235, loss: 43.206879\n",
      "step 1236, loss: 43.154598\n",
      "step 1237, loss: 43.102360\n",
      "step 1238, loss: 43.050179\n",
      "step 1239, loss: 42.998047\n",
      "step 1240, loss: 42.945957\n",
      "step 1241, loss: 42.893921\n",
      "step 1242, loss: 42.841927\n",
      "step 1243, loss: 42.789982\n",
      "step 1244, loss: 42.738079\n",
      "step 1245, loss: 42.686234\n",
      "step 1246, loss: 42.634438\n",
      "step 1247, loss: 42.582687\n",
      "step 1248, loss: 42.530975\n",
      "step 1249, loss: 42.479317\n",
      "step 1250, loss: 42.427708\n",
      "step 1251, loss: 42.376152\n",
      "step 1252, loss: 42.324638\n",
      "step 1253, loss: 42.273174\n",
      "step 1254, loss: 42.221752\n",
      "step 1255, loss: 42.170380\n",
      "step 1256, loss: 42.119053\n",
      "step 1257, loss: 42.067780\n",
      "step 1258, loss: 42.016556\n",
      "step 1259, loss: 41.965378\n",
      "step 1260, loss: 41.914238\n",
      "step 1261, loss: 41.863152\n",
      "step 1262, loss: 41.812111\n",
      "step 1263, loss: 41.761120\n",
      "step 1264, loss: 41.710175\n",
      "step 1265, loss: 41.659286\n",
      "step 1266, loss: 41.608429\n",
      "step 1267, loss: 41.557632\n",
      "step 1268, loss: 41.506874\n",
      "step 1269, loss: 41.456161\n",
      "step 1270, loss: 41.405506\n",
      "step 1271, loss: 41.354893\n",
      "step 1272, loss: 41.304321\n",
      "step 1273, loss: 41.253803\n",
      "step 1274, loss: 41.203331\n",
      "step 1275, loss: 41.152908\n",
      "step 1276, loss: 41.102531\n",
      "step 1277, loss: 41.052200\n",
      "step 1278, loss: 41.001911\n",
      "step 1279, loss: 40.951675\n",
      "step 1280, loss: 40.901482\n",
      "step 1281, loss: 40.851341\n",
      "step 1282, loss: 40.801243\n",
      "step 1283, loss: 40.751194\n",
      "step 1284, loss: 40.701199\n",
      "step 1285, loss: 40.651237\n",
      "step 1286, loss: 40.601330\n",
      "step 1287, loss: 40.551468\n",
      "step 1288, loss: 40.501652\n",
      "step 1289, loss: 40.451885\n",
      "step 1290, loss: 40.402168\n",
      "step 1291, loss: 40.352489\n",
      "step 1292, loss: 40.302856\n",
      "step 1293, loss: 40.253273\n",
      "step 1294, loss: 40.203735\n",
      "step 1295, loss: 40.154247\n",
      "step 1296, loss: 40.104813\n",
      "step 1297, loss: 40.055405\n",
      "step 1298, loss: 40.006058\n",
      "step 1299, loss: 39.956749\n",
      "step 1300, loss: 39.907494\n",
      "step 1301, loss: 39.858284\n",
      "step 1302, loss: 39.809120\n",
      "step 1303, loss: 39.759998\n",
      "step 1304, loss: 39.710922\n",
      "step 1305, loss: 39.661896\n",
      "step 1306, loss: 39.612915\n",
      "step 1307, loss: 39.563980\n",
      "step 1308, loss: 39.515099\n",
      "step 1309, loss: 39.466259\n",
      "step 1310, loss: 39.417461\n",
      "step 1311, loss: 39.368710\n",
      "step 1312, loss: 39.320007\n",
      "step 1313, loss: 39.271347\n",
      "step 1314, loss: 39.222740\n",
      "step 1315, loss: 39.174179\n",
      "step 1316, loss: 39.125652\n",
      "step 1317, loss: 39.077179\n",
      "step 1318, loss: 39.028748\n",
      "step 1319, loss: 38.980366\n",
      "step 1320, loss: 38.932034\n",
      "step 1321, loss: 38.883743\n",
      "step 1322, loss: 38.835495\n",
      "step 1323, loss: 38.787292\n",
      "step 1324, loss: 38.739140\n",
      "step 1325, loss: 38.691029\n",
      "step 1326, loss: 38.642971\n",
      "step 1327, loss: 38.594955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1328, loss: 38.546989\n",
      "step 1329, loss: 38.499062\n",
      "step 1330, loss: 38.451180\n",
      "step 1331, loss: 38.403343\n",
      "step 1332, loss: 38.355556\n",
      "step 1333, loss: 38.307816\n",
      "step 1334, loss: 38.260120\n",
      "step 1335, loss: 38.212463\n",
      "step 1336, loss: 38.164856\n",
      "step 1337, loss: 38.117287\n",
      "step 1338, loss: 38.069771\n",
      "step 1339, loss: 38.022301\n",
      "step 1340, loss: 37.974884\n",
      "step 1341, loss: 37.927502\n",
      "step 1342, loss: 37.880165\n",
      "step 1343, loss: 37.832870\n",
      "step 1344, loss: 37.785625\n",
      "step 1345, loss: 37.738426\n",
      "step 1346, loss: 37.691273\n",
      "step 1347, loss: 37.644165\n",
      "step 1348, loss: 37.597095\n",
      "step 1349, loss: 37.550076\n",
      "step 1350, loss: 37.503101\n",
      "step 1351, loss: 37.456169\n",
      "step 1352, loss: 37.409283\n",
      "step 1353, loss: 37.362450\n",
      "step 1354, loss: 37.315655\n",
      "step 1355, loss: 37.268902\n",
      "step 1356, loss: 37.222195\n",
      "step 1357, loss: 37.175529\n",
      "step 1358, loss: 37.128918\n",
      "step 1359, loss: 37.082348\n",
      "step 1360, loss: 37.035824\n",
      "step 1361, loss: 36.989346\n",
      "step 1362, loss: 36.942905\n",
      "step 1363, loss: 36.896515\n",
      "step 1364, loss: 36.850166\n",
      "step 1365, loss: 36.803864\n",
      "step 1366, loss: 36.757607\n",
      "step 1367, loss: 36.711391\n",
      "step 1368, loss: 36.665222\n",
      "step 1369, loss: 36.619091\n",
      "step 1370, loss: 36.573009\n",
      "step 1371, loss: 36.526974\n",
      "step 1372, loss: 36.480984\n",
      "step 1373, loss: 36.435036\n",
      "step 1374, loss: 36.389137\n",
      "step 1375, loss: 36.343277\n",
      "step 1376, loss: 36.297459\n",
      "step 1377, loss: 36.251690\n",
      "step 1378, loss: 36.205959\n",
      "step 1379, loss: 36.160282\n",
      "step 1380, loss: 36.114647\n",
      "step 1381, loss: 36.069057\n",
      "step 1382, loss: 36.023502\n",
      "step 1383, loss: 35.977997\n",
      "step 1384, loss: 35.932533\n",
      "step 1385, loss: 35.887119\n",
      "step 1386, loss: 35.841747\n",
      "step 1387, loss: 35.796421\n",
      "step 1388, loss: 35.751137\n",
      "step 1389, loss: 35.705894\n",
      "step 1390, loss: 35.660698\n",
      "step 1391, loss: 35.615540\n",
      "step 1392, loss: 35.570431\n",
      "step 1393, loss: 35.525372\n",
      "step 1394, loss: 35.480347\n",
      "step 1395, loss: 35.435375\n",
      "step 1396, loss: 35.390438\n",
      "step 1397, loss: 35.345547\n",
      "step 1398, loss: 35.300697\n",
      "step 1399, loss: 35.255894\n",
      "step 1400, loss: 35.211136\n",
      "step 1401, loss: 35.166420\n",
      "step 1402, loss: 35.121754\n",
      "step 1403, loss: 35.077126\n",
      "step 1404, loss: 35.032536\n",
      "step 1405, loss: 34.987995\n",
      "step 1406, loss: 34.943497\n",
      "step 1407, loss: 34.899044\n",
      "step 1408, loss: 34.854633\n",
      "step 1409, loss: 34.810272\n",
      "step 1410, loss: 34.765942\n",
      "step 1411, loss: 34.721664\n",
      "step 1412, loss: 34.677422\n",
      "step 1413, loss: 34.633228\n",
      "step 1414, loss: 34.589081\n",
      "step 1415, loss: 34.544971\n",
      "step 1416, loss: 34.500916\n",
      "step 1417, loss: 34.456890\n",
      "step 1418, loss: 34.412907\n",
      "step 1419, loss: 34.368973\n",
      "step 1420, loss: 34.325081\n",
      "step 1421, loss: 34.281235\n",
      "step 1422, loss: 34.237431\n",
      "step 1423, loss: 34.193668\n",
      "step 1424, loss: 34.149948\n",
      "step 1425, loss: 34.106266\n",
      "step 1426, loss: 34.062634\n",
      "step 1427, loss: 34.019043\n",
      "step 1428, loss: 33.975494\n",
      "step 1429, loss: 33.931995\n",
      "step 1430, loss: 33.888535\n",
      "step 1431, loss: 33.845116\n",
      "step 1432, loss: 33.801739\n",
      "step 1433, loss: 33.758404\n",
      "step 1434, loss: 33.715115\n",
      "step 1435, loss: 33.671864\n",
      "step 1436, loss: 33.628662\n",
      "step 1437, loss: 33.585503\n",
      "step 1438, loss: 33.542385\n",
      "step 1439, loss: 33.499302\n",
      "step 1440, loss: 33.456268\n",
      "step 1441, loss: 33.413277\n",
      "step 1442, loss: 33.370323\n",
      "step 1443, loss: 33.327419\n",
      "step 1444, loss: 33.284554\n",
      "step 1445, loss: 33.241737\n",
      "step 1446, loss: 33.198956\n",
      "step 1447, loss: 33.156216\n",
      "step 1448, loss: 33.113518\n",
      "step 1449, loss: 33.070866\n",
      "step 1450, loss: 33.028255\n",
      "step 1451, loss: 32.985691\n",
      "step 1452, loss: 32.943165\n",
      "step 1453, loss: 32.900688\n",
      "step 1454, loss: 32.858238\n",
      "step 1455, loss: 32.815842\n",
      "step 1456, loss: 32.773483\n",
      "step 1457, loss: 32.731167\n",
      "step 1458, loss: 32.688896\n",
      "step 1459, loss: 32.646667\n",
      "step 1460, loss: 32.604485\n",
      "step 1461, loss: 32.562344\n",
      "step 1462, loss: 32.520233\n",
      "step 1463, loss: 32.478172\n",
      "step 1464, loss: 32.436150\n",
      "step 1465, loss: 32.394173\n",
      "step 1466, loss: 32.352238\n",
      "step 1467, loss: 32.310345\n",
      "step 1468, loss: 32.268497\n",
      "step 1469, loss: 32.226685\n",
      "step 1470, loss: 32.184914\n",
      "step 1471, loss: 32.143181\n",
      "step 1472, loss: 32.101494\n",
      "step 1473, loss: 32.059856\n",
      "step 1474, loss: 32.018253\n",
      "step 1475, loss: 31.976696\n",
      "step 1476, loss: 31.935179\n",
      "step 1477, loss: 31.893702\n",
      "step 1478, loss: 31.852264\n",
      "step 1479, loss: 31.810867\n",
      "step 1480, loss: 31.769516\n",
      "step 1481, loss: 31.728209\n",
      "step 1482, loss: 31.686939\n",
      "step 1483, loss: 31.645714\n",
      "step 1484, loss: 31.604530\n",
      "step 1485, loss: 31.563381\n",
      "step 1486, loss: 31.522276\n",
      "step 1487, loss: 31.481215\n",
      "step 1488, loss: 31.440195\n",
      "step 1489, loss: 31.399216\n",
      "step 1490, loss: 31.358278\n",
      "step 1491, loss: 31.317383\n",
      "step 1492, loss: 31.276531\n",
      "step 1493, loss: 31.235716\n",
      "step 1494, loss: 31.194942\n",
      "step 1495, loss: 31.154211\n",
      "step 1496, loss: 31.113520\n",
      "step 1497, loss: 31.072872\n",
      "step 1498, loss: 31.032265\n",
      "step 1499, loss: 30.991695\n",
      "step 1500, loss: 30.951168\n",
      "step 1501, loss: 30.910685\n",
      "step 1502, loss: 30.870241\n",
      "step 1503, loss: 30.829838\n",
      "step 1504, loss: 30.789473\n",
      "step 1505, loss: 30.749153\n",
      "step 1506, loss: 30.708870\n",
      "step 1507, loss: 30.668634\n",
      "step 1508, loss: 30.628435\n",
      "step 1509, loss: 30.588278\n",
      "step 1510, loss: 30.548161\n",
      "step 1511, loss: 30.508085\n",
      "step 1512, loss: 30.468048\n",
      "step 1513, loss: 30.428055\n",
      "step 1514, loss: 30.388100\n",
      "step 1515, loss: 30.348190\n",
      "step 1516, loss: 30.308313\n",
      "step 1517, loss: 30.268482\n",
      "step 1518, loss: 30.228689\n",
      "step 1519, loss: 30.188938\n",
      "step 1520, loss: 30.149229\n",
      "step 1521, loss: 30.109558\n",
      "step 1522, loss: 30.069927\n",
      "step 1523, loss: 30.030340\n",
      "step 1524, loss: 29.990789\n",
      "step 1525, loss: 29.951281\n",
      "step 1526, loss: 29.911812\n",
      "step 1527, loss: 29.872383\n",
      "step 1528, loss: 29.833000\n",
      "step 1529, loss: 29.793653\n",
      "step 1530, loss: 29.754345\n",
      "step 1531, loss: 29.715078\n",
      "step 1532, loss: 29.675854\n",
      "step 1533, loss: 29.636665\n",
      "step 1534, loss: 29.597521\n",
      "step 1535, loss: 29.558414\n",
      "step 1536, loss: 29.519348\n",
      "step 1537, loss: 29.480322\n",
      "step 1538, loss: 29.441336\n",
      "step 1539, loss: 29.402390\n",
      "step 1540, loss: 29.363487\n",
      "step 1541, loss: 29.324623\n",
      "step 1542, loss: 29.285797\n",
      "step 1543, loss: 29.247007\n",
      "step 1544, loss: 29.208263\n",
      "step 1545, loss: 29.169559\n",
      "step 1546, loss: 29.130890\n",
      "step 1547, loss: 29.092266\n",
      "step 1548, loss: 29.053677\n",
      "step 1549, loss: 29.015133\n",
      "step 1550, loss: 28.976627\n",
      "step 1551, loss: 28.938158\n",
      "step 1552, loss: 28.899731\n",
      "step 1553, loss: 28.861343\n",
      "step 1554, loss: 28.822998\n",
      "step 1555, loss: 28.784689\n",
      "step 1556, loss: 28.746418\n",
      "step 1557, loss: 28.708191\n",
      "step 1558, loss: 28.669998\n",
      "step 1559, loss: 28.631851\n",
      "step 1560, loss: 28.593739\n",
      "step 1561, loss: 28.555668\n",
      "step 1562, loss: 28.517635\n",
      "step 1563, loss: 28.479649\n",
      "step 1564, loss: 28.441690\n",
      "step 1565, loss: 28.403776\n",
      "step 1566, loss: 28.365900\n",
      "step 1567, loss: 28.328068\n",
      "step 1568, loss: 28.290272\n",
      "step 1569, loss: 28.252518\n",
      "step 1570, loss: 28.214798\n",
      "step 1571, loss: 28.177122\n",
      "step 1572, loss: 28.139482\n",
      "step 1573, loss: 28.101885\n",
      "step 1574, loss: 28.064323\n",
      "step 1575, loss: 28.026802\n",
      "step 1576, loss: 27.989319\n",
      "step 1577, loss: 27.951880\n",
      "step 1578, loss: 27.914473\n",
      "step 1579, loss: 27.877108\n",
      "step 1580, loss: 27.839779\n",
      "step 1581, loss: 27.802496\n",
      "step 1582, loss: 27.765247\n",
      "step 1583, loss: 27.728037\n",
      "step 1584, loss: 27.690868\n",
      "step 1585, loss: 27.653732\n",
      "step 1586, loss: 27.616642\n",
      "step 1587, loss: 27.579590\n",
      "step 1588, loss: 27.542574\n",
      "step 1589, loss: 27.505596\n",
      "step 1590, loss: 27.468658\n",
      "step 1591, loss: 27.431763\n",
      "step 1592, loss: 27.394899\n",
      "step 1593, loss: 27.358080\n",
      "step 1594, loss: 27.321295\n",
      "step 1595, loss: 27.284552\n",
      "step 1596, loss: 27.247847\n",
      "step 1597, loss: 27.211182\n",
      "step 1598, loss: 27.174553\n",
      "step 1599, loss: 27.137962\n",
      "step 1600, loss: 27.101408\n",
      "step 1601, loss: 27.064898\n",
      "step 1602, loss: 27.028425\n",
      "step 1603, loss: 26.991985\n",
      "step 1604, loss: 26.955589\n",
      "step 1605, loss: 26.919228\n",
      "step 1606, loss: 26.882908\n",
      "step 1607, loss: 26.846628\n",
      "step 1608, loss: 26.810381\n",
      "step 1609, loss: 26.774176\n",
      "step 1610, loss: 26.738005\n",
      "step 1611, loss: 26.701878\n",
      "step 1612, loss: 26.665791\n",
      "step 1613, loss: 26.629734\n",
      "step 1614, loss: 26.593718\n",
      "step 1615, loss: 26.557739\n",
      "step 1616, loss: 26.521801\n",
      "step 1617, loss: 26.485901\n",
      "step 1618, loss: 26.450035\n",
      "step 1619, loss: 26.414209\n",
      "step 1620, loss: 26.378424\n",
      "step 1621, loss: 26.342674\n",
      "step 1622, loss: 26.306965\n",
      "step 1623, loss: 26.271294\n",
      "step 1624, loss: 26.235657\n",
      "step 1625, loss: 26.200058\n",
      "step 1626, loss: 26.164497\n",
      "step 1627, loss: 26.128977\n",
      "step 1628, loss: 26.093494\n",
      "step 1629, loss: 26.058046\n",
      "step 1630, loss: 26.022636\n",
      "step 1631, loss: 25.987267\n",
      "step 1632, loss: 25.951935\n",
      "step 1633, loss: 25.916641\n",
      "step 1634, loss: 25.881382\n",
      "step 1635, loss: 25.846161\n",
      "step 1636, loss: 25.810974\n",
      "step 1637, loss: 25.775833\n",
      "step 1638, loss: 25.740728\n",
      "step 1639, loss: 25.705656\n",
      "step 1640, loss: 25.670622\n",
      "step 1641, loss: 25.635628\n",
      "step 1642, loss: 25.600670\n",
      "step 1643, loss: 25.565750\n",
      "step 1644, loss: 25.530869\n",
      "step 1645, loss: 25.496021\n",
      "step 1646, loss: 25.461212\n",
      "step 1647, loss: 25.426441\n",
      "step 1648, loss: 25.391706\n",
      "step 1649, loss: 25.357012\n",
      "step 1650, loss: 25.322353\n",
      "step 1651, loss: 25.287729\n",
      "step 1652, loss: 25.253143\n",
      "step 1653, loss: 25.218597\n",
      "step 1654, loss: 25.184086\n",
      "step 1655, loss: 25.149612\n",
      "step 1656, loss: 25.115179\n",
      "step 1657, loss: 25.080778\n",
      "step 1658, loss: 25.046415\n",
      "step 1659, loss: 25.012091\n",
      "step 1660, loss: 24.977804\n",
      "step 1661, loss: 24.943550\n",
      "step 1662, loss: 24.909338\n",
      "step 1663, loss: 24.875160\n",
      "step 1664, loss: 24.841019\n",
      "step 1665, loss: 24.806915\n",
      "step 1666, loss: 24.772846\n",
      "step 1667, loss: 24.738817\n",
      "step 1668, loss: 24.704823\n",
      "step 1669, loss: 24.670866\n",
      "step 1670, loss: 24.636946\n",
      "step 1671, loss: 24.603064\n",
      "step 1672, loss: 24.569218\n",
      "step 1673, loss: 24.535410\n",
      "step 1674, loss: 24.501635\n",
      "step 1675, loss: 24.467897\n",
      "step 1676, loss: 24.434195\n",
      "step 1677, loss: 24.400534\n",
      "step 1678, loss: 24.366909\n",
      "step 1679, loss: 24.333317\n",
      "step 1680, loss: 24.299763\n",
      "step 1681, loss: 24.266243\n",
      "step 1682, loss: 24.232761\n",
      "step 1683, loss: 24.199318\n",
      "step 1684, loss: 24.165911\n",
      "step 1685, loss: 24.132538\n",
      "step 1686, loss: 24.099203\n",
      "step 1687, loss: 24.065905\n",
      "step 1688, loss: 24.032640\n",
      "step 1689, loss: 23.999416\n",
      "step 1690, loss: 23.966225\n",
      "step 1691, loss: 23.933071\n",
      "step 1692, loss: 23.899956\n",
      "step 1693, loss: 23.866871\n",
      "step 1694, loss: 23.833824\n",
      "step 1695, loss: 23.800816\n",
      "step 1696, loss: 23.767841\n",
      "step 1697, loss: 23.734905\n",
      "step 1698, loss: 23.702003\n",
      "step 1699, loss: 23.669140\n",
      "step 1700, loss: 23.636309\n",
      "step 1701, loss: 23.603516\n",
      "step 1702, loss: 23.570759\n",
      "step 1703, loss: 23.538038\n",
      "step 1704, loss: 23.505352\n",
      "step 1705, loss: 23.472702\n",
      "step 1706, loss: 23.440088\n",
      "step 1707, loss: 23.407513\n",
      "step 1708, loss: 23.374969\n",
      "step 1709, loss: 23.342461\n",
      "step 1710, loss: 23.309990\n",
      "step 1711, loss: 23.277557\n",
      "step 1712, loss: 23.245159\n",
      "step 1713, loss: 23.212793\n",
      "step 1714, loss: 23.180466\n",
      "step 1715, loss: 23.148172\n",
      "step 1716, loss: 23.115913\n",
      "step 1717, loss: 23.083694\n",
      "step 1718, loss: 23.051508\n",
      "step 1719, loss: 23.019361\n",
      "step 1720, loss: 22.987244\n",
      "step 1721, loss: 22.955164\n",
      "step 1722, loss: 22.923122\n",
      "step 1723, loss: 22.891111\n",
      "step 1724, loss: 22.859140\n",
      "step 1725, loss: 22.827204\n",
      "step 1726, loss: 22.795303\n",
      "step 1727, loss: 22.763435\n",
      "step 1728, loss: 22.731602\n",
      "step 1729, loss: 22.699804\n",
      "step 1730, loss: 22.668045\n",
      "step 1731, loss: 22.636318\n",
      "step 1732, loss: 22.604630\n",
      "step 1733, loss: 22.572975\n",
      "step 1734, loss: 22.541353\n",
      "step 1735, loss: 22.509768\n",
      "step 1736, loss: 22.478218\n",
      "step 1737, loss: 22.446705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1738, loss: 22.415222\n",
      "step 1739, loss: 22.383778\n",
      "step 1740, loss: 22.352371\n",
      "step 1741, loss: 22.320997\n",
      "step 1742, loss: 22.289656\n",
      "step 1743, loss: 22.258348\n",
      "step 1744, loss: 22.227077\n",
      "step 1745, loss: 22.195843\n",
      "step 1746, loss: 22.164642\n",
      "step 1747, loss: 22.133480\n",
      "step 1748, loss: 22.102350\n",
      "step 1749, loss: 22.071255\n",
      "step 1750, loss: 22.040194\n",
      "step 1751, loss: 22.009167\n",
      "step 1752, loss: 21.978174\n",
      "step 1753, loss: 21.947218\n",
      "step 1754, loss: 21.916296\n",
      "step 1755, loss: 21.885408\n",
      "step 1756, loss: 21.854557\n",
      "step 1757, loss: 21.823742\n",
      "step 1758, loss: 21.792953\n",
      "step 1759, loss: 21.762205\n",
      "step 1760, loss: 21.731491\n",
      "step 1761, loss: 21.700809\n",
      "step 1762, loss: 21.670166\n",
      "step 1763, loss: 21.639555\n",
      "step 1764, loss: 21.608976\n",
      "step 1765, loss: 21.578436\n",
      "step 1766, loss: 21.547926\n",
      "step 1767, loss: 21.517454\n",
      "step 1768, loss: 21.487015\n",
      "step 1769, loss: 21.456608\n",
      "step 1770, loss: 21.426237\n",
      "step 1771, loss: 21.395901\n",
      "step 1772, loss: 21.365602\n",
      "step 1773, loss: 21.335333\n",
      "step 1774, loss: 21.305099\n",
      "step 1775, loss: 21.274899\n",
      "step 1776, loss: 21.244732\n",
      "step 1777, loss: 21.214600\n",
      "step 1778, loss: 21.184502\n",
      "step 1779, loss: 21.154438\n",
      "step 1780, loss: 21.124409\n",
      "step 1781, loss: 21.094418\n",
      "step 1782, loss: 21.064455\n",
      "step 1783, loss: 21.034525\n",
      "step 1784, loss: 21.004633\n",
      "step 1785, loss: 20.974773\n",
      "step 1786, loss: 20.944946\n",
      "step 1787, loss: 20.915154\n",
      "step 1788, loss: 20.885397\n",
      "step 1789, loss: 20.855671\n",
      "step 1790, loss: 20.825985\n",
      "step 1791, loss: 20.796329\n",
      "step 1792, loss: 20.766705\n",
      "step 1793, loss: 20.737116\n",
      "step 1794, loss: 20.707558\n",
      "step 1795, loss: 20.678036\n",
      "step 1796, loss: 20.648548\n",
      "step 1797, loss: 20.619093\n",
      "step 1798, loss: 20.589672\n",
      "step 1799, loss: 20.560287\n",
      "step 1800, loss: 20.530933\n",
      "step 1801, loss: 20.501617\n",
      "step 1802, loss: 20.472328\n",
      "step 1803, loss: 20.443073\n",
      "step 1804, loss: 20.413853\n",
      "step 1805, loss: 20.384666\n",
      "step 1806, loss: 20.355515\n",
      "step 1807, loss: 20.326397\n",
      "step 1808, loss: 20.297310\n",
      "step 1809, loss: 20.268257\n",
      "step 1810, loss: 20.239239\n",
      "step 1811, loss: 20.210255\n",
      "step 1812, loss: 20.181301\n",
      "step 1813, loss: 20.152380\n",
      "step 1814, loss: 20.123493\n",
      "step 1815, loss: 20.094639\n",
      "step 1816, loss: 20.065819\n",
      "step 1817, loss: 20.037031\n",
      "step 1818, loss: 20.008278\n",
      "step 1819, loss: 19.979559\n",
      "step 1820, loss: 19.950871\n",
      "step 1821, loss: 19.922218\n",
      "step 1822, loss: 19.893597\n",
      "step 1823, loss: 19.865009\n",
      "step 1824, loss: 19.836452\n",
      "step 1825, loss: 19.807930\n",
      "step 1826, loss: 19.779438\n",
      "step 1827, loss: 19.750982\n",
      "step 1828, loss: 19.722557\n",
      "step 1829, loss: 19.694168\n",
      "step 1830, loss: 19.665810\n",
      "step 1831, loss: 19.637486\n",
      "step 1832, loss: 19.609194\n",
      "step 1833, loss: 19.580935\n",
      "step 1834, loss: 19.552708\n",
      "step 1835, loss: 19.524515\n",
      "step 1836, loss: 19.496353\n",
      "step 1837, loss: 19.468220\n",
      "step 1838, loss: 19.440125\n",
      "step 1839, loss: 19.412060\n",
      "step 1840, loss: 19.384029\n",
      "step 1841, loss: 19.356031\n",
      "step 1842, loss: 19.328066\n",
      "step 1843, loss: 19.300135\n",
      "step 1844, loss: 19.272236\n",
      "step 1845, loss: 19.244366\n",
      "step 1846, loss: 19.216532\n",
      "step 1847, loss: 19.188726\n",
      "step 1848, loss: 19.160955\n",
      "step 1849, loss: 19.133217\n",
      "step 1850, loss: 19.105509\n",
      "step 1851, loss: 19.077835\n",
      "step 1852, loss: 19.050192\n",
      "step 1853, loss: 19.022583\n",
      "step 1854, loss: 18.995007\n",
      "step 1855, loss: 18.967461\n",
      "step 1856, loss: 18.939949\n",
      "step 1857, loss: 18.912470\n",
      "step 1858, loss: 18.885023\n",
      "step 1859, loss: 18.857609\n",
      "step 1860, loss: 18.830225\n",
      "step 1861, loss: 18.802872\n",
      "step 1862, loss: 18.775551\n",
      "step 1863, loss: 18.748262\n",
      "step 1864, loss: 18.721006\n",
      "step 1865, loss: 18.693783\n",
      "step 1866, loss: 18.666590\n",
      "step 1867, loss: 18.639429\n",
      "step 1868, loss: 18.612303\n",
      "step 1869, loss: 18.585205\n",
      "step 1870, loss: 18.558142\n",
      "step 1871, loss: 18.531111\n",
      "step 1872, loss: 18.504110\n",
      "step 1873, loss: 18.477144\n",
      "step 1874, loss: 18.450207\n",
      "step 1875, loss: 18.423306\n",
      "step 1876, loss: 18.396431\n",
      "step 1877, loss: 18.369587\n",
      "step 1878, loss: 18.342777\n",
      "step 1879, loss: 18.315998\n",
      "step 1880, loss: 18.289251\n",
      "step 1881, loss: 18.262535\n",
      "step 1882, loss: 18.235853\n",
      "step 1883, loss: 18.209200\n",
      "step 1884, loss: 18.182579\n",
      "step 1885, loss: 18.155993\n",
      "step 1886, loss: 18.129436\n",
      "step 1887, loss: 18.102909\n",
      "step 1888, loss: 18.076416\n",
      "step 1889, loss: 18.049955\n",
      "step 1890, loss: 18.023523\n",
      "step 1891, loss: 17.997126\n",
      "step 1892, loss: 17.970758\n",
      "step 1893, loss: 17.944422\n",
      "step 1894, loss: 17.918116\n",
      "step 1895, loss: 17.891840\n",
      "step 1896, loss: 17.865595\n",
      "step 1897, loss: 17.839384\n",
      "step 1898, loss: 17.813202\n",
      "step 1899, loss: 17.787050\n",
      "step 1900, loss: 17.760933\n",
      "step 1901, loss: 17.734844\n",
      "step 1902, loss: 17.708790\n",
      "step 1903, loss: 17.682762\n",
      "step 1904, loss: 17.656769\n",
      "step 1905, loss: 17.630806\n",
      "step 1906, loss: 17.604876\n",
      "step 1907, loss: 17.578974\n",
      "step 1908, loss: 17.553106\n",
      "step 1909, loss: 17.527267\n",
      "step 1910, loss: 17.501461\n",
      "step 1911, loss: 17.475685\n",
      "step 1912, loss: 17.449940\n",
      "step 1913, loss: 17.424225\n",
      "step 1914, loss: 17.398542\n",
      "step 1915, loss: 17.372887\n",
      "step 1916, loss: 17.347263\n",
      "step 1917, loss: 17.321671\n",
      "step 1918, loss: 17.296108\n",
      "step 1919, loss: 17.270578\n",
      "step 1920, loss: 17.245077\n",
      "step 1921, loss: 17.219606\n",
      "step 1922, loss: 17.194168\n",
      "step 1923, loss: 17.168758\n",
      "step 1924, loss: 17.143381\n",
      "step 1925, loss: 17.118034\n",
      "step 1926, loss: 17.092718\n",
      "step 1927, loss: 17.067434\n",
      "step 1928, loss: 17.042179\n",
      "step 1929, loss: 17.016954\n",
      "step 1930, loss: 16.991760\n",
      "step 1931, loss: 16.966599\n",
      "step 1932, loss: 16.941465\n",
      "step 1933, loss: 16.916363\n",
      "step 1934, loss: 16.891293\n",
      "step 1935, loss: 16.866249\n",
      "step 1936, loss: 16.841240\n",
      "step 1937, loss: 16.816259\n",
      "step 1938, loss: 16.791307\n",
      "step 1939, loss: 16.766388\n",
      "step 1940, loss: 16.741499\n",
      "step 1941, loss: 16.716640\n",
      "step 1942, loss: 16.691811\n",
      "step 1943, loss: 16.667013\n",
      "step 1944, loss: 16.642244\n",
      "step 1945, loss: 16.617506\n",
      "step 1946, loss: 16.592796\n",
      "step 1947, loss: 16.568115\n",
      "step 1948, loss: 16.543465\n",
      "step 1949, loss: 16.518845\n",
      "step 1950, loss: 16.494253\n",
      "step 1951, loss: 16.469694\n",
      "step 1952, loss: 16.445164\n",
      "step 1953, loss: 16.420664\n",
      "step 1954, loss: 16.396193\n",
      "step 1955, loss: 16.371754\n",
      "step 1956, loss: 16.347343\n",
      "step 1957, loss: 16.322964\n",
      "step 1958, loss: 16.298611\n",
      "step 1959, loss: 16.274290\n",
      "step 1960, loss: 16.250000\n",
      "step 1961, loss: 16.225739\n",
      "step 1962, loss: 16.201508\n",
      "step 1963, loss: 16.177307\n",
      "step 1964, loss: 16.153133\n",
      "step 1965, loss: 16.128992\n",
      "step 1966, loss: 16.104879\n",
      "step 1967, loss: 16.080797\n",
      "step 1968, loss: 16.056744\n",
      "step 1969, loss: 16.032721\n",
      "step 1970, loss: 16.008726\n",
      "step 1971, loss: 15.984761\n",
      "step 1972, loss: 15.960827\n",
      "step 1973, loss: 15.936921\n",
      "step 1974, loss: 15.913046\n",
      "step 1975, loss: 15.889199\n",
      "step 1976, loss: 15.865381\n",
      "step 1977, loss: 15.841595\n",
      "step 1978, loss: 15.817837\n",
      "step 1979, loss: 15.794106\n",
      "step 1980, loss: 15.770408\n",
      "step 1981, loss: 15.746737\n",
      "step 1982, loss: 15.723097\n",
      "step 1983, loss: 15.699486\n",
      "step 1984, loss: 15.675903\n",
      "step 1985, loss: 15.652349\n",
      "step 1986, loss: 15.628825\n",
      "step 1987, loss: 15.605331\n",
      "step 1988, loss: 15.581864\n",
      "step 1989, loss: 15.558429\n",
      "step 1990, loss: 15.535021\n",
      "step 1991, loss: 15.511642\n",
      "step 1992, loss: 15.488293\n",
      "step 1993, loss: 15.464972\n",
      "step 1994, loss: 15.441681\n",
      "step 1995, loss: 15.418419\n",
      "step 1996, loss: 15.395185\n",
      "step 1997, loss: 15.371981\n",
      "step 1998, loss: 15.348804\n",
      "step 1999, loss: 15.325657\n",
      "step 2000, loss: 15.302541\n",
      "step 2001, loss: 15.279451\n",
      "step 2002, loss: 15.256391\n",
      "step 2003, loss: 15.233358\n",
      "step 2004, loss: 15.210356\n",
      "step 2005, loss: 15.187383\n",
      "step 2006, loss: 15.164436\n",
      "step 2007, loss: 15.141520\n",
      "step 2008, loss: 15.118631\n",
      "step 2009, loss: 15.095772\n",
      "step 2010, loss: 15.072941\n",
      "step 2011, loss: 15.050138\n",
      "step 2012, loss: 15.027366\n",
      "step 2013, loss: 15.004621\n",
      "step 2014, loss: 14.981903\n",
      "step 2015, loss: 14.959215\n",
      "step 2016, loss: 14.936556\n",
      "step 2017, loss: 14.913924\n",
      "step 2018, loss: 14.891321\n",
      "step 2019, loss: 14.868748\n",
      "step 2020, loss: 14.846201\n",
      "step 2021, loss: 14.823684\n",
      "step 2022, loss: 14.801193\n",
      "step 2023, loss: 14.778733\n",
      "step 2024, loss: 14.756300\n",
      "step 2025, loss: 14.733896\n",
      "step 2026, loss: 14.711520\n",
      "step 2027, loss: 14.689171\n",
      "step 2028, loss: 14.666851\n",
      "step 2029, loss: 14.644559\n",
      "step 2030, loss: 14.622295\n",
      "step 2031, loss: 14.600060\n",
      "step 2032, loss: 14.577853\n",
      "step 2033, loss: 14.555674\n",
      "step 2034, loss: 14.533522\n",
      "step 2035, loss: 14.511398\n",
      "step 2036, loss: 14.489303\n",
      "step 2037, loss: 14.467237\n",
      "step 2038, loss: 14.445200\n",
      "step 2039, loss: 14.423192\n",
      "step 2040, loss: 14.401211\n",
      "step 2041, loss: 14.379257\n",
      "step 2042, loss: 14.357330\n",
      "step 2043, loss: 14.335433\n",
      "step 2044, loss: 14.313562\n",
      "step 2045, loss: 14.291720\n",
      "step 2046, loss: 14.269905\n",
      "step 2047, loss: 14.248119\n",
      "step 2048, loss: 14.226360\n",
      "step 2049, loss: 14.204629\n",
      "step 2050, loss: 14.182924\n",
      "step 2051, loss: 14.161248\n",
      "step 2052, loss: 14.139599\n",
      "step 2053, loss: 14.117978\n",
      "step 2054, loss: 14.096386\n",
      "step 2055, loss: 14.074819\n",
      "step 2056, loss: 14.053282\n",
      "step 2057, loss: 14.031770\n",
      "step 2058, loss: 14.010285\n",
      "step 2059, loss: 13.988831\n",
      "step 2060, loss: 13.967402\n",
      "step 2061, loss: 13.946000\n",
      "step 2062, loss: 13.924626\n",
      "step 2063, loss: 13.903280\n",
      "step 2064, loss: 13.881960\n",
      "step 2065, loss: 13.860669\n",
      "step 2066, loss: 13.839403\n",
      "step 2067, loss: 13.818166\n",
      "step 2068, loss: 13.796959\n",
      "step 2069, loss: 13.775778\n",
      "step 2070, loss: 13.754623\n",
      "step 2071, loss: 13.733498\n",
      "step 2072, loss: 13.712399\n",
      "step 2073, loss: 13.691326\n",
      "step 2074, loss: 13.670280\n",
      "step 2075, loss: 13.649263\n",
      "step 2076, loss: 13.628272\n",
      "step 2077, loss: 13.607307\n",
      "step 2078, loss: 13.586371\n",
      "step 2079, loss: 13.565460\n",
      "step 2080, loss: 13.544579\n",
      "step 2081, loss: 13.523722\n",
      "step 2082, loss: 13.502892\n",
      "step 2083, loss: 13.482090\n",
      "step 2084, loss: 13.461314\n",
      "step 2085, loss: 13.440566\n",
      "step 2086, loss: 13.419844\n",
      "step 2087, loss: 13.399148\n",
      "step 2088, loss: 13.378480\n",
      "step 2089, loss: 13.357841\n",
      "step 2090, loss: 13.337229\n",
      "step 2091, loss: 13.316641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2092, loss: 13.296082\n",
      "step 2093, loss: 13.275549\n",
      "step 2094, loss: 13.255044\n",
      "step 2095, loss: 13.234564\n",
      "step 2096, loss: 13.214111\n",
      "step 2097, loss: 13.193684\n",
      "step 2098, loss: 13.173285\n",
      "step 2099, loss: 13.152912\n",
      "step 2100, loss: 13.132565\n",
      "step 2101, loss: 13.112242\n",
      "step 2102, loss: 13.091949\n",
      "step 2103, loss: 13.071682\n",
      "step 2104, loss: 13.051439\n",
      "step 2105, loss: 13.031225\n",
      "step 2106, loss: 13.011037\n",
      "step 2107, loss: 12.990876\n",
      "step 2108, loss: 12.970743\n",
      "step 2109, loss: 12.950634\n",
      "step 2110, loss: 12.930553\n",
      "step 2111, loss: 12.910498\n",
      "step 2112, loss: 12.890469\n",
      "step 2113, loss: 12.870466\n",
      "step 2114, loss: 12.850491\n",
      "step 2115, loss: 12.830538\n",
      "step 2116, loss: 12.810614\n",
      "step 2117, loss: 12.790716\n",
      "step 2118, loss: 12.770844\n",
      "step 2119, loss: 12.750998\n",
      "step 2120, loss: 12.731177\n",
      "step 2121, loss: 12.711383\n",
      "step 2122, loss: 12.691616\n",
      "step 2123, loss: 12.671877\n",
      "step 2124, loss: 12.652162\n",
      "step 2125, loss: 12.632474\n",
      "step 2126, loss: 12.612810\n",
      "step 2127, loss: 12.593175\n",
      "step 2128, loss: 12.573563\n",
      "step 2129, loss: 12.553978\n",
      "step 2130, loss: 12.534419\n",
      "step 2131, loss: 12.514886\n",
      "step 2132, loss: 12.495378\n",
      "step 2133, loss: 12.475895\n",
      "step 2134, loss: 12.456439\n",
      "step 2135, loss: 12.437009\n",
      "step 2136, loss: 12.417605\n",
      "step 2137, loss: 12.398228\n",
      "step 2138, loss: 12.378876\n",
      "step 2139, loss: 12.359550\n",
      "step 2140, loss: 12.340249\n",
      "step 2141, loss: 12.320973\n",
      "step 2142, loss: 12.301723\n",
      "step 2143, loss: 12.282501\n",
      "step 2144, loss: 12.263301\n",
      "step 2145, loss: 12.244127\n",
      "step 2146, loss: 12.224980\n",
      "step 2147, loss: 12.205856\n",
      "step 2148, loss: 12.186762\n",
      "step 2149, loss: 12.167692\n",
      "step 2150, loss: 12.148649\n",
      "step 2151, loss: 12.129629\n",
      "step 2152, loss: 12.110635\n",
      "step 2153, loss: 12.091667\n",
      "step 2154, loss: 12.072723\n",
      "step 2155, loss: 12.053804\n",
      "step 2156, loss: 12.034912\n",
      "step 2157, loss: 12.016043\n",
      "step 2158, loss: 11.997201\n",
      "step 2159, loss: 11.978383\n",
      "step 2160, loss: 11.959593\n",
      "step 2161, loss: 11.940827\n",
      "step 2162, loss: 11.922087\n",
      "step 2163, loss: 11.903372\n",
      "step 2164, loss: 11.884682\n",
      "step 2165, loss: 11.866015\n",
      "step 2166, loss: 11.847375\n",
      "step 2167, loss: 11.828760\n",
      "step 2168, loss: 11.810168\n",
      "step 2169, loss: 11.791603\n",
      "step 2170, loss: 11.773062\n",
      "step 2171, loss: 11.754549\n",
      "step 2172, loss: 11.736059\n",
      "step 2173, loss: 11.717594\n",
      "step 2174, loss: 11.699156\n",
      "step 2175, loss: 11.680740\n",
      "step 2176, loss: 11.662350\n",
      "step 2177, loss: 11.643984\n",
      "step 2178, loss: 11.625644\n",
      "step 2179, loss: 11.607328\n",
      "step 2180, loss: 11.589035\n",
      "step 2181, loss: 11.570772\n",
      "step 2182, loss: 11.552532\n",
      "step 2183, loss: 11.534316\n",
      "step 2184, loss: 11.516125\n",
      "step 2185, loss: 11.497958\n",
      "step 2186, loss: 11.479816\n",
      "step 2187, loss: 11.461698\n",
      "step 2188, loss: 11.443605\n",
      "step 2189, loss: 11.425536\n",
      "step 2190, loss: 11.407492\n",
      "step 2191, loss: 11.389473\n",
      "step 2192, loss: 11.371480\n",
      "step 2193, loss: 11.353511\n",
      "step 2194, loss: 11.335567\n",
      "step 2195, loss: 11.317646\n",
      "step 2196, loss: 11.299749\n",
      "step 2197, loss: 11.281876\n",
      "step 2198, loss: 11.264030\n",
      "step 2199, loss: 11.246205\n",
      "step 2200, loss: 11.228406\n",
      "step 2201, loss: 11.210633\n",
      "step 2202, loss: 11.192884\n",
      "step 2203, loss: 11.175159\n",
      "step 2204, loss: 11.157458\n",
      "step 2205, loss: 11.139782\n",
      "step 2206, loss: 11.122128\n",
      "step 2207, loss: 11.104500\n",
      "step 2208, loss: 11.086895\n",
      "step 2209, loss: 11.069316\n",
      "step 2210, loss: 11.051762\n",
      "step 2211, loss: 11.034229\n",
      "step 2212, loss: 11.016723\n",
      "step 2213, loss: 10.999239\n",
      "step 2214, loss: 10.981780\n",
      "step 2215, loss: 10.964345\n",
      "step 2216, loss: 10.946932\n",
      "step 2217, loss: 10.929544\n",
      "step 2218, loss: 10.912182\n",
      "step 2219, loss: 10.894843\n",
      "step 2220, loss: 10.877529\n",
      "step 2221, loss: 10.860238\n",
      "step 2222, loss: 10.842972\n",
      "step 2223, loss: 10.825727\n",
      "step 2224, loss: 10.808508\n",
      "step 2225, loss: 10.791311\n",
      "step 2226, loss: 10.774140\n",
      "step 2227, loss: 10.756993\n",
      "step 2228, loss: 10.739870\n",
      "step 2229, loss: 10.722769\n",
      "step 2230, loss: 10.705692\n",
      "step 2231, loss: 10.688639\n",
      "step 2232, loss: 10.671610\n",
      "step 2233, loss: 10.654604\n",
      "step 2234, loss: 10.637621\n",
      "step 2235, loss: 10.620665\n",
      "step 2236, loss: 10.603731\n",
      "step 2237, loss: 10.586819\n",
      "step 2238, loss: 10.569933\n",
      "step 2239, loss: 10.553068\n",
      "step 2240, loss: 10.536228\n",
      "step 2241, loss: 10.519409\n",
      "step 2242, loss: 10.502617\n",
      "step 2243, loss: 10.485847\n",
      "step 2244, loss: 10.469103\n",
      "step 2245, loss: 10.452379\n",
      "step 2246, loss: 10.435679\n",
      "step 2247, loss: 10.419003\n",
      "step 2248, loss: 10.402349\n",
      "step 2249, loss: 10.385720\n",
      "step 2250, loss: 10.369116\n",
      "step 2251, loss: 10.352533\n",
      "step 2252, loss: 10.335974\n",
      "step 2253, loss: 10.319437\n",
      "step 2254, loss: 10.302924\n",
      "step 2255, loss: 10.286434\n",
      "step 2256, loss: 10.269967\n",
      "step 2257, loss: 10.253524\n",
      "step 2258, loss: 10.237104\n",
      "step 2259, loss: 10.220707\n",
      "step 2260, loss: 10.204333\n",
      "step 2261, loss: 10.187983\n",
      "step 2262, loss: 10.171654\n",
      "step 2263, loss: 10.155348\n",
      "step 2264, loss: 10.139067\n",
      "step 2265, loss: 10.122809\n",
      "step 2266, loss: 10.106573\n",
      "step 2267, loss: 10.090361\n",
      "step 2268, loss: 10.074170\n",
      "step 2269, loss: 10.058002\n",
      "step 2270, loss: 10.041858\n",
      "step 2271, loss: 10.025738\n",
      "step 2272, loss: 10.009640\n",
      "step 2273, loss: 9.993565\n",
      "step 2274, loss: 9.977512\n",
      "step 2275, loss: 9.961481\n",
      "step 2276, loss: 9.945474\n",
      "step 2277, loss: 9.929489\n",
      "step 2278, loss: 9.913528\n",
      "step 2279, loss: 9.897590\n",
      "step 2280, loss: 9.881674\n",
      "step 2281, loss: 9.865780\n",
      "step 2282, loss: 9.849909\n",
      "step 2283, loss: 9.834060\n",
      "step 2284, loss: 9.818233\n",
      "step 2285, loss: 9.802430\n",
      "step 2286, loss: 9.786651\n",
      "step 2287, loss: 9.770892\n",
      "step 2288, loss: 9.755157\n",
      "step 2289, loss: 9.739443\n",
      "step 2290, loss: 9.723751\n",
      "step 2291, loss: 9.708084\n",
      "step 2292, loss: 9.692439\n",
      "step 2293, loss: 9.676817\n",
      "step 2294, loss: 9.661216\n",
      "step 2295, loss: 9.645636\n",
      "step 2296, loss: 9.630079\n",
      "step 2297, loss: 9.614545\n",
      "step 2298, loss: 9.599033\n",
      "step 2299, loss: 9.583545\n",
      "step 2300, loss: 9.568078\n",
      "step 2301, loss: 9.552632\n",
      "step 2302, loss: 9.537210\n",
      "step 2303, loss: 9.521809\n",
      "step 2304, loss: 9.506432\n",
      "step 2305, loss: 9.491076\n",
      "step 2306, loss: 9.475742\n",
      "step 2307, loss: 9.460430\n",
      "step 2308, loss: 9.445140\n",
      "step 2309, loss: 9.429873\n",
      "step 2310, loss: 9.414626\n",
      "step 2311, loss: 9.399405\n",
      "step 2312, loss: 9.384203\n",
      "step 2313, loss: 9.369023\n",
      "step 2314, loss: 9.353865\n",
      "step 2315, loss: 9.338729\n",
      "step 2316, loss: 9.323616\n",
      "step 2317, loss: 9.308525\n",
      "step 2318, loss: 9.293455\n",
      "step 2319, loss: 9.278408\n",
      "step 2320, loss: 9.263381\n",
      "step 2321, loss: 9.248376\n",
      "step 2322, loss: 9.233395\n",
      "step 2323, loss: 9.218433\n",
      "step 2324, loss: 9.203495\n",
      "step 2325, loss: 9.188578\n",
      "step 2326, loss: 9.173681\n",
      "step 2327, loss: 9.158807\n",
      "step 2328, loss: 9.143957\n",
      "step 2329, loss: 9.129126\n",
      "step 2330, loss: 9.114317\n",
      "step 2331, loss: 9.099529\n",
      "step 2332, loss: 9.084764\n",
      "step 2333, loss: 9.070019\n",
      "step 2334, loss: 9.055297\n",
      "step 2335, loss: 9.040597\n",
      "step 2336, loss: 9.025917\n",
      "step 2337, loss: 9.011259\n",
      "step 2338, loss: 8.996622\n",
      "step 2339, loss: 8.982006\n",
      "step 2340, loss: 8.967413\n",
      "step 2341, loss: 8.952842\n",
      "step 2342, loss: 8.938290\n",
      "step 2343, loss: 8.923760\n",
      "step 2344, loss: 8.909252\n",
      "step 2345, loss: 8.894764\n",
      "step 2346, loss: 8.880300\n",
      "step 2347, loss: 8.865854\n",
      "step 2348, loss: 8.851433\n",
      "step 2349, loss: 8.837029\n",
      "step 2350, loss: 8.822649\n",
      "step 2351, loss: 8.808290\n",
      "step 2352, loss: 8.793952\n",
      "step 2353, loss: 8.779635\n",
      "step 2354, loss: 8.765339\n",
      "step 2355, loss: 8.751065\n",
      "step 2356, loss: 8.736812\n",
      "step 2357, loss: 8.722579\n",
      "step 2358, loss: 8.708367\n",
      "step 2359, loss: 8.694177\n",
      "step 2360, loss: 8.680007\n",
      "step 2361, loss: 8.665858\n",
      "step 2362, loss: 8.651730\n",
      "step 2363, loss: 8.637625\n",
      "step 2364, loss: 8.623539\n",
      "step 2365, loss: 8.609474\n",
      "step 2366, loss: 8.595430\n",
      "step 2367, loss: 8.581408\n",
      "step 2368, loss: 8.567405\n",
      "step 2369, loss: 8.553424\n",
      "step 2370, loss: 8.539463\n",
      "step 2371, loss: 8.525523\n",
      "step 2372, loss: 8.511605\n",
      "step 2373, loss: 8.497707\n",
      "step 2374, loss: 8.483829\n",
      "step 2375, loss: 8.469973\n",
      "step 2376, loss: 8.456137\n",
      "step 2377, loss: 8.442321\n",
      "step 2378, loss: 8.428526\n",
      "step 2379, loss: 8.414752\n",
      "step 2380, loss: 8.400999\n",
      "step 2381, loss: 8.387266\n",
      "step 2382, loss: 8.373554\n",
      "step 2383, loss: 8.359861\n",
      "step 2384, loss: 8.346190\n",
      "step 2385, loss: 8.332540\n",
      "step 2386, loss: 8.318910\n",
      "step 2387, loss: 8.305300\n",
      "step 2388, loss: 8.291710\n",
      "step 2389, loss: 8.278141\n",
      "step 2390, loss: 8.264592\n",
      "step 2391, loss: 8.251064\n",
      "step 2392, loss: 8.237556\n",
      "step 2393, loss: 8.224069\n",
      "step 2394, loss: 8.210601\n",
      "step 2395, loss: 8.197154\n",
      "step 2396, loss: 8.183727\n",
      "step 2397, loss: 8.170321\n",
      "step 2398, loss: 8.156936\n",
      "step 2399, loss: 8.143569\n",
      "step 2400, loss: 8.130222\n",
      "step 2401, loss: 8.116897\n",
      "step 2402, loss: 8.103592\n",
      "step 2403, loss: 8.090307\n",
      "step 2404, loss: 8.077042\n",
      "step 2405, loss: 8.063795\n",
      "step 2406, loss: 8.050571\n",
      "step 2407, loss: 8.037366\n",
      "step 2408, loss: 8.024180\n",
      "step 2409, loss: 8.011016\n",
      "step 2410, loss: 7.997871\n",
      "step 2411, loss: 7.984746\n",
      "step 2412, loss: 7.971641\n",
      "step 2413, loss: 7.958556\n",
      "step 2414, loss: 7.945491\n",
      "step 2415, loss: 7.932446\n",
      "step 2416, loss: 7.919420\n",
      "step 2417, loss: 7.906416\n",
      "step 2418, loss: 7.893429\n",
      "step 2419, loss: 7.880464\n",
      "step 2420, loss: 7.867518\n",
      "step 2421, loss: 7.854591\n",
      "step 2422, loss: 7.841685\n",
      "step 2423, loss: 7.828797\n",
      "step 2424, loss: 7.815931\n",
      "step 2425, loss: 7.803082\n",
      "step 2426, loss: 7.790257\n",
      "step 2427, loss: 7.777448\n",
      "step 2428, loss: 7.764659\n",
      "step 2429, loss: 7.751890\n",
      "step 2430, loss: 7.739141\n",
      "step 2431, loss: 7.726411\n",
      "step 2432, loss: 7.713702\n",
      "step 2433, loss: 7.701010\n",
      "step 2434, loss: 7.688339\n",
      "step 2435, loss: 7.675687\n",
      "step 2436, loss: 7.663055\n",
      "step 2437, loss: 7.650441\n",
      "step 2438, loss: 7.637849\n",
      "step 2439, loss: 7.625275\n",
      "step 2440, loss: 7.612720\n",
      "step 2441, loss: 7.600185\n",
      "step 2442, loss: 7.587668\n",
      "step 2443, loss: 7.575172\n",
      "step 2444, loss: 7.562695\n",
      "step 2445, loss: 7.550238\n",
      "step 2446, loss: 7.537798\n",
      "step 2447, loss: 7.525380\n",
      "step 2448, loss: 7.512980\n",
      "step 2449, loss: 7.500598\n",
      "step 2450, loss: 7.488236\n",
      "step 2451, loss: 7.475894\n",
      "step 2452, loss: 7.463571\n",
      "step 2453, loss: 7.451266\n",
      "step 2454, loss: 7.438982\n",
      "step 2455, loss: 7.426715\n",
      "step 2456, loss: 7.414467\n",
      "step 2457, loss: 7.402240\n",
      "step 2458, loss: 7.390030\n",
      "step 2459, loss: 7.377841\n",
      "step 2460, loss: 7.365670\n",
      "step 2461, loss: 7.353518\n",
      "step 2462, loss: 7.341385\n",
      "step 2463, loss: 7.329271\n",
      "step 2464, loss: 7.317175\n",
      "step 2465, loss: 7.305099\n",
      "step 2466, loss: 7.293042\n",
      "step 2467, loss: 7.281003\n",
      "step 2468, loss: 7.268983\n",
      "step 2469, loss: 7.256982\n",
      "step 2470, loss: 7.245000\n",
      "step 2471, loss: 7.233037\n",
      "step 2472, loss: 7.221092\n",
      "step 2473, loss: 7.209166\n",
      "step 2474, loss: 7.197260\n",
      "step 2475, loss: 7.185371\n",
      "step 2476, loss: 7.173501\n",
      "step 2477, loss: 7.161649\n",
      "step 2478, loss: 7.149817\n",
      "step 2479, loss: 7.138003\n",
      "step 2480, loss: 7.126208\n",
      "step 2481, loss: 7.114431\n",
      "step 2482, loss: 7.102673\n",
      "step 2483, loss: 7.090933\n",
      "step 2484, loss: 7.079212\n",
      "step 2485, loss: 7.067509\n",
      "step 2486, loss: 7.055824\n",
      "step 2487, loss: 7.044160\n",
      "step 2488, loss: 7.032513\n",
      "step 2489, loss: 7.020884\n",
      "step 2490, loss: 7.009273\n",
      "step 2491, loss: 6.997681\n",
      "step 2492, loss: 6.986107\n",
      "step 2493, loss: 6.974551\n",
      "step 2494, loss: 6.963015\n",
      "step 2495, loss: 6.951496\n",
      "step 2496, loss: 6.939996\n",
      "step 2497, loss: 6.928513\n",
      "step 2498, loss: 6.917049\n",
      "step 2499, loss: 6.905603\n",
      "step 2500, loss: 6.894176\n",
      "step 2501, loss: 6.882767\n",
      "step 2502, loss: 6.871376\n",
      "step 2503, loss: 6.860003\n",
      "step 2504, loss: 6.848648\n",
      "step 2505, loss: 6.837311\n",
      "step 2506, loss: 6.825993\n",
      "step 2507, loss: 6.814692\n",
      "step 2508, loss: 6.803409\n",
      "step 2509, loss: 6.792144\n",
      "step 2510, loss: 6.780897\n",
      "step 2511, loss: 6.769669\n",
      "step 2512, loss: 6.758458\n",
      "step 2513, loss: 6.747265\n",
      "step 2514, loss: 6.736091\n",
      "step 2515, loss: 6.724934\n",
      "step 2516, loss: 6.713795\n",
      "step 2517, loss: 6.702674\n",
      "step 2518, loss: 6.691570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2519, loss: 6.680484\n",
      "step 2520, loss: 6.669417\n",
      "step 2521, loss: 6.658368\n",
      "step 2522, loss: 6.647335\n",
      "step 2523, loss: 6.636321\n",
      "step 2524, loss: 6.625325\n",
      "step 2525, loss: 6.614347\n",
      "step 2526, loss: 6.603385\n",
      "step 2527, loss: 6.592442\n",
      "step 2528, loss: 6.581516\n",
      "step 2529, loss: 6.570608\n",
      "step 2530, loss: 6.559718\n",
      "step 2531, loss: 6.548845\n",
      "step 2532, loss: 6.537990\n",
      "step 2533, loss: 6.527152\n",
      "step 2534, loss: 6.516332\n",
      "step 2535, loss: 6.505529\n",
      "step 2536, loss: 6.494744\n",
      "step 2537, loss: 6.483977\n",
      "step 2538, loss: 6.473226\n",
      "step 2539, loss: 6.462494\n",
      "step 2540, loss: 6.451778\n",
      "step 2541, loss: 6.441081\n",
      "step 2542, loss: 6.430400\n",
      "step 2543, loss: 6.419737\n",
      "step 2544, loss: 6.409091\n",
      "step 2545, loss: 6.398462\n",
      "step 2546, loss: 6.387851\n",
      "step 2547, loss: 6.377256\n",
      "step 2548, loss: 6.366679\n",
      "step 2549, loss: 6.356120\n",
      "step 2550, loss: 6.345578\n",
      "step 2551, loss: 6.335052\n",
      "step 2552, loss: 6.324544\n",
      "step 2553, loss: 6.314054\n",
      "step 2554, loss: 6.303580\n",
      "step 2555, loss: 6.293123\n",
      "step 2556, loss: 6.282685\n",
      "step 2557, loss: 6.272263\n",
      "step 2558, loss: 6.261858\n",
      "step 2559, loss: 6.251470\n",
      "step 2560, loss: 6.241099\n",
      "step 2561, loss: 6.230746\n",
      "step 2562, loss: 6.220408\n",
      "step 2563, loss: 6.210090\n",
      "step 2564, loss: 6.199786\n",
      "step 2565, loss: 6.189501\n",
      "step 2566, loss: 6.179232\n",
      "step 2567, loss: 6.168980\n",
      "step 2568, loss: 6.158744\n",
      "step 2569, loss: 6.148527\n",
      "step 2570, loss: 6.138325\n",
      "step 2571, loss: 6.128140\n",
      "step 2572, loss: 6.117973\n",
      "step 2573, loss: 6.107822\n",
      "step 2574, loss: 6.097688\n",
      "step 2575, loss: 6.087570\n",
      "step 2576, loss: 6.077470\n",
      "step 2577, loss: 6.067386\n",
      "step 2578, loss: 6.057318\n",
      "step 2579, loss: 6.047267\n",
      "step 2580, loss: 6.037234\n",
      "step 2581, loss: 6.027217\n",
      "step 2582, loss: 6.017215\n",
      "step 2583, loss: 6.007231\n",
      "step 2584, loss: 5.997263\n",
      "step 2585, loss: 5.987313\n",
      "step 2586, loss: 5.977378\n",
      "step 2587, loss: 5.967460\n",
      "step 2588, loss: 5.957559\n",
      "step 2589, loss: 5.947674\n",
      "step 2590, loss: 5.937806\n",
      "step 2591, loss: 5.927954\n",
      "step 2592, loss: 5.918118\n",
      "step 2593, loss: 5.908299\n",
      "step 2594, loss: 5.898497\n",
      "step 2595, loss: 5.888710\n",
      "step 2596, loss: 5.878941\n",
      "step 2597, loss: 5.869187\n",
      "step 2598, loss: 5.859450\n",
      "step 2599, loss: 5.849729\n",
      "step 2600, loss: 5.840025\n",
      "step 2601, loss: 5.830336\n",
      "step 2602, loss: 5.820664\n",
      "step 2603, loss: 5.811008\n",
      "step 2604, loss: 5.801368\n",
      "step 2605, loss: 5.791745\n",
      "step 2606, loss: 5.782138\n",
      "step 2607, loss: 5.772547\n",
      "step 2608, loss: 5.762972\n",
      "step 2609, loss: 5.753412\n",
      "step 2610, loss: 5.743870\n",
      "step 2611, loss: 5.734343\n",
      "step 2612, loss: 5.724832\n",
      "step 2613, loss: 5.715337\n",
      "step 2614, loss: 5.705859\n",
      "step 2615, loss: 5.696396\n",
      "step 2616, loss: 5.686950\n",
      "step 2617, loss: 5.677520\n",
      "step 2618, loss: 5.668105\n",
      "step 2619, loss: 5.658707\n",
      "step 2620, loss: 5.649323\n",
      "step 2621, loss: 5.639956\n",
      "step 2622, loss: 5.630606\n",
      "step 2623, loss: 5.621271\n",
      "step 2624, loss: 5.611951\n",
      "step 2625, loss: 5.602648\n",
      "step 2626, loss: 5.593359\n",
      "step 2627, loss: 5.584087\n",
      "step 2628, loss: 5.574831\n",
      "step 2629, loss: 5.565591\n",
      "step 2630, loss: 5.556367\n",
      "step 2631, loss: 5.547158\n",
      "step 2632, loss: 5.537965\n",
      "step 2633, loss: 5.528787\n",
      "step 2634, loss: 5.519626\n",
      "step 2635, loss: 5.510479\n",
      "step 2636, loss: 5.501349\n",
      "step 2637, loss: 5.492233\n",
      "step 2638, loss: 5.483134\n",
      "step 2639, loss: 5.474050\n",
      "step 2640, loss: 5.464982\n",
      "step 2641, loss: 5.455929\n",
      "step 2642, loss: 5.446892\n",
      "step 2643, loss: 5.437870\n",
      "step 2644, loss: 5.428864\n",
      "step 2645, loss: 5.419873\n",
      "step 2646, loss: 5.410898\n",
      "step 2647, loss: 5.401937\n",
      "step 2648, loss: 5.392993\n",
      "step 2649, loss: 5.384064\n",
      "step 2650, loss: 5.375150\n",
      "step 2651, loss: 5.366251\n",
      "step 2652, loss: 5.357368\n",
      "step 2653, loss: 5.348500\n",
      "step 2654, loss: 5.339647\n",
      "step 2655, loss: 5.330810\n",
      "step 2656, loss: 5.321989\n",
      "step 2657, loss: 5.313181\n",
      "step 2658, loss: 5.304390\n",
      "step 2659, loss: 5.295614\n",
      "step 2660, loss: 5.286852\n",
      "step 2661, loss: 5.278107\n",
      "step 2662, loss: 5.269375\n",
      "step 2663, loss: 5.260659\n",
      "step 2664, loss: 5.251959\n",
      "step 2665, loss: 5.243273\n",
      "step 2666, loss: 5.234602\n",
      "step 2667, loss: 5.225948\n",
      "step 2668, loss: 5.217307\n",
      "step 2669, loss: 5.208682\n",
      "step 2670, loss: 5.200072\n",
      "step 2671, loss: 5.191477\n",
      "step 2672, loss: 5.182896\n",
      "step 2673, loss: 5.174331\n",
      "step 2674, loss: 5.165779\n",
      "step 2675, loss: 5.157244\n",
      "step 2676, loss: 5.148724\n",
      "step 2677, loss: 5.140217\n",
      "step 2678, loss: 5.131727\n",
      "step 2679, loss: 5.123250\n",
      "step 2680, loss: 5.114790\n",
      "step 2681, loss: 5.106343\n",
      "step 2682, loss: 5.097911\n",
      "step 2683, loss: 5.089494\n",
      "step 2684, loss: 5.081092\n",
      "step 2685, loss: 5.072704\n",
      "step 2686, loss: 5.064332\n",
      "step 2687, loss: 5.055974\n",
      "step 2688, loss: 5.047630\n",
      "step 2689, loss: 5.039302\n",
      "step 2690, loss: 5.030988\n",
      "step 2691, loss: 5.022688\n",
      "step 2692, loss: 5.014403\n",
      "step 2693, loss: 5.006133\n",
      "step 2694, loss: 4.997877\n",
      "step 2695, loss: 4.989636\n",
      "step 2696, loss: 4.981410\n",
      "step 2697, loss: 4.973198\n",
      "step 2698, loss: 4.965000\n",
      "step 2699, loss: 4.956817\n",
      "step 2700, loss: 4.948648\n",
      "step 2701, loss: 4.940494\n",
      "step 2702, loss: 4.932354\n",
      "step 2703, loss: 4.924229\n",
      "step 2704, loss: 4.916118\n",
      "step 2705, loss: 4.908021\n",
      "step 2706, loss: 4.899939\n",
      "step 2707, loss: 4.891871\n",
      "step 2708, loss: 4.883817\n",
      "step 2709, loss: 4.875779\n",
      "step 2710, loss: 4.867753\n",
      "step 2711, loss: 4.859743\n",
      "step 2712, loss: 4.851746\n",
      "step 2713, loss: 4.843764\n",
      "step 2714, loss: 4.835795\n",
      "step 2715, loss: 4.827841\n",
      "step 2716, loss: 4.819902\n",
      "step 2717, loss: 4.811976\n",
      "step 2718, loss: 4.804065\n",
      "step 2719, loss: 4.796168\n",
      "step 2720, loss: 4.788285\n",
      "step 2721, loss: 4.780416\n",
      "step 2722, loss: 4.772561\n",
      "step 2723, loss: 4.764720\n",
      "step 2724, loss: 4.756893\n",
      "step 2725, loss: 4.749081\n",
      "step 2726, loss: 4.741282\n",
      "step 2727, loss: 4.733497\n",
      "step 2728, loss: 4.725726\n",
      "step 2729, loss: 4.717969\n",
      "step 2730, loss: 4.710226\n",
      "step 2731, loss: 4.702497\n",
      "step 2732, loss: 4.694782\n",
      "step 2733, loss: 4.687080\n",
      "step 2734, loss: 4.679393\n",
      "step 2735, loss: 4.671720\n",
      "step 2736, loss: 4.664060\n",
      "step 2737, loss: 4.656414\n",
      "step 2738, loss: 4.648782\n",
      "step 2739, loss: 4.641163\n",
      "step 2740, loss: 4.633559\n",
      "step 2741, loss: 4.625968\n",
      "step 2742, loss: 4.618391\n",
      "step 2743, loss: 4.610827\n",
      "step 2744, loss: 4.603278\n",
      "step 2745, loss: 4.595741\n",
      "step 2746, loss: 4.588219\n",
      "step 2747, loss: 4.580711\n",
      "step 2748, loss: 4.573215\n",
      "step 2749, loss: 4.565734\n",
      "step 2750, loss: 4.558266\n",
      "step 2751, loss: 4.550812\n",
      "step 2752, loss: 4.543371\n",
      "step 2753, loss: 4.535944\n",
      "step 2754, loss: 4.528530\n",
      "step 2755, loss: 4.521130\n",
      "step 2756, loss: 4.513743\n",
      "step 2757, loss: 4.506370\n",
      "step 2758, loss: 4.499010\n",
      "step 2759, loss: 4.491664\n",
      "step 2760, loss: 4.484331\n",
      "step 2761, loss: 4.477011\n",
      "step 2762, loss: 4.469705\n",
      "step 2763, loss: 4.462412\n",
      "step 2764, loss: 4.455132\n",
      "step 2765, loss: 4.447866\n",
      "step 2766, loss: 4.440614\n",
      "step 2767, loss: 4.433374\n",
      "step 2768, loss: 4.426148\n",
      "step 2769, loss: 4.418934\n",
      "step 2770, loss: 4.411735\n",
      "step 2771, loss: 4.404548\n",
      "step 2772, loss: 4.397375\n",
      "step 2773, loss: 4.390214\n",
      "step 2774, loss: 4.383068\n",
      "step 2775, loss: 4.375933\n",
      "step 2776, loss: 4.368813\n",
      "step 2777, loss: 4.361706\n",
      "step 2778, loss: 4.354611\n",
      "step 2779, loss: 4.347530\n",
      "step 2780, loss: 4.340462\n",
      "step 2781, loss: 4.333406\n",
      "step 2782, loss: 4.326364\n",
      "step 2783, loss: 4.319335\n",
      "step 2784, loss: 4.312318\n",
      "step 2785, loss: 4.305315\n",
      "step 2786, loss: 4.298326\n",
      "step 2787, loss: 4.291348\n",
      "step 2788, loss: 4.284384\n",
      "step 2789, loss: 4.277432\n",
      "step 2790, loss: 4.270494\n",
      "step 2791, loss: 4.263568\n",
      "step 2792, loss: 4.256655\n",
      "step 2793, loss: 4.249755\n",
      "step 2794, loss: 4.242868\n",
      "step 2795, loss: 4.235993\n",
      "step 2796, loss: 4.229132\n",
      "step 2797, loss: 4.222284\n",
      "step 2798, loss: 4.215447\n",
      "step 2799, loss: 4.208625\n",
      "step 2800, loss: 4.201814\n",
      "step 2801, loss: 4.195016\n",
      "step 2802, loss: 4.188231\n",
      "step 2803, loss: 4.181459\n",
      "step 2804, loss: 4.174699\n",
      "step 2805, loss: 4.167952\n",
      "step 2806, loss: 4.161218\n",
      "step 2807, loss: 4.154496\n",
      "step 2808, loss: 4.147787\n",
      "step 2809, loss: 4.141090\n",
      "step 2810, loss: 4.134406\n",
      "step 2811, loss: 4.127735\n",
      "step 2812, loss: 4.121076\n",
      "step 2813, loss: 4.114429\n",
      "step 2814, loss: 4.107795\n",
      "step 2815, loss: 4.101173\n",
      "step 2816, loss: 4.094565\n",
      "step 2817, loss: 4.087968\n",
      "step 2818, loss: 4.081384\n",
      "step 2819, loss: 4.074813\n",
      "step 2820, loss: 4.068253\n",
      "step 2821, loss: 4.061707\n",
      "step 2822, loss: 4.055171\n",
      "step 2823, loss: 4.048649\n",
      "step 2824, loss: 4.042140\n",
      "step 2825, loss: 4.035642\n",
      "step 2826, loss: 4.029157\n",
      "step 2827, loss: 4.022684\n",
      "step 2828, loss: 4.016223\n",
      "step 2829, loss: 4.009775\n",
      "step 2830, loss: 4.003338\n",
      "step 2831, loss: 3.996915\n",
      "step 2832, loss: 3.990503\n",
      "step 2833, loss: 3.984103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2834, loss: 3.977715\n",
      "step 2835, loss: 3.971340\n",
      "step 2836, loss: 3.964977\n",
      "step 2837, loss: 3.958627\n",
      "step 2838, loss: 3.952287\n",
      "step 2839, loss: 3.945961\n",
      "step 2840, loss: 3.939646\n",
      "step 2841, loss: 3.933343\n",
      "step 2842, loss: 3.927052\n",
      "step 2843, loss: 3.920774\n",
      "step 2844, loss: 3.914507\n",
      "step 2845, loss: 3.908252\n",
      "step 2846, loss: 3.902010\n",
      "step 2847, loss: 3.895779\n",
      "step 2848, loss: 3.889560\n",
      "step 2849, loss: 3.883353\n",
      "step 2850, loss: 3.877158\n",
      "step 2851, loss: 3.870975\n",
      "step 2852, loss: 3.864804\n",
      "step 2853, loss: 3.858645\n",
      "step 2854, loss: 3.852497\n",
      "step 2855, loss: 3.846362\n",
      "step 2856, loss: 3.840238\n",
      "step 2857, loss: 3.834126\n",
      "step 2858, loss: 3.828025\n",
      "step 2859, loss: 3.821937\n",
      "step 2860, loss: 3.815860\n",
      "step 2861, loss: 3.809795\n",
      "step 2862, loss: 3.803742\n",
      "step 2863, loss: 3.797700\n",
      "step 2864, loss: 3.791670\n",
      "step 2865, loss: 3.785652\n",
      "step 2866, loss: 3.779645\n",
      "step 2867, loss: 3.773650\n",
      "step 2868, loss: 3.767666\n",
      "step 2869, loss: 3.761695\n",
      "step 2870, loss: 3.755734\n",
      "step 2871, loss: 3.749785\n",
      "step 2872, loss: 3.743849\n",
      "step 2873, loss: 3.737923\n",
      "step 2874, loss: 3.732009\n",
      "step 2875, loss: 3.726106\n",
      "step 2876, loss: 3.720215\n",
      "step 2877, loss: 3.714336\n",
      "step 2878, loss: 3.708467\n",
      "step 2879, loss: 3.702611\n",
      "step 2880, loss: 3.696765\n",
      "step 2881, loss: 3.690932\n",
      "step 2882, loss: 3.685109\n",
      "step 2883, loss: 3.679298\n",
      "step 2884, loss: 3.673498\n",
      "step 2885, loss: 3.667710\n",
      "step 2886, loss: 3.661932\n",
      "step 2887, loss: 3.656167\n",
      "step 2888, loss: 3.650412\n",
      "step 2889, loss: 3.644669\n",
      "step 2890, loss: 3.638937\n",
      "step 2891, loss: 3.633216\n",
      "step 2892, loss: 3.627507\n",
      "step 2893, loss: 3.621809\n",
      "step 2894, loss: 3.616121\n",
      "step 2895, loss: 3.610446\n",
      "step 2896, loss: 3.604781\n",
      "step 2897, loss: 3.599128\n",
      "step 2898, loss: 3.593486\n",
      "step 2899, loss: 3.587854\n",
      "step 2900, loss: 3.582234\n",
      "step 2901, loss: 3.576625\n",
      "step 2902, loss: 3.571027\n",
      "step 2903, loss: 3.565440\n",
      "step 2904, loss: 3.559864\n",
      "step 2905, loss: 3.554299\n",
      "step 2906, loss: 3.548746\n",
      "step 2907, loss: 3.543203\n",
      "step 2908, loss: 3.537671\n",
      "step 2909, loss: 3.532150\n",
      "step 2910, loss: 3.526639\n",
      "step 2911, loss: 3.521140\n",
      "step 2912, loss: 3.515652\n",
      "step 2913, loss: 3.510175\n",
      "step 2914, loss: 3.504708\n",
      "step 2915, loss: 3.499253\n",
      "step 2916, loss: 3.493808\n",
      "step 2917, loss: 3.488374\n",
      "step 2918, loss: 3.482951\n",
      "step 2919, loss: 3.477539\n",
      "step 2920, loss: 3.472138\n",
      "step 2921, loss: 3.466747\n",
      "step 2922, loss: 3.461368\n",
      "step 2923, loss: 3.455999\n",
      "step 2924, loss: 3.450641\n",
      "step 2925, loss: 3.445293\n",
      "step 2926, loss: 3.439956\n",
      "step 2927, loss: 3.434630\n",
      "step 2928, loss: 3.429315\n",
      "step 2929, loss: 3.424010\n",
      "step 2930, loss: 3.418715\n",
      "step 2931, loss: 3.413432\n",
      "step 2932, loss: 3.408159\n",
      "step 2933, loss: 3.402896\n",
      "step 2934, loss: 3.397645\n",
      "step 2935, loss: 3.392404\n",
      "step 2936, loss: 3.387173\n",
      "step 2937, loss: 3.381953\n",
      "step 2938, loss: 3.376743\n",
      "step 2939, loss: 3.371544\n",
      "step 2940, loss: 3.366356\n",
      "step 2941, loss: 3.361177\n",
      "step 2942, loss: 3.356009\n",
      "step 2943, loss: 3.350852\n",
      "step 2944, loss: 3.345706\n",
      "step 2945, loss: 3.340569\n",
      "step 2946, loss: 3.335443\n",
      "step 2947, loss: 3.330327\n",
      "step 2948, loss: 3.325222\n",
      "step 2949, loss: 3.320127\n",
      "step 2950, loss: 3.315042\n",
      "step 2951, loss: 3.309968\n",
      "step 2952, loss: 3.304904\n",
      "step 2953, loss: 3.299850\n",
      "step 2954, loss: 3.294806\n",
      "step 2955, loss: 3.289773\n",
      "step 2956, loss: 3.284750\n",
      "step 2957, loss: 3.279737\n",
      "step 2958, loss: 3.274734\n",
      "step 2959, loss: 3.269742\n",
      "step 2960, loss: 3.264759\n",
      "step 2961, loss: 3.259788\n",
      "step 2962, loss: 3.254826\n",
      "step 2963, loss: 3.249874\n",
      "step 2964, loss: 3.244931\n",
      "step 2965, loss: 3.240000\n",
      "step 2966, loss: 3.235079\n",
      "step 2967, loss: 3.230167\n",
      "step 2968, loss: 3.225266\n",
      "step 2969, loss: 3.220375\n",
      "step 2970, loss: 3.215494\n",
      "step 2971, loss: 3.210622\n",
      "step 2972, loss: 3.205761\n",
      "step 2973, loss: 3.200910\n",
      "step 2974, loss: 3.196069\n",
      "step 2975, loss: 3.191238\n",
      "step 2976, loss: 3.186417\n",
      "step 2977, loss: 3.181605\n",
      "step 2978, loss: 3.176803\n",
      "step 2979, loss: 3.172011\n",
      "step 2980, loss: 3.167230\n",
      "step 2981, loss: 3.162458\n",
      "step 2982, loss: 3.157696\n",
      "step 2983, loss: 3.152943\n",
      "step 2984, loss: 3.148201\n",
      "step 2985, loss: 3.143469\n",
      "step 2986, loss: 3.138746\n",
      "step 2987, loss: 3.134033\n",
      "step 2988, loss: 3.129330\n",
      "step 2989, loss: 3.124636\n",
      "step 2990, loss: 3.119952\n",
      "step 2991, loss: 3.115279\n",
      "step 2992, loss: 3.110614\n",
      "step 2993, loss: 3.105960\n",
      "step 2994, loss: 3.101315\n",
      "step 2995, loss: 3.096679\n",
      "step 2996, loss: 3.092054\n",
      "step 2997, loss: 3.087438\n",
      "step 2998, loss: 3.082831\n",
      "step 2999, loss: 3.078235\n",
      "step 3000, loss: 3.073647\n",
      "step 3001, loss: 3.069069\n",
      "step 3002, loss: 3.064502\n",
      "step 3003, loss: 3.059943\n",
      "step 3004, loss: 3.055394\n",
      "step 3005, loss: 3.050855\n",
      "step 3006, loss: 3.046325\n",
      "step 3007, loss: 3.041804\n",
      "step 3008, loss: 3.037293\n",
      "step 3009, loss: 3.032792\n",
      "step 3010, loss: 3.028299\n",
      "step 3011, loss: 3.023816\n",
      "step 3012, loss: 3.019343\n",
      "step 3013, loss: 3.014880\n",
      "step 3014, loss: 3.010425\n",
      "step 3015, loss: 3.005980\n",
      "step 3016, loss: 3.001544\n",
      "step 3017, loss: 2.997118\n",
      "step 3018, loss: 2.992700\n",
      "step 3019, loss: 2.988292\n",
      "step 3020, loss: 2.983895\n",
      "step 3021, loss: 2.979505\n",
      "step 3022, loss: 2.975125\n",
      "step 3023, loss: 2.970755\n",
      "step 3024, loss: 2.966393\n",
      "step 3025, loss: 2.962041\n",
      "step 3026, loss: 2.957698\n",
      "step 3027, loss: 2.953364\n",
      "step 3028, loss: 2.949039\n",
      "step 3029, loss: 2.944724\n",
      "step 3030, loss: 2.940417\n",
      "step 3031, loss: 2.936121\n",
      "step 3032, loss: 2.931832\n",
      "step 3033, loss: 2.927553\n",
      "step 3034, loss: 2.923284\n",
      "step 3035, loss: 2.919023\n",
      "step 3036, loss: 2.914771\n",
      "step 3037, loss: 2.910529\n",
      "step 3038, loss: 2.906295\n",
      "step 3039, loss: 2.902071\n",
      "step 3040, loss: 2.897855\n",
      "step 3041, loss: 2.893648\n",
      "step 3042, loss: 2.889451\n",
      "step 3043, loss: 2.885262\n",
      "step 3044, loss: 2.881083\n",
      "step 3045, loss: 2.876912\n",
      "step 3046, loss: 2.872750\n",
      "step 3047, loss: 2.868597\n",
      "step 3048, loss: 2.864453\n",
      "step 3049, loss: 2.860318\n",
      "step 3050, loss: 2.856192\n",
      "step 3051, loss: 2.852075\n",
      "step 3052, loss: 2.847966\n",
      "step 3053, loss: 2.843867\n",
      "step 3054, loss: 2.839776\n",
      "step 3055, loss: 2.835694\n",
      "step 3056, loss: 2.831620\n",
      "step 3057, loss: 2.827556\n",
      "step 3058, loss: 2.823500\n",
      "step 3059, loss: 2.819453\n",
      "step 3060, loss: 2.815415\n",
      "step 3061, loss: 2.811385\n",
      "step 3062, loss: 2.807365\n",
      "step 3063, loss: 2.803353\n",
      "step 3064, loss: 2.799349\n",
      "step 3065, loss: 2.795354\n",
      "step 3066, loss: 2.791368\n",
      "step 3067, loss: 2.787391\n",
      "step 3068, loss: 2.783422\n",
      "step 3069, loss: 2.779462\n",
      "step 3070, loss: 2.775510\n",
      "step 3071, loss: 2.771567\n",
      "step 3072, loss: 2.767632\n",
      "step 3073, loss: 2.763707\n",
      "step 3074, loss: 2.759789\n",
      "step 3075, loss: 2.755880\n",
      "step 3076, loss: 2.751980\n",
      "step 3077, loss: 2.748088\n",
      "step 3078, loss: 2.744205\n",
      "step 3079, loss: 2.740330\n",
      "step 3080, loss: 2.736463\n",
      "step 3081, loss: 2.732605\n",
      "step 3082, loss: 2.728755\n",
      "step 3083, loss: 2.724914\n",
      "step 3084, loss: 2.721081\n",
      "step 3085, loss: 2.717257\n",
      "step 3086, loss: 2.713441\n",
      "step 3087, loss: 2.709633\n",
      "step 3088, loss: 2.705834\n",
      "step 3089, loss: 2.702043\n",
      "step 3090, loss: 2.698260\n",
      "step 3091, loss: 2.694486\n",
      "step 3092, loss: 2.690720\n",
      "step 3093, loss: 2.686962\n",
      "step 3094, loss: 2.683213\n",
      "step 3095, loss: 2.679471\n",
      "step 3096, loss: 2.675738\n",
      "step 3097, loss: 2.672013\n",
      "step 3098, loss: 2.668296\n",
      "step 3099, loss: 2.664588\n",
      "step 3100, loss: 2.660888\n",
      "step 3101, loss: 2.657196\n",
      "step 3102, loss: 2.653512\n",
      "step 3103, loss: 2.649836\n",
      "step 3104, loss: 2.646168\n",
      "step 3105, loss: 2.642509\n",
      "step 3106, loss: 2.638858\n",
      "step 3107, loss: 2.635214\n",
      "step 3108, loss: 2.631579\n",
      "step 3109, loss: 2.627952\n",
      "step 3110, loss: 2.624333\n",
      "step 3111, loss: 2.620722\n",
      "step 3112, loss: 2.617119\n",
      "step 3113, loss: 2.613524\n",
      "step 3114, loss: 2.609937\n",
      "step 3115, loss: 2.606358\n",
      "step 3116, loss: 2.602787\n",
      "step 3117, loss: 2.599224\n",
      "step 3118, loss: 2.595669\n",
      "step 3119, loss: 2.592121\n",
      "step 3120, loss: 2.588582\n",
      "step 3121, loss: 2.585050\n",
      "step 3122, loss: 2.581527\n",
      "step 3123, loss: 2.578011\n",
      "step 3124, loss: 2.574504\n",
      "step 3125, loss: 2.571004\n",
      "step 3126, loss: 2.567512\n",
      "step 3127, loss: 2.564027\n",
      "step 3128, loss: 2.560551\n",
      "step 3129, loss: 2.557082\n",
      "step 3130, loss: 2.553621\n",
      "step 3131, loss: 2.550169\n",
      "step 3132, loss: 2.546723\n",
      "step 3133, loss: 2.543286\n",
      "step 3134, loss: 2.539856\n",
      "step 3135, loss: 2.536434\n",
      "step 3136, loss: 2.533020\n",
      "step 3137, loss: 2.529613\n",
      "step 3138, loss: 2.526214\n",
      "step 3139, loss: 2.522823\n",
      "step 3140, loss: 2.519439\n",
      "step 3141, loss: 2.516063\n",
      "step 3142, loss: 2.512695\n",
      "step 3143, loss: 2.509334\n",
      "step 3144, loss: 2.505981\n",
      "step 3145, loss: 2.502635\n",
      "step 3146, loss: 2.499297\n",
      "step 3147, loss: 2.495966\n",
      "step 3148, loss: 2.492644\n",
      "step 3149, loss: 2.489329\n",
      "step 3150, loss: 2.486021\n",
      "step 3151, loss: 2.482720\n",
      "step 3152, loss: 2.479428\n",
      "step 3153, loss: 2.476142\n",
      "step 3154, loss: 2.472865\n",
      "step 3155, loss: 2.469594\n",
      "step 3156, loss: 2.466332\n",
      "step 3157, loss: 2.463076\n",
      "step 3158, loss: 2.459828\n",
      "step 3159, loss: 2.456588\n",
      "step 3160, loss: 2.453355\n",
      "step 3161, loss: 2.450129\n",
      "step 3162, loss: 2.446910\n",
      "step 3163, loss: 2.443699\n",
      "step 3164, loss: 2.440496\n",
      "step 3165, loss: 2.437299\n",
      "step 3166, loss: 2.434110\n",
      "step 3167, loss: 2.430928\n",
      "step 3168, loss: 2.427754\n",
      "step 3169, loss: 2.424587\n",
      "step 3170, loss: 2.421427\n",
      "step 3171, loss: 2.418275\n",
      "step 3172, loss: 2.415129\n",
      "step 3173, loss: 2.411991\n",
      "step 3174, loss: 2.408860\n",
      "step 3175, loss: 2.405736\n",
      "step 3176, loss: 2.402620\n",
      "step 3177, loss: 2.399511\n",
      "step 3178, loss: 2.396409\n",
      "step 3179, loss: 2.393314\n",
      "step 3180, loss: 2.390226\n",
      "step 3181, loss: 2.387146\n",
      "step 3182, loss: 2.384072\n",
      "step 3183, loss: 2.381006\n",
      "step 3184, loss: 2.377946\n",
      "step 3185, loss: 2.374895\n",
      "step 3186, loss: 2.371849\n",
      "step 3187, loss: 2.368811\n",
      "step 3188, loss: 2.365780\n",
      "step 3189, loss: 2.362756\n",
      "step 3190, loss: 2.359739\n",
      "step 3191, loss: 2.356729\n",
      "step 3192, loss: 2.353726\n",
      "step 3193, loss: 2.350730\n",
      "step 3194, loss: 2.347741\n",
      "step 3195, loss: 2.344759\n",
      "step 3196, loss: 2.341784\n",
      "step 3197, loss: 2.338816\n",
      "step 3198, loss: 2.335855\n",
      "step 3199, loss: 2.332901\n",
      "step 3200, loss: 2.329953\n",
      "step 3201, loss: 2.327013\n",
      "step 3202, loss: 2.324080\n",
      "step 3203, loss: 2.321153\n",
      "step 3204, loss: 2.318233\n",
      "step 3205, loss: 2.315320\n",
      "step 3206, loss: 2.312414\n",
      "step 3207, loss: 2.309515\n",
      "step 3208, loss: 2.306622\n",
      "step 3209, loss: 2.303736\n",
      "step 3210, loss: 2.300857\n",
      "step 3211, loss: 2.297985\n",
      "step 3212, loss: 2.295120\n",
      "step 3213, loss: 2.292261\n",
      "step 3214, loss: 2.289409\n",
      "step 3215, loss: 2.286564\n",
      "step 3216, loss: 2.283725\n",
      "step 3217, loss: 2.280894\n",
      "step 3218, loss: 2.278068\n",
      "step 3219, loss: 2.275250\n",
      "step 3220, loss: 2.272438\n",
      "step 3221, loss: 2.269633\n",
      "step 3222, loss: 2.266834\n",
      "step 3223, loss: 2.264043\n",
      "step 3224, loss: 2.261257\n",
      "step 3225, loss: 2.258479\n",
      "step 3226, loss: 2.255707\n",
      "step 3227, loss: 2.252941\n",
      "step 3228, loss: 2.250182\n",
      "step 3229, loss: 2.247430\n",
      "step 3230, loss: 2.244684\n",
      "step 3231, loss: 2.241945\n",
      "step 3232, loss: 2.239212\n",
      "step 3233, loss: 2.236486\n",
      "step 3234, loss: 2.233766\n",
      "step 3235, loss: 2.231053\n",
      "step 3236, loss: 2.228346\n",
      "step 3237, loss: 2.225646\n",
      "step 3238, loss: 2.222952\n",
      "step 3239, loss: 2.220265\n",
      "step 3240, loss: 2.217584\n",
      "step 3241, loss: 2.214909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3242, loss: 2.212241\n",
      "step 3243, loss: 2.209580\n",
      "step 3244, loss: 2.206924\n",
      "step 3245, loss: 2.204275\n",
      "step 3246, loss: 2.201632\n",
      "step 3247, loss: 2.198996\n",
      "step 3248, loss: 2.196366\n",
      "step 3249, loss: 2.193743\n",
      "step 3250, loss: 2.191125\n",
      "step 3251, loss: 2.188514\n",
      "step 3252, loss: 2.185910\n",
      "step 3253, loss: 2.183311\n",
      "step 3254, loss: 2.180719\n",
      "step 3255, loss: 2.178133\n",
      "step 3256, loss: 2.175553\n",
      "step 3257, loss: 2.172980\n",
      "step 3258, loss: 2.170412\n",
      "step 3259, loss: 2.167851\n",
      "step 3260, loss: 2.165297\n",
      "step 3261, loss: 2.162748\n",
      "step 3262, loss: 2.160205\n",
      "step 3263, loss: 2.157669\n",
      "step 3264, loss: 2.155139\n",
      "step 3265, loss: 2.152615\n",
      "step 3266, loss: 2.150097\n",
      "step 3267, loss: 2.147585\n",
      "step 3268, loss: 2.145080\n",
      "step 3269, loss: 2.142580\n",
      "step 3270, loss: 2.140086\n",
      "step 3271, loss: 2.137599\n",
      "step 3272, loss: 2.135117\n",
      "step 3273, loss: 2.132642\n",
      "step 3274, loss: 2.130173\n",
      "step 3275, loss: 2.127709\n",
      "step 3276, loss: 2.125252\n",
      "step 3277, loss: 2.122801\n",
      "step 3278, loss: 2.120356\n",
      "step 3279, loss: 2.117917\n",
      "step 3280, loss: 2.115483\n",
      "step 3281, loss: 2.113056\n",
      "step 3282, loss: 2.110635\n",
      "step 3283, loss: 2.108219\n",
      "step 3284, loss: 2.105809\n",
      "step 3285, loss: 2.103406\n",
      "step 3286, loss: 2.101008\n",
      "step 3287, loss: 2.098616\n",
      "step 3288, loss: 2.096230\n",
      "step 3289, loss: 2.093850\n",
      "step 3290, loss: 2.091476\n",
      "step 3291, loss: 2.089108\n",
      "step 3292, loss: 2.086745\n",
      "step 3293, loss: 2.084388\n",
      "step 3294, loss: 2.082037\n",
      "step 3295, loss: 2.079692\n",
      "step 3296, loss: 2.077353\n",
      "step 3297, loss: 2.075019\n",
      "step 3298, loss: 2.072692\n",
      "step 3299, loss: 2.070369\n",
      "step 3300, loss: 2.068053\n",
      "step 3301, loss: 2.065743\n",
      "step 3302, loss: 2.063438\n",
      "step 3303, loss: 2.061139\n",
      "step 3304, loss: 2.058846\n",
      "step 3305, loss: 2.056558\n",
      "step 3306, loss: 2.054276\n",
      "step 3307, loss: 2.052000\n",
      "step 3308, loss: 2.049729\n",
      "step 3309, loss: 2.047464\n",
      "step 3310, loss: 2.045205\n",
      "step 3311, loss: 2.042952\n",
      "step 3312, loss: 2.040704\n",
      "step 3313, loss: 2.038461\n",
      "step 3314, loss: 2.036224\n",
      "step 3315, loss: 2.033993\n",
      "step 3316, loss: 2.031767\n",
      "step 3317, loss: 2.029547\n",
      "step 3318, loss: 2.027333\n",
      "step 3319, loss: 2.025124\n",
      "step 3320, loss: 2.022920\n",
      "step 3321, loss: 2.020722\n",
      "step 3322, loss: 2.018530\n",
      "step 3323, loss: 2.016343\n",
      "step 3324, loss: 2.014161\n",
      "step 3325, loss: 2.011985\n",
      "step 3326, loss: 2.009815\n",
      "step 3327, loss: 2.007650\n",
      "step 3328, loss: 2.005490\n",
      "step 3329, loss: 2.003336\n",
      "step 3330, loss: 2.001187\n",
      "step 3331, loss: 1.999044\n",
      "step 3332, loss: 1.996906\n",
      "step 3333, loss: 1.994773\n",
      "step 3334, loss: 1.992646\n",
      "step 3335, loss: 1.990524\n",
      "step 3336, loss: 1.988408\n",
      "step 3337, loss: 1.986297\n",
      "step 3338, loss: 1.984191\n",
      "step 3339, loss: 1.982091\n",
      "step 3340, loss: 1.979996\n",
      "step 3341, loss: 1.977906\n",
      "step 3342, loss: 1.975822\n",
      "step 3343, loss: 1.973742\n",
      "step 3344, loss: 1.971669\n",
      "step 3345, loss: 1.969600\n",
      "step 3346, loss: 1.967537\n",
      "step 3347, loss: 1.965479\n",
      "step 3348, loss: 1.963426\n",
      "step 3349, loss: 1.961378\n",
      "step 3350, loss: 1.959336\n",
      "step 3351, loss: 1.957299\n",
      "step 3352, loss: 1.955267\n",
      "step 3353, loss: 1.953240\n",
      "step 3354, loss: 1.951219\n",
      "step 3355, loss: 1.949202\n",
      "step 3356, loss: 1.947191\n",
      "step 3357, loss: 1.945185\n",
      "step 3358, loss: 1.943184\n",
      "step 3359, loss: 1.941188\n",
      "step 3360, loss: 1.939197\n",
      "step 3361, loss: 1.937212\n",
      "step 3362, loss: 1.935231\n",
      "step 3363, loss: 1.933256\n",
      "step 3364, loss: 1.931285\n",
      "step 3365, loss: 1.929320\n",
      "step 3366, loss: 1.927360\n",
      "step 3367, loss: 1.925404\n",
      "step 3368, loss: 1.923454\n",
      "step 3369, loss: 1.921509\n",
      "step 3370, loss: 1.919569\n",
      "step 3371, loss: 1.917633\n",
      "step 3372, loss: 1.915704\n",
      "step 3373, loss: 1.913778\n",
      "step 3374, loss: 1.911858\n",
      "step 3375, loss: 1.909943\n",
      "step 3376, loss: 1.908033\n",
      "step 3377, loss: 1.906127\n",
      "step 3378, loss: 1.904227\n",
      "step 3379, loss: 1.902332\n",
      "step 3380, loss: 1.900441\n",
      "step 3381, loss: 1.898556\n",
      "step 3382, loss: 1.896675\n",
      "step 3383, loss: 1.894799\n",
      "step 3384, loss: 1.892928\n",
      "step 3385, loss: 1.891062\n",
      "step 3386, loss: 1.889201\n",
      "step 3387, loss: 1.887344\n",
      "step 3388, loss: 1.885493\n",
      "step 3389, loss: 1.883646\n",
      "step 3390, loss: 1.881804\n",
      "step 3391, loss: 1.879967\n",
      "step 3392, loss: 1.878134\n",
      "step 3393, loss: 1.876307\n",
      "step 3394, loss: 1.874484\n",
      "step 3395, loss: 1.872666\n",
      "step 3396, loss: 1.870853\n",
      "step 3397, loss: 1.869045\n",
      "step 3398, loss: 1.867241\n",
      "step 3399, loss: 1.865442\n",
      "step 3400, loss: 1.863648\n",
      "step 3401, loss: 1.861858\n",
      "step 3402, loss: 1.860073\n",
      "step 3403, loss: 1.858293\n",
      "step 3404, loss: 1.856518\n",
      "step 3405, loss: 1.854747\n",
      "step 3406, loss: 1.852981\n",
      "step 3407, loss: 1.851219\n",
      "step 3408, loss: 1.849462\n",
      "step 3409, loss: 1.847710\n",
      "step 3410, loss: 1.845963\n",
      "step 3411, loss: 1.844220\n",
      "step 3412, loss: 1.842481\n",
      "step 3413, loss: 1.840748\n",
      "step 3414, loss: 1.839018\n",
      "step 3415, loss: 1.837294\n",
      "step 3416, loss: 1.835574\n",
      "step 3417, loss: 1.833858\n",
      "step 3418, loss: 1.832147\n",
      "step 3419, loss: 1.830441\n",
      "step 3420, loss: 1.828739\n",
      "step 3421, loss: 1.827042\n",
      "step 3422, loss: 1.825349\n",
      "step 3423, loss: 1.823661\n",
      "step 3424, loss: 1.821977\n",
      "step 3425, loss: 1.820297\n",
      "step 3426, loss: 1.818623\n",
      "step 3427, loss: 1.816953\n",
      "step 3428, loss: 1.815287\n",
      "step 3429, loss: 1.813625\n",
      "step 3430, loss: 1.811968\n",
      "step 3431, loss: 1.810315\n",
      "step 3432, loss: 1.808667\n",
      "step 3433, loss: 1.807024\n",
      "step 3434, loss: 1.805384\n",
      "step 3435, loss: 1.803749\n",
      "step 3436, loss: 1.802118\n",
      "step 3437, loss: 1.800492\n",
      "step 3438, loss: 1.798870\n",
      "step 3439, loss: 1.797253\n",
      "step 3440, loss: 1.795640\n",
      "step 3441, loss: 1.794031\n",
      "step 3442, loss: 1.792426\n",
      "step 3443, loss: 1.790826\n",
      "step 3444, loss: 1.789230\n",
      "step 3445, loss: 1.787638\n",
      "step 3446, loss: 1.786051\n",
      "step 3447, loss: 1.784468\n",
      "step 3448, loss: 1.782889\n",
      "step 3449, loss: 1.781315\n",
      "step 3450, loss: 1.779744\n",
      "step 3451, loss: 1.778179\n",
      "step 3452, loss: 1.776617\n",
      "step 3453, loss: 1.775059\n",
      "step 3454, loss: 1.773506\n",
      "step 3455, loss: 1.771957\n",
      "step 3456, loss: 1.770412\n",
      "step 3457, loss: 1.768871\n",
      "step 3458, loss: 1.767335\n",
      "step 3459, loss: 1.765802\n",
      "step 3460, loss: 1.764274\n",
      "step 3461, loss: 1.762750\n",
      "step 3462, loss: 1.761230\n",
      "step 3463, loss: 1.759715\n",
      "step 3464, loss: 1.758203\n",
      "step 3465, loss: 1.756695\n",
      "step 3466, loss: 1.755192\n",
      "step 3467, loss: 1.753693\n",
      "step 3468, loss: 1.752198\n",
      "step 3469, loss: 1.750706\n",
      "step 3470, loss: 1.749219\n",
      "step 3471, loss: 1.747736\n",
      "step 3472, loss: 1.746257\n",
      "step 3473, loss: 1.744782\n",
      "step 3474, loss: 1.743312\n",
      "step 3475, loss: 1.741845\n",
      "step 3476, loss: 1.740382\n",
      "step 3477, loss: 1.738924\n",
      "step 3478, loss: 1.737469\n",
      "step 3479, loss: 1.736018\n",
      "step 3480, loss: 1.734571\n",
      "step 3481, loss: 1.733128\n",
      "step 3482, loss: 1.731689\n",
      "step 3483, loss: 1.730255\n",
      "step 3484, loss: 1.728824\n",
      "step 3485, loss: 1.727397\n",
      "step 3486, loss: 1.725974\n",
      "step 3487, loss: 1.724555\n",
      "step 3488, loss: 1.723140\n",
      "step 3489, loss: 1.721728\n",
      "step 3490, loss: 1.720321\n",
      "step 3491, loss: 1.718918\n",
      "step 3492, loss: 1.717518\n",
      "step 3493, loss: 1.716122\n",
      "step 3494, loss: 1.714731\n",
      "step 3495, loss: 1.713343\n",
      "step 3496, loss: 1.711959\n",
      "step 3497, loss: 1.710578\n",
      "step 3498, loss: 1.709202\n",
      "step 3499, loss: 1.707829\n",
      "step 3500, loss: 1.706460\n",
      "step 3501, loss: 1.705096\n",
      "step 3502, loss: 1.703734\n",
      "step 3503, loss: 1.702377\n",
      "step 3504, loss: 1.701023\n",
      "step 3505, loss: 1.699674\n",
      "step 3506, loss: 1.698328\n",
      "step 3507, loss: 1.696985\n",
      "step 3508, loss: 1.695647\n",
      "step 3509, loss: 1.694312\n",
      "step 3510, loss: 1.692981\n",
      "step 3511, loss: 1.691654\n",
      "step 3512, loss: 1.690330\n",
      "step 3513, loss: 1.689010\n",
      "step 3514, loss: 1.687694\n",
      "step 3515, loss: 1.686381\n",
      "step 3516, loss: 1.685073\n",
      "step 3517, loss: 1.683767\n",
      "step 3518, loss: 1.682466\n",
      "step 3519, loss: 1.681168\n",
      "step 3520, loss: 1.679874\n",
      "step 3521, loss: 1.678584\n",
      "step 3522, loss: 1.677297\n",
      "step 3523, loss: 1.676013\n",
      "step 3524, loss: 1.674734\n",
      "step 3525, loss: 1.673458\n",
      "step 3526, loss: 1.672185\n",
      "step 3527, loss: 1.670916\n",
      "step 3528, loss: 1.669651\n",
      "step 3529, loss: 1.668390\n",
      "step 3530, loss: 1.667132\n",
      "step 3531, loss: 1.665877\n",
      "step 3532, loss: 1.664626\n",
      "step 3533, loss: 1.663379\n",
      "step 3534, loss: 1.662135\n",
      "step 3535, loss: 1.660895\n",
      "step 3536, loss: 1.659658\n",
      "step 3537, loss: 1.658424\n",
      "step 3538, loss: 1.657195\n",
      "step 3539, loss: 1.655968\n",
      "step 3540, loss: 1.654745\n",
      "step 3541, loss: 1.653526\n",
      "step 3542, loss: 1.652310\n",
      "step 3543, loss: 1.651098\n",
      "step 3544, loss: 1.649889\n",
      "step 3545, loss: 1.648684\n",
      "step 3546, loss: 1.647482\n",
      "step 3547, loss: 1.646283\n",
      "step 3548, loss: 1.645088\n",
      "step 3549, loss: 1.643896\n",
      "step 3550, loss: 1.642708\n",
      "step 3551, loss: 1.641523\n",
      "step 3552, loss: 1.640342\n",
      "step 3553, loss: 1.639163\n",
      "step 3554, loss: 1.637989\n",
      "step 3555, loss: 1.636817\n",
      "step 3556, loss: 1.635649\n",
      "step 3557, loss: 1.634485\n",
      "step 3558, loss: 1.633324\n",
      "step 3559, loss: 1.632165\n",
      "step 3560, loss: 1.631011\n",
      "step 3561, loss: 1.629860\n",
      "step 3562, loss: 1.628712\n",
      "step 3563, loss: 1.627567\n",
      "step 3564, loss: 1.626426\n",
      "step 3565, loss: 1.625288\n",
      "step 3566, loss: 1.624153\n",
      "step 3567, loss: 1.623021\n",
      "step 3568, loss: 1.621893\n",
      "step 3569, loss: 1.620768\n",
      "step 3570, loss: 1.619647\n",
      "step 3571, loss: 1.618528\n",
      "step 3572, loss: 1.617413\n",
      "step 3573, loss: 1.616301\n",
      "step 3574, loss: 1.615192\n",
      "step 3575, loss: 1.614087\n",
      "step 3576, loss: 1.612985\n",
      "step 3577, loss: 1.611886\n",
      "step 3578, loss: 1.610790\n",
      "step 3579, loss: 1.609697\n",
      "step 3580, loss: 1.608608\n",
      "step 3581, loss: 1.607522\n",
      "step 3582, loss: 1.606438\n",
      "step 3583, loss: 1.605358\n",
      "step 3584, loss: 1.604282\n",
      "step 3585, loss: 1.603208\n",
      "step 3586, loss: 1.602137\n",
      "step 3587, loss: 1.601070\n",
      "step 3588, loss: 1.600006\n",
      "step 3589, loss: 1.598945\n",
      "step 3590, loss: 1.597887\n",
      "step 3591, loss: 1.596832\n",
      "step 3592, loss: 1.595780\n",
      "step 3593, loss: 1.594731\n",
      "step 3594, loss: 1.593685\n",
      "step 3595, loss: 1.592643\n",
      "step 3596, loss: 1.591603\n",
      "step 3597, loss: 1.590567\n",
      "step 3598, loss: 1.589533\n",
      "step 3599, loss: 1.588503\n",
      "step 3600, loss: 1.587476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3601, loss: 1.586452\n",
      "step 3602, loss: 1.585430\n",
      "step 3603, loss: 1.584412\n",
      "step 3604, loss: 1.583397\n",
      "step 3605, loss: 1.582385\n",
      "step 3606, loss: 1.581376\n",
      "step 3607, loss: 1.580369\n",
      "step 3608, loss: 1.579366\n",
      "step 3609, loss: 1.578366\n",
      "step 3610, loss: 1.577368\n",
      "step 3611, loss: 1.576374\n",
      "step 3612, loss: 1.575383\n",
      "step 3613, loss: 1.574394\n",
      "step 3614, loss: 1.573409\n",
      "step 3615, loss: 1.572426\n",
      "step 3616, loss: 1.571446\n",
      "step 3617, loss: 1.570470\n",
      "step 3618, loss: 1.569496\n",
      "step 3619, loss: 1.568525\n",
      "step 3620, loss: 1.567557\n",
      "step 3621, loss: 1.566592\n",
      "step 3622, loss: 1.565630\n",
      "step 3623, loss: 1.564670\n",
      "step 3624, loss: 1.563714\n",
      "step 3625, loss: 1.562760\n",
      "step 3626, loss: 1.561809\n",
      "step 3627, loss: 1.560861\n",
      "step 3628, loss: 1.559916\n",
      "step 3629, loss: 1.558974\n",
      "step 3630, loss: 1.558035\n",
      "step 3631, loss: 1.557098\n",
      "step 3632, loss: 1.556164\n",
      "step 3633, loss: 1.555233\n",
      "step 3634, loss: 1.554305\n",
      "step 3635, loss: 1.553380\n",
      "step 3636, loss: 1.552457\n",
      "step 3637, loss: 1.551537\n",
      "step 3638, loss: 1.550620\n",
      "step 3639, loss: 1.549706\n",
      "step 3640, loss: 1.548794\n",
      "step 3641, loss: 1.547886\n",
      "step 3642, loss: 1.546980\n",
      "step 3643, loss: 1.546076\n",
      "step 3644, loss: 1.545176\n",
      "step 3645, loss: 1.544278\n",
      "step 3646, loss: 1.543383\n",
      "step 3647, loss: 1.542490\n",
      "step 3648, loss: 1.541601\n",
      "step 3649, loss: 1.540714\n",
      "step 3650, loss: 1.539829\n",
      "step 3651, loss: 1.538948\n",
      "step 3652, loss: 1.538069\n",
      "step 3653, loss: 1.537193\n",
      "step 3654, loss: 1.536319\n",
      "step 3655, loss: 1.535448\n",
      "step 3656, loss: 1.534580\n",
      "step 3657, loss: 1.533714\n",
      "step 3658, loss: 1.532851\n",
      "step 3659, loss: 1.531991\n",
      "step 3660, loss: 1.531133\n",
      "step 3661, loss: 1.530278\n",
      "step 3662, loss: 1.529426\n",
      "step 3663, loss: 1.528576\n",
      "step 3664, loss: 1.527729\n",
      "step 3665, loss: 1.526884\n",
      "step 3666, loss: 1.526042\n",
      "step 3667, loss: 1.525202\n",
      "step 3668, loss: 1.524366\n",
      "step 3669, loss: 1.523531\n",
      "step 3670, loss: 1.522699\n",
      "step 3671, loss: 1.521870\n",
      "step 3672, loss: 1.521044\n",
      "step 3673, loss: 1.520219\n",
      "step 3674, loss: 1.519398\n",
      "step 3675, loss: 1.518579\n",
      "step 3676, loss: 1.517762\n",
      "step 3677, loss: 1.516948\n",
      "step 3678, loss: 1.516137\n",
      "step 3679, loss: 1.515328\n",
      "step 3680, loss: 1.514521\n",
      "step 3681, loss: 1.513717\n",
      "step 3682, loss: 1.512915\n",
      "step 3683, loss: 1.512117\n",
      "step 3684, loss: 1.511320\n",
      "step 3685, loss: 1.510526\n",
      "step 3686, loss: 1.509735\n",
      "step 3687, loss: 1.508945\n",
      "step 3688, loss: 1.508158\n",
      "step 3689, loss: 1.507374\n",
      "step 3690, loss: 1.506592\n",
      "step 3691, loss: 1.505813\n",
      "step 3692, loss: 1.505036\n",
      "step 3693, loss: 1.504261\n",
      "step 3694, loss: 1.503489\n",
      "step 3695, loss: 1.502720\n",
      "step 3696, loss: 1.501952\n",
      "step 3697, loss: 1.501187\n",
      "step 3698, loss: 1.500425\n",
      "step 3699, loss: 1.499665\n",
      "step 3700, loss: 1.498907\n",
      "step 3701, loss: 1.498151\n",
      "step 3702, loss: 1.497398\n",
      "step 3703, loss: 1.496647\n",
      "step 3704, loss: 1.495899\n",
      "step 3705, loss: 1.495153\n",
      "step 3706, loss: 1.494410\n",
      "step 3707, loss: 1.493668\n",
      "step 3708, loss: 1.492929\n",
      "step 3709, loss: 1.492193\n",
      "step 3710, loss: 1.491458\n",
      "step 3711, loss: 1.490726\n",
      "step 3712, loss: 1.489997\n",
      "step 3713, loss: 1.489269\n",
      "step 3714, loss: 1.488544\n",
      "step 3715, loss: 1.487821\n",
      "step 3716, loss: 1.487101\n",
      "step 3717, loss: 1.486382\n",
      "step 3718, loss: 1.485666\n",
      "step 3719, loss: 1.484952\n",
      "step 3720, loss: 1.484241\n",
      "step 3721, loss: 1.483532\n",
      "step 3722, loss: 1.482825\n",
      "step 3723, loss: 1.482120\n",
      "step 3724, loss: 1.481417\n",
      "step 3725, loss: 1.480717\n",
      "step 3726, loss: 1.480019\n",
      "step 3727, loss: 1.479323\n",
      "step 3728, loss: 1.478630\n",
      "step 3729, loss: 1.477938\n",
      "step 3730, loss: 1.477249\n",
      "step 3731, loss: 1.476562\n",
      "step 3732, loss: 1.475877\n",
      "step 3733, loss: 1.475194\n",
      "step 3734, loss: 1.474514\n",
      "step 3735, loss: 1.473835\n",
      "step 3736, loss: 1.473159\n",
      "step 3737, loss: 1.472485\n",
      "step 3738, loss: 1.471814\n",
      "step 3739, loss: 1.471144\n",
      "step 3740, loss: 1.470476\n",
      "step 3741, loss: 1.469811\n",
      "step 3742, loss: 1.469148\n",
      "step 3743, loss: 1.468487\n",
      "step 3744, loss: 1.467828\n",
      "step 3745, loss: 1.467171\n",
      "step 3746, loss: 1.466516\n",
      "step 3747, loss: 1.465863\n",
      "step 3748, loss: 1.465213\n",
      "step 3749, loss: 1.464564\n",
      "step 3750, loss: 1.463917\n",
      "step 3751, loss: 1.463274\n",
      "step 3752, loss: 1.462631\n",
      "step 3753, loss: 1.461991\n",
      "step 3754, loss: 1.461353\n",
      "step 3755, loss: 1.460717\n",
      "step 3756, loss: 1.460083\n",
      "step 3757, loss: 1.459451\n",
      "step 3758, loss: 1.458821\n",
      "step 3759, loss: 1.458193\n",
      "step 3760, loss: 1.457567\n",
      "step 3761, loss: 1.456943\n",
      "step 3762, loss: 1.456321\n",
      "step 3763, loss: 1.455702\n",
      "step 3764, loss: 1.455084\n",
      "step 3765, loss: 1.454468\n",
      "step 3766, loss: 1.453855\n",
      "step 3767, loss: 1.453243\n",
      "step 3768, loss: 1.452633\n",
      "step 3769, loss: 1.452025\n",
      "step 3770, loss: 1.451420\n",
      "step 3771, loss: 1.450816\n",
      "step 3772, loss: 1.450214\n",
      "step 3773, loss: 1.449614\n",
      "step 3774, loss: 1.449016\n",
      "step 3775, loss: 1.448420\n",
      "step 3776, loss: 1.447826\n",
      "step 3777, loss: 1.447234\n",
      "step 3778, loss: 1.446644\n",
      "step 3779, loss: 1.446056\n",
      "step 3780, loss: 1.445469\n",
      "step 3781, loss: 1.444885\n",
      "step 3782, loss: 1.444303\n",
      "step 3783, loss: 1.443722\n",
      "step 3784, loss: 1.443144\n",
      "step 3785, loss: 1.442567\n",
      "step 3786, loss: 1.441992\n",
      "step 3787, loss: 1.441419\n",
      "step 3788, loss: 1.440848\n",
      "step 3789, loss: 1.440279\n",
      "step 3790, loss: 1.439712\n",
      "step 3791, loss: 1.439146\n",
      "step 3792, loss: 1.438583\n",
      "step 3793, loss: 1.438021\n",
      "step 3794, loss: 1.437461\n",
      "step 3795, loss: 1.436903\n",
      "step 3796, loss: 1.436347\n",
      "step 3797, loss: 1.435793\n",
      "step 3798, loss: 1.435240\n",
      "step 3799, loss: 1.434690\n",
      "step 3800, loss: 1.434141\n",
      "step 3801, loss: 1.433594\n",
      "step 3802, loss: 1.433049\n",
      "step 3803, loss: 1.432506\n",
      "step 3804, loss: 1.431964\n",
      "step 3805, loss: 1.431424\n",
      "step 3806, loss: 1.430886\n",
      "step 3807, loss: 1.430350\n",
      "step 3808, loss: 1.429816\n",
      "step 3809, loss: 1.429283\n",
      "step 3810, loss: 1.428752\n",
      "step 3811, loss: 1.428223\n",
      "step 3812, loss: 1.427696\n",
      "step 3813, loss: 1.427170\n",
      "step 3814, loss: 1.426647\n",
      "step 3815, loss: 1.426125\n",
      "step 3816, loss: 1.425604\n",
      "step 3817, loss: 1.425086\n",
      "step 3818, loss: 1.424569\n",
      "step 3819, loss: 1.424054\n",
      "step 3820, loss: 1.423541\n",
      "step 3821, loss: 1.423029\n",
      "step 3822, loss: 1.422519\n",
      "step 3823, loss: 1.422011\n",
      "step 3824, loss: 1.421505\n",
      "step 3825, loss: 1.421000\n",
      "step 3826, loss: 1.420497\n",
      "step 3827, loss: 1.419995\n",
      "step 3828, loss: 1.419496\n",
      "step 3829, loss: 1.418998\n",
      "step 3830, loss: 1.418501\n",
      "step 3831, loss: 1.418007\n",
      "step 3832, loss: 1.417514\n",
      "step 3833, loss: 1.417023\n",
      "step 3834, loss: 1.416533\n",
      "step 3835, loss: 1.416045\n",
      "step 3836, loss: 1.415559\n",
      "step 3837, loss: 1.415074\n",
      "step 3838, loss: 1.414591\n",
      "step 3839, loss: 1.414110\n",
      "step 3840, loss: 1.413630\n",
      "step 3841, loss: 1.413152\n",
      "step 3842, loss: 1.412675\n",
      "step 3843, loss: 1.412200\n",
      "step 3844, loss: 1.411727\n",
      "step 3845, loss: 1.411255\n",
      "step 3846, loss: 1.410785\n",
      "step 3847, loss: 1.410317\n",
      "step 3848, loss: 1.409850\n",
      "step 3849, loss: 1.409385\n",
      "step 3850, loss: 1.408921\n",
      "step 3851, loss: 1.408459\n",
      "step 3852, loss: 1.407999\n",
      "step 3853, loss: 1.407540\n",
      "step 3854, loss: 1.407082\n",
      "step 3855, loss: 1.406626\n",
      "step 3856, loss: 1.406172\n",
      "step 3857, loss: 1.405720\n",
      "step 3858, loss: 1.405268\n",
      "step 3859, loss: 1.404819\n",
      "step 3860, loss: 1.404371\n",
      "step 3861, loss: 1.403924\n",
      "step 3862, loss: 1.403479\n",
      "step 3863, loss: 1.403036\n",
      "step 3864, loss: 1.402594\n",
      "step 3865, loss: 1.402154\n",
      "step 3866, loss: 1.401715\n",
      "step 3867, loss: 1.401277\n",
      "step 3868, loss: 1.400842\n",
      "step 3869, loss: 1.400407\n",
      "step 3870, loss: 1.399975\n",
      "step 3871, loss: 1.399543\n",
      "step 3872, loss: 1.399113\n",
      "step 3873, loss: 1.398685\n",
      "step 3874, loss: 1.398258\n",
      "step 3875, loss: 1.397833\n",
      "step 3876, loss: 1.397409\n",
      "step 3877, loss: 1.396986\n",
      "step 3878, loss: 1.396565\n",
      "step 3879, loss: 1.396146\n",
      "step 3880, loss: 1.395728\n",
      "step 3881, loss: 1.395311\n",
      "step 3882, loss: 1.394896\n",
      "step 3883, loss: 1.394482\n",
      "step 3884, loss: 1.394070\n",
      "step 3885, loss: 1.393659\n",
      "step 3886, loss: 1.393250\n",
      "step 3887, loss: 1.392842\n",
      "step 3888, loss: 1.392435\n",
      "step 3889, loss: 1.392030\n",
      "step 3890, loss: 1.391626\n",
      "step 3891, loss: 1.391224\n",
      "step 3892, loss: 1.390823\n",
      "step 3893, loss: 1.390424\n",
      "step 3894, loss: 1.390026\n",
      "step 3895, loss: 1.389629\n",
      "step 3896, loss: 1.389234\n",
      "step 3897, loss: 1.388840\n",
      "step 3898, loss: 1.388447\n",
      "step 3899, loss: 1.388056\n",
      "step 3900, loss: 1.387666\n",
      "step 3901, loss: 1.387278\n",
      "step 3902, loss: 1.386891\n",
      "step 3903, loss: 1.386505\n",
      "step 3904, loss: 1.386121\n",
      "step 3905, loss: 1.385738\n",
      "step 3906, loss: 1.385356\n",
      "step 3907, loss: 1.384976\n",
      "step 3908, loss: 1.384597\n",
      "step 3909, loss: 1.384219\n",
      "step 3910, loss: 1.383843\n",
      "step 3911, loss: 1.383468\n",
      "step 3912, loss: 1.383095\n",
      "step 3913, loss: 1.382722\n",
      "step 3914, loss: 1.382351\n",
      "step 3915, loss: 1.381982\n",
      "step 3916, loss: 1.381613\n",
      "step 3917, loss: 1.381246\n",
      "step 3918, loss: 1.380881\n",
      "step 3919, loss: 1.380516\n",
      "step 3920, loss: 1.380153\n",
      "step 3921, loss: 1.379791\n",
      "step 3922, loss: 1.379431\n",
      "step 3923, loss: 1.379071\n",
      "step 3924, loss: 1.378714\n",
      "step 3925, loss: 1.378357\n",
      "step 3926, loss: 1.378001\n",
      "step 3927, loss: 1.377647\n",
      "step 3928, loss: 1.377294\n",
      "step 3929, loss: 1.376943\n",
      "step 3930, loss: 1.376593\n",
      "step 3931, loss: 1.376243\n",
      "step 3932, loss: 1.375895\n",
      "step 3933, loss: 1.375549\n",
      "step 3934, loss: 1.375203\n",
      "step 3935, loss: 1.374859\n",
      "step 3936, loss: 1.374516\n",
      "step 3937, loss: 1.374175\n",
      "step 3938, loss: 1.373834\n",
      "step 3939, loss: 1.373495\n",
      "step 3940, loss: 1.373157\n",
      "step 3941, loss: 1.372820\n",
      "step 3942, loss: 1.372485\n",
      "step 3943, loss: 1.372150\n",
      "step 3944, loss: 1.371817\n",
      "step 3945, loss: 1.371485\n",
      "step 3946, loss: 1.371155\n",
      "step 3947, loss: 1.370825\n",
      "step 3948, loss: 1.370497\n",
      "step 3949, loss: 1.370170\n",
      "step 3950, loss: 1.369843\n",
      "step 3951, loss: 1.369519\n",
      "step 3952, loss: 1.369195\n",
      "step 3953, loss: 1.368873\n",
      "step 3954, loss: 1.368551\n",
      "step 3955, loss: 1.368231\n",
      "step 3956, loss: 1.367912\n",
      "step 3957, loss: 1.367594\n",
      "step 3958, loss: 1.367278\n",
      "step 3959, loss: 1.366962\n",
      "step 3960, loss: 1.366648\n",
      "step 3961, loss: 1.366335\n",
      "step 3962, loss: 1.366023\n",
      "step 3963, loss: 1.365712\n",
      "step 3964, loss: 1.365402\n",
      "step 3965, loss: 1.365093\n",
      "step 3966, loss: 1.364786\n",
      "step 3967, loss: 1.364479\n",
      "step 3968, loss: 1.364174\n",
      "step 3969, loss: 1.363870\n",
      "step 3970, loss: 1.363567\n",
      "step 3971, loss: 1.363265\n",
      "step 3972, loss: 1.362964\n",
      "step 3973, loss: 1.362664\n",
      "step 3974, loss: 1.362366\n",
      "step 3975, loss: 1.362068\n",
      "step 3976, loss: 1.361772\n",
      "step 3977, loss: 1.361476\n",
      "step 3978, loss: 1.361182\n",
      "step 3979, loss: 1.360889\n",
      "step 3980, loss: 1.360597\n",
      "step 3981, loss: 1.360306\n",
      "step 3982, loss: 1.360016\n",
      "step 3983, loss: 1.359727\n",
      "step 3984, loss: 1.359439\n",
      "step 3985, loss: 1.359152\n",
      "step 3986, loss: 1.358867\n",
      "step 3987, loss: 1.358582\n",
      "step 3988, loss: 1.358299\n",
      "step 3989, loss: 1.358016\n",
      "step 3990, loss: 1.357735\n",
      "step 3991, loss: 1.357454\n",
      "step 3992, loss: 1.357175\n",
      "step 3993, loss: 1.356896\n",
      "step 3994, loss: 1.356619\n",
      "step 3995, loss: 1.356343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 3996, loss: 1.356067\n",
      "step 3997, loss: 1.355793\n",
      "step 3998, loss: 1.355520\n",
      "step 3999, loss: 1.355248\n"
     ]
    }
   ],
   "source": [
    "iteration = 4000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # Training the model\n",
    "    for i in range(iteration):\n",
    "        feed_dict = {x:x_normal,y:y_normal}\n",
    "        _, summary, loss = sess.run([train_op, merged, l], feed_dict=feed_dict)\n",
    "        print(\"step %d, loss: %f\" % (i, loss))\n",
    "        train_writer.add_summary(summary, i)\n",
    "\n",
    "    # Get weights\n",
    "    m_pred, n_pred = sess.run([m, n])\n",
    "    \n",
    "    # Make Predictions\n",
    "    y_pred_final = sess.run(y_pred, feed_dict={x: x_normal})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualising the Normalized data\n",
    "plt.scatter(x_normal,y_normal,color='blue')\n",
    "plt.plot(x_normal,y_pred_final,color='red')\n",
    "plt.title('Linear Model')\n",
    "plt.xlabel('x values')\n",
    "plt.ylabel('y values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualising the Training data\n",
    "plt.scatter(x_true,y_true,color='blue')\n",
    "plt.plot(x_true,y_pred_final * y_std + y_mean,color='red')\n",
    "plt.title('Linear Model')\n",
    "plt.xlabel('x values')\n",
    "plt.ylabel('y values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predicting new values\n",
    "def predict(x_new):\n",
    "    x_normal = (x_new - x_mean) / x_std\n",
    "    y_normal = m * x_normal + n\n",
    "    y_pred = y_normal * y_std + y_mean\n",
    "    return y_pred\n",
    "\n",
    "plt.scatter(x_true,y_true,color='blue')\n",
    "plt.plot(x_true,y_pred_final * y_std + y_mean,color='red')\n",
    "plt.title('Linear Model')\n",
    "plt.xlabel('x values')\n",
    "plt.ylabel('y values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
